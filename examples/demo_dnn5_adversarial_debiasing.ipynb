{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Load all necessary packages\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.datasets import AdultDataset, GermanDataset, CompasDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from aif360.metrics.utils import compute_boolean_conditioning_vector\n",
    "\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions import load_preproc_data_adult, load_preproc_data_compas, load_preproc_data_german\n",
    "\n",
    "from aif360.algorithms.inprocessing.adversarial_debiasing_dnn5 import AdversarialDebiasingDnn5\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataset and split into train and test\n",
    "dataset_orig = load_preproc_data_adult()\n",
    "\n",
    "privileged_groups = [{'sex': 1}]\n",
    "unprivileged_groups = [{'sex': 0}]\n",
    "\n",
    "dataset_orig_train, dataset_orig_test = dataset_orig.split([0.8], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Training Dataset shape"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39073, 18)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Favorable and unfavorable labels"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Protected attribute names"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sex', 'race']\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Privileged and unprivileged protected attribute values"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1.]), array([1.])] [array([0.]), array([0.])]\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Dataset feature names"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['race', 'sex', 'Age (decade)=10', 'Age (decade)=20', 'Age (decade)=30', 'Age (decade)=40', 'Age (decade)=50', 'Age (decade)=60', 'Age (decade)=>=70', 'Education Years=6', 'Education Years=7', 'Education Years=8', 'Education Years=9', 'Education Years=10', 'Education Years=11', 'Education Years=12', 'Education Years=<6', 'Education Years=>12']\n"
     ]
    }
   ],
   "source": [
    "# print out some labels, names, etc.\n",
    "display(Markdown(\"#### Training Dataset shape\"))\n",
    "print(dataset_orig_train.features.shape)\n",
    "display(Markdown(\"#### Favorable and unfavorable labels\"))\n",
    "print(dataset_orig_train.favorable_label, dataset_orig_train.unfavorable_label)\n",
    "display(Markdown(\"#### Protected attribute names\"))\n",
    "print(dataset_orig_train.protected_attribute_names)\n",
    "display(Markdown(\"#### Privileged and unprivileged protected attribute values\"))\n",
    "print(dataset_orig_train.privileged_protected_attributes, \n",
    "      dataset_orig_train.unprivileged_protected_attributes)\n",
    "display(Markdown(\"#### Dataset feature names\"))\n",
    "print(dataset_orig_train.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metric for original training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Original training dataset"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Difference in mean outcomes between unprivileged and privileged groups = -0.195369\n",
      "Test set: Difference in mean outcomes between unprivileged and privileged groups = -0.191207\n"
     ]
    }
   ],
   "source": [
    "# Metric for the original dataset\n",
    "metric_orig_train = BinaryLabelDatasetMetric(dataset_orig_train, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Original training dataset\"))\n",
    "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())\n",
    "metric_orig_test = BinaryLabelDatasetMetric(dataset_orig_test, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_test.mean_difference())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Scaled dataset - Verify that the scaling does not affect the group label statistics"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Difference in mean outcomes between unprivileged and privileged groups = -0.195369\n",
      "Test set: Difference in mean outcomes between unprivileged and privileged groups = -0.191207\n"
     ]
    }
   ],
   "source": [
    "min_max_scaler = MaxAbsScaler()\n",
    "dataset_orig_train.features = min_max_scaler.fit_transform(dataset_orig_train.features)\n",
    "dataset_orig_test.features = min_max_scaler.transform(dataset_orig_test.features)\n",
    "metric_scaled_train = BinaryLabelDatasetMetric(dataset_orig_train, \n",
    "                             unprivileged_groups=unprivileged_groups,\n",
    "                             privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Scaled dataset - Verify that the scaling does not affect the group label statistics\"))\n",
    "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_scaled_train.mean_difference())\n",
    "metric_scaled_test = BinaryLabelDatasetMetric(dataset_orig_test, \n",
    "                             unprivileged_groups=unprivileged_groups,\n",
    "                             privileged_groups=privileged_groups)\n",
    "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_scaled_test.mean_difference())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn plan classifier without debiasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load post-processing algorithm that equalizes the odds\n",
    "# Learn parameters with debias set to False\n",
    "sess = tf.Session()\n",
    "plain_model = AdversarialDebiasingDnn5(privileged_groups = privileged_groups,\n",
    "                          unprivileged_groups = unprivileged_groups,\n",
    "                          scope_name='plain_classifier',\n",
    "                          debias=False,\n",
    "                          sess=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\PycharmProjects\\AIF360\\aif360\\algorithms\\inprocessing\\adversarial_debiasing_dnn5.py:119: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\PycharmProjects\\AIF360\\aif360\\algorithms\\inprocessing\\adversarial_debiasing_dnn5.py:128: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\PycharmProjects\\AIF360\\aif360\\load_model\\layer.py:23: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\PycharmProjects\\AIF360\\aif360\\load_model\\layer.py:25: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From D:\\Software\\anaconda3\\envs\\newtf115\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From D:\\PycharmProjects\\AIF360\\aif360\\algorithms\\inprocessing\\adversarial_debiasing_dnn5.py:159: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\PycharmProjects\\AIF360\\aif360\\algorithms\\inprocessing\\adversarial_debiasing_dnn5.py:161: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\PycharmProjects\\AIF360\\aif360\\algorithms\\inprocessing\\adversarial_debiasing_dnn5.py:167: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\PycharmProjects\\AIF360\\aif360\\algorithms\\inprocessing\\adversarial_debiasing_dnn5.py:207: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\PycharmProjects\\AIF360\\aif360\\algorithms\\inprocessing\\adversarial_debiasing_dnn5.py:208: The name tf.local_variables_initializer is deprecated. Please use tf.compat.v1.local_variables_initializer instead.\n",
      "\n",
      "epoch 0; iter: 0; batch classifier loss: 0.692423\n",
      "epoch 0; iter: 200; batch classifier loss: 0.365509\n",
      "epoch 1; iter: 0; batch classifier loss: 0.394288\n",
      "epoch 1; iter: 200; batch classifier loss: 0.484862\n",
      "epoch 2; iter: 0; batch classifier loss: 0.391677\n",
      "epoch 2; iter: 200; batch classifier loss: 0.431666\n",
      "epoch 3; iter: 0; batch classifier loss: 0.381287\n",
      "epoch 3; iter: 200; batch classifier loss: 0.426387\n",
      "epoch 4; iter: 0; batch classifier loss: 0.382331\n",
      "epoch 4; iter: 200; batch classifier loss: 0.453986\n",
      "epoch 5; iter: 0; batch classifier loss: 0.391427\n",
      "epoch 5; iter: 200; batch classifier loss: 0.353242\n",
      "epoch 6; iter: 0; batch classifier loss: 0.501550\n",
      "epoch 6; iter: 200; batch classifier loss: 0.366409\n",
      "epoch 7; iter: 0; batch classifier loss: 0.365766\n",
      "epoch 7; iter: 200; batch classifier loss: 0.384066\n",
      "epoch 8; iter: 0; batch classifier loss: 0.406643\n",
      "epoch 8; iter: 200; batch classifier loss: 0.459809\n",
      "epoch 9; iter: 0; batch classifier loss: 0.333373\n",
      "epoch 9; iter: 200; batch classifier loss: 0.391289\n",
      "epoch 10; iter: 0; batch classifier loss: 0.500987\n",
      "epoch 10; iter: 200; batch classifier loss: 0.389641\n",
      "epoch 11; iter: 0; batch classifier loss: 0.377994\n",
      "epoch 11; iter: 200; batch classifier loss: 0.360039\n",
      "epoch 12; iter: 0; batch classifier loss: 0.398509\n",
      "epoch 12; iter: 200; batch classifier loss: 0.405418\n",
      "epoch 13; iter: 0; batch classifier loss: 0.492584\n",
      "epoch 13; iter: 200; batch classifier loss: 0.450652\n",
      "epoch 14; iter: 0; batch classifier loss: 0.392104\n",
      "epoch 14; iter: 200; batch classifier loss: 0.461304\n",
      "epoch 15; iter: 0; batch classifier loss: 0.377872\n",
      "epoch 15; iter: 200; batch classifier loss: 0.453595\n",
      "epoch 16; iter: 0; batch classifier loss: 0.344517\n",
      "epoch 16; iter: 200; batch classifier loss: 0.383831\n",
      "epoch 17; iter: 0; batch classifier loss: 0.346732\n",
      "epoch 17; iter: 200; batch classifier loss: 0.312735\n",
      "epoch 18; iter: 0; batch classifier loss: 0.358267\n",
      "epoch 18; iter: 200; batch classifier loss: 0.448156\n",
      "epoch 19; iter: 0; batch classifier loss: 0.437848\n",
      "epoch 19; iter: 200; batch classifier loss: 0.344079\n",
      "epoch 20; iter: 0; batch classifier loss: 0.437854\n",
      "epoch 20; iter: 200; batch classifier loss: 0.331181\n",
      "epoch 21; iter: 0; batch classifier loss: 0.368664\n",
      "epoch 21; iter: 200; batch classifier loss: 0.465972\n",
      "epoch 22; iter: 0; batch classifier loss: 0.382965\n",
      "epoch 22; iter: 200; batch classifier loss: 0.390245\n",
      "epoch 23; iter: 0; batch classifier loss: 0.455211\n",
      "epoch 23; iter: 200; batch classifier loss: 0.482287\n",
      "epoch 24; iter: 0; batch classifier loss: 0.402040\n",
      "epoch 24; iter: 200; batch classifier loss: 0.447882\n",
      "epoch 25; iter: 0; batch classifier loss: 0.368030\n",
      "epoch 25; iter: 200; batch classifier loss: 0.409596\n",
      "epoch 26; iter: 0; batch classifier loss: 0.432426\n",
      "epoch 26; iter: 200; batch classifier loss: 0.320791\n",
      "epoch 27; iter: 0; batch classifier loss: 0.444868\n",
      "epoch 27; iter: 200; batch classifier loss: 0.410471\n",
      "epoch 28; iter: 0; batch classifier loss: 0.428413\n",
      "epoch 28; iter: 200; batch classifier loss: 0.391573\n",
      "epoch 29; iter: 0; batch classifier loss: 0.426380\n",
      "epoch 29; iter: 200; batch classifier loss: 0.402024\n",
      "epoch 30; iter: 0; batch classifier loss: 0.402365\n",
      "epoch 30; iter: 200; batch classifier loss: 0.393400\n",
      "epoch 31; iter: 0; batch classifier loss: 0.453603\n",
      "epoch 31; iter: 200; batch classifier loss: 0.408275\n",
      "epoch 32; iter: 0; batch classifier loss: 0.370219\n",
      "epoch 32; iter: 200; batch classifier loss: 0.333559\n",
      "epoch 33; iter: 0; batch classifier loss: 0.431782\n",
      "epoch 33; iter: 200; batch classifier loss: 0.417457\n",
      "epoch 34; iter: 0; batch classifier loss: 0.438809\n",
      "epoch 34; iter: 200; batch classifier loss: 0.434445\n",
      "epoch 35; iter: 0; batch classifier loss: 0.407642\n",
      "epoch 35; iter: 200; batch classifier loss: 0.423106\n",
      "epoch 36; iter: 0; batch classifier loss: 0.524199\n",
      "epoch 36; iter: 200; batch classifier loss: 0.401825\n",
      "epoch 37; iter: 0; batch classifier loss: 0.425828\n",
      "epoch 37; iter: 200; batch classifier loss: 0.450479\n",
      "epoch 38; iter: 0; batch classifier loss: 0.466174\n",
      "epoch 38; iter: 200; batch classifier loss: 0.432984\n",
      "epoch 39; iter: 0; batch classifier loss: 0.415089\n",
      "epoch 39; iter: 200; batch classifier loss: 0.489478\n",
      "epoch 40; iter: 0; batch classifier loss: 0.363781\n",
      "epoch 40; iter: 200; batch classifier loss: 0.349578\n",
      "epoch 41; iter: 0; batch classifier loss: 0.418096\n",
      "epoch 41; iter: 200; batch classifier loss: 0.310959\n",
      "epoch 42; iter: 0; batch classifier loss: 0.406242\n",
      "epoch 42; iter: 200; batch classifier loss: 0.412609\n",
      "epoch 43; iter: 0; batch classifier loss: 0.403491\n",
      "epoch 43; iter: 200; batch classifier loss: 0.386792\n",
      "epoch 44; iter: 0; batch classifier loss: 0.405125\n",
      "epoch 44; iter: 200; batch classifier loss: 0.541675\n",
      "epoch 45; iter: 0; batch classifier loss: 0.443557\n",
      "epoch 45; iter: 200; batch classifier loss: 0.335178\n",
      "epoch 46; iter: 0; batch classifier loss: 0.436443\n",
      "epoch 46; iter: 200; batch classifier loss: 0.448328\n",
      "epoch 47; iter: 0; batch classifier loss: 0.420527\n",
      "epoch 47; iter: 200; batch classifier loss: 0.446783\n",
      "epoch 48; iter: 0; batch classifier loss: 0.410181\n",
      "epoch 48; iter: 200; batch classifier loss: 0.386946\n",
      "epoch 49; iter: 0; batch classifier loss: 0.427764\n",
      "epoch 49; iter: 200; batch classifier loss: 0.441916\n",
      "epoch 50; iter: 0; batch classifier loss: 0.429535\n",
      "epoch 50; iter: 200; batch classifier loss: 0.371036\n",
      "epoch 51; iter: 0; batch classifier loss: 0.315406\n",
      "epoch 51; iter: 200; batch classifier loss: 0.500851\n",
      "epoch 52; iter: 0; batch classifier loss: 0.386806\n",
      "epoch 52; iter: 200; batch classifier loss: 0.431901\n",
      "epoch 53; iter: 0; batch classifier loss: 0.400642\n",
      "epoch 53; iter: 200; batch classifier loss: 0.445406\n",
      "epoch 54; iter: 0; batch classifier loss: 0.465869\n",
      "epoch 54; iter: 200; batch classifier loss: 0.493183\n",
      "epoch 55; iter: 0; batch classifier loss: 0.427408\n",
      "epoch 55; iter: 200; batch classifier loss: 0.364384\n",
      "epoch 56; iter: 0; batch classifier loss: 0.443066\n",
      "epoch 56; iter: 200; batch classifier loss: 0.432578\n",
      "epoch 57; iter: 0; batch classifier loss: 0.404939\n",
      "epoch 57; iter: 200; batch classifier loss: 0.423857\n",
      "epoch 58; iter: 0; batch classifier loss: 0.447107\n",
      "epoch 58; iter: 200; batch classifier loss: 0.429979\n",
      "epoch 59; iter: 0; batch classifier loss: 0.446618\n",
      "epoch 59; iter: 200; batch classifier loss: 0.442969\n",
      "epoch 60; iter: 0; batch classifier loss: 0.442229\n",
      "epoch 60; iter: 200; batch classifier loss: 0.314402\n",
      "epoch 61; iter: 0; batch classifier loss: 0.441616\n",
      "epoch 61; iter: 200; batch classifier loss: 0.433790\n",
      "epoch 62; iter: 0; batch classifier loss: 0.376809\n",
      "epoch 62; iter: 200; batch classifier loss: 0.336465\n",
      "epoch 63; iter: 0; batch classifier loss: 0.429874\n",
      "epoch 63; iter: 200; batch classifier loss: 0.443685\n",
      "epoch 64; iter: 0; batch classifier loss: 0.380269\n",
      "epoch 64; iter: 200; batch classifier loss: 0.445724\n",
      "epoch 65; iter: 0; batch classifier loss: 0.413617\n",
      "epoch 65; iter: 200; batch classifier loss: 0.490865\n",
      "epoch 66; iter: 0; batch classifier loss: 0.452199\n",
      "epoch 66; iter: 200; batch classifier loss: 0.432365\n",
      "epoch 67; iter: 0; batch classifier loss: 0.443406\n",
      "epoch 67; iter: 200; batch classifier loss: 0.343434\n",
      "epoch 68; iter: 0; batch classifier loss: 0.432209\n",
      "epoch 68; iter: 200; batch classifier loss: 0.391506\n",
      "epoch 69; iter: 0; batch classifier loss: 0.474125\n",
      "epoch 69; iter: 200; batch classifier loss: 0.433547\n",
      "epoch 70; iter: 0; batch classifier loss: 0.412356\n",
      "epoch 70; iter: 200; batch classifier loss: 0.512398\n",
      "epoch 71; iter: 0; batch classifier loss: 0.429003\n",
      "epoch 71; iter: 200; batch classifier loss: 0.388681\n",
      "epoch 72; iter: 0; batch classifier loss: 0.409247\n",
      "epoch 72; iter: 200; batch classifier loss: 0.347998\n",
      "epoch 73; iter: 0; batch classifier loss: 0.367991\n",
      "epoch 73; iter: 200; batch classifier loss: 0.383698\n",
      "epoch 74; iter: 0; batch classifier loss: 0.407146\n",
      "epoch 74; iter: 200; batch classifier loss: 0.445422\n",
      "epoch 75; iter: 0; batch classifier loss: 0.459998\n",
      "epoch 75; iter: 200; batch classifier loss: 0.494626\n",
      "epoch 76; iter: 0; batch classifier loss: 0.374835\n",
      "epoch 76; iter: 200; batch classifier loss: 0.454803\n",
      "epoch 77; iter: 0; batch classifier loss: 0.386326\n",
      "epoch 77; iter: 200; batch classifier loss: 0.407672\n",
      "epoch 78; iter: 0; batch classifier loss: 0.388734\n",
      "epoch 78; iter: 200; batch classifier loss: 0.424127\n",
      "epoch 79; iter: 0; batch classifier loss: 0.414817\n",
      "epoch 79; iter: 200; batch classifier loss: 0.404179\n",
      "epoch 80; iter: 0; batch classifier loss: 0.479162\n",
      "epoch 80; iter: 200; batch classifier loss: 0.438998\n",
      "epoch 81; iter: 0; batch classifier loss: 0.377884\n",
      "epoch 81; iter: 200; batch classifier loss: 0.490707\n",
      "epoch 82; iter: 0; batch classifier loss: 0.419878\n",
      "epoch 82; iter: 200; batch classifier loss: 0.418663\n",
      "epoch 83; iter: 0; batch classifier loss: 0.458004\n",
      "epoch 83; iter: 200; batch classifier loss: 0.355952\n",
      "epoch 84; iter: 0; batch classifier loss: 0.369740\n",
      "epoch 84; iter: 200; batch classifier loss: 0.515164\n",
      "epoch 85; iter: 0; batch classifier loss: 0.378592\n",
      "epoch 85; iter: 200; batch classifier loss: 0.473574\n",
      "epoch 86; iter: 0; batch classifier loss: 0.431626\n",
      "epoch 86; iter: 200; batch classifier loss: 0.487739\n",
      "epoch 87; iter: 0; batch classifier loss: 0.457585\n",
      "epoch 87; iter: 200; batch classifier loss: 0.420271\n",
      "epoch 88; iter: 0; batch classifier loss: 0.422996\n",
      "epoch 88; iter: 200; batch classifier loss: 0.303913\n",
      "epoch 89; iter: 0; batch classifier loss: 0.387250\n",
      "epoch 89; iter: 200; batch classifier loss: 0.496133\n",
      "epoch 90; iter: 0; batch classifier loss: 0.379632\n",
      "epoch 90; iter: 200; batch classifier loss: 0.449883\n",
      "epoch 91; iter: 0; batch classifier loss: 0.529285\n",
      "epoch 91; iter: 200; batch classifier loss: 0.470412\n",
      "epoch 92; iter: 0; batch classifier loss: 0.385444\n",
      "epoch 92; iter: 200; batch classifier loss: 0.443942\n",
      "epoch 93; iter: 0; batch classifier loss: 0.409383\n",
      "epoch 93; iter: 200; batch classifier loss: 0.409310\n",
      "epoch 94; iter: 0; batch classifier loss: 0.443033\n",
      "epoch 94; iter: 200; batch classifier loss: 0.417302\n",
      "epoch 95; iter: 0; batch classifier loss: 0.391223\n",
      "epoch 95; iter: 200; batch classifier loss: 0.409177\n",
      "epoch 96; iter: 0; batch classifier loss: 0.447284\n",
      "epoch 96; iter: 200; batch classifier loss: 0.366089\n",
      "epoch 97; iter: 0; batch classifier loss: 0.431445\n",
      "epoch 97; iter: 200; batch classifier loss: 0.418827\n",
      "epoch 98; iter: 0; batch classifier loss: 0.552621\n",
      "epoch 98; iter: 200; batch classifier loss: 0.452633\n",
      "epoch 99; iter: 0; batch classifier loss: 0.464722\n",
      "epoch 99; iter: 200; batch classifier loss: 0.459123\n"
     ]
    },
    {
     "data": {
      "text/plain": "<aif360.algorithms.inprocessing.adversarial_debiasing_dnn5.AdversarialDebiasingDnn5 at 0x204b912f488>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plain_model.fit(dataset_orig_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the plain model to test data\n",
    "dataset_nodebiasing_train = plain_model.predict(dataset_orig_train)\n",
    "dataset_nodebiasing_test = plain_model.predict(dataset_orig_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Plain model - without debiasing - dataset metrics"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Difference in mean outcomes between unprivileged and privileged groups = 0.000000\n",
      "Test set: Difference in mean outcomes between unprivileged and privileged groups = 0.000000\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Plain model - without debiasing - classification metrics"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Classification accuracy = 0.234722\n",
      "Test set: Balanced classification accuracy = 0.500000\n",
      "Test set: Disparate impact = 1.000000\n",
      "Test set: Equal opportunity difference = 0.000000\n",
      "Test set: Average odds difference = 0.000000\n",
      "Test set: Theil_index = 0.032674\n"
     ]
    }
   ],
   "source": [
    "# Metrics for the dataset from plain model (without debiasing)\n",
    "display(Markdown(\"#### Plain model - without debiasing - dataset metrics\"))\n",
    "metric_dataset_nodebiasing_train = BinaryLabelDatasetMetric(dataset_nodebiasing_train, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "\n",
    "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_nodebiasing_train.mean_difference())\n",
    "\n",
    "metric_dataset_nodebiasing_test = BinaryLabelDatasetMetric(dataset_nodebiasing_test, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "\n",
    "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_nodebiasing_test.mean_difference())\n",
    "\n",
    "display(Markdown(\"#### Plain model - without debiasing - classification metrics\"))\n",
    "classified_metric_nodebiasing_test = ClassificationMetric(dataset_orig_test, \n",
    "                                                 dataset_nodebiasing_test,\n",
    "                                                 unprivileged_groups=unprivileged_groups,\n",
    "                                                 privileged_groups=privileged_groups)\n",
    "print(\"Test set: Classification accuracy = %f\" % classified_metric_nodebiasing_test.accuracy())\n",
    "TPR = classified_metric_nodebiasing_test.true_positive_rate()\n",
    "TNR = classified_metric_nodebiasing_test.true_negative_rate()\n",
    "bal_acc_nodebiasing_test = 0.5*(TPR+TNR)\n",
    "print(\"Test set: Balanced classification accuracy = %f\" % bal_acc_nodebiasing_test)\n",
    "print(\"Test set: Disparate impact = %f\" % classified_metric_nodebiasing_test.disparate_impact())\n",
    "print(\"Test set: Equal opportunity difference = %f\" % classified_metric_nodebiasing_test.equal_opportunity_difference())\n",
    "print(\"Test set: Average odds difference = %f\" % classified_metric_nodebiasing_test.average_odds_difference())\n",
    "print(\"Test set: Theil_index = %f\" % classified_metric_nodebiasing_test.theil_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply in-processing algorithm based on adversarial learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn parameters with debias set to True\n",
    "debiased_model = AdversarialDebiasingDnn5(privileged_groups = privileged_groups,\n",
    "                          unprivileged_groups = unprivileged_groups,\n",
    "                          scope_name='debiased_classifier',\n",
    "                          debias=True,\n",
    "                          sess=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\PycharmProjects\\AIF360\\aif360\\algorithms\\inprocessing\\adversarial_debiasing_dnn5.py:79: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\PycharmProjects\\AIF360\\aif360\\algorithms\\inprocessing\\adversarial_debiasing_dnn5.py:79: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 0.693334; batch adversarial loss: 0.673883\n",
      "epoch 0; iter: 200; batch classifier loss: 0.487157; batch adversarial loss: 0.689070\n",
      "epoch 1; iter: 0; batch classifier loss: 0.477019; batch adversarial loss: 0.666642\n",
      "epoch 1; iter: 200; batch classifier loss: 0.493647; batch adversarial loss: 0.663019\n",
      "epoch 2; iter: 0; batch classifier loss: 0.550235; batch adversarial loss: 0.646801\n",
      "epoch 2; iter: 200; batch classifier loss: 0.424443; batch adversarial loss: 0.627628\n",
      "epoch 3; iter: 0; batch classifier loss: 0.383093; batch adversarial loss: 0.669071\n",
      "epoch 3; iter: 200; batch classifier loss: 0.449101; batch adversarial loss: 0.576407\n",
      "epoch 4; iter: 0; batch classifier loss: 0.467868; batch adversarial loss: 0.642048\n",
      "epoch 4; iter: 200; batch classifier loss: 0.461415; batch adversarial loss: 0.608070\n",
      "epoch 5; iter: 0; batch classifier loss: 0.460204; batch adversarial loss: 0.639730\n",
      "epoch 5; iter: 200; batch classifier loss: 0.477885; batch adversarial loss: 0.565614\n",
      "epoch 6; iter: 0; batch classifier loss: 0.462498; batch adversarial loss: 0.610482\n",
      "epoch 6; iter: 200; batch classifier loss: 0.510658; batch adversarial loss: 0.643146\n",
      "epoch 7; iter: 0; batch classifier loss: 0.372924; batch adversarial loss: 0.595361\n",
      "epoch 7; iter: 200; batch classifier loss: 0.432472; batch adversarial loss: 0.634223\n",
      "epoch 8; iter: 0; batch classifier loss: 0.435883; batch adversarial loss: 0.615687\n",
      "epoch 8; iter: 200; batch classifier loss: 0.387383; batch adversarial loss: 0.650803\n",
      "epoch 9; iter: 0; batch classifier loss: 0.476149; batch adversarial loss: 0.617725\n",
      "epoch 9; iter: 200; batch classifier loss: 0.399859; batch adversarial loss: 0.570237\n",
      "epoch 10; iter: 0; batch classifier loss: 0.436345; batch adversarial loss: 0.620772\n",
      "epoch 10; iter: 200; batch classifier loss: 0.563074; batch adversarial loss: 0.624918\n",
      "epoch 11; iter: 0; batch classifier loss: 0.380138; batch adversarial loss: 0.664502\n",
      "epoch 11; iter: 200; batch classifier loss: 0.397055; batch adversarial loss: 0.695275\n",
      "epoch 12; iter: 0; batch classifier loss: 0.437794; batch adversarial loss: 0.556883\n",
      "epoch 12; iter: 200; batch classifier loss: 0.471895; batch adversarial loss: 0.589759\n",
      "epoch 13; iter: 0; batch classifier loss: 0.496910; batch adversarial loss: 0.648675\n",
      "epoch 13; iter: 200; batch classifier loss: 0.368798; batch adversarial loss: 0.614984\n",
      "epoch 14; iter: 0; batch classifier loss: 0.389005; batch adversarial loss: 0.625820\n",
      "epoch 14; iter: 200; batch classifier loss: 0.372452; batch adversarial loss: 0.613233\n",
      "epoch 15; iter: 0; batch classifier loss: 0.387827; batch adversarial loss: 0.675084\n",
      "epoch 15; iter: 200; batch classifier loss: 0.398831; batch adversarial loss: 0.669509\n",
      "epoch 16; iter: 0; batch classifier loss: 0.333332; batch adversarial loss: 0.604194\n",
      "epoch 16; iter: 200; batch classifier loss: 0.494406; batch adversarial loss: 0.596760\n",
      "epoch 17; iter: 0; batch classifier loss: 0.452872; batch adversarial loss: 0.613315\n",
      "epoch 17; iter: 200; batch classifier loss: 0.467631; batch adversarial loss: 0.731639\n",
      "epoch 18; iter: 0; batch classifier loss: 0.422142; batch adversarial loss: 0.620801\n",
      "epoch 18; iter: 200; batch classifier loss: 0.452736; batch adversarial loss: 0.575273\n",
      "epoch 19; iter: 0; batch classifier loss: 0.426479; batch adversarial loss: 0.649529\n",
      "epoch 19; iter: 200; batch classifier loss: 0.487641; batch adversarial loss: 0.663132\n",
      "epoch 20; iter: 0; batch classifier loss: 0.410712; batch adversarial loss: 0.631641\n",
      "epoch 20; iter: 200; batch classifier loss: 0.437746; batch adversarial loss: 0.613322\n",
      "epoch 21; iter: 0; batch classifier loss: 0.465223; batch adversarial loss: 0.644596\n",
      "epoch 21; iter: 200; batch classifier loss: 0.377833; batch adversarial loss: 0.587100\n",
      "epoch 22; iter: 0; batch classifier loss: 0.463743; batch adversarial loss: 0.599951\n",
      "epoch 22; iter: 200; batch classifier loss: 0.396458; batch adversarial loss: 0.629024\n",
      "epoch 23; iter: 0; batch classifier loss: 0.424838; batch adversarial loss: 0.609726\n",
      "epoch 23; iter: 200; batch classifier loss: 0.461952; batch adversarial loss: 0.645959\n",
      "epoch 24; iter: 0; batch classifier loss: 0.457090; batch adversarial loss: 0.644418\n",
      "epoch 24; iter: 200; batch classifier loss: 0.390035; batch adversarial loss: 0.609788\n",
      "epoch 25; iter: 0; batch classifier loss: 0.345870; batch adversarial loss: 0.685205\n",
      "epoch 25; iter: 200; batch classifier loss: 0.458399; batch adversarial loss: 0.596867\n",
      "epoch 26; iter: 0; batch classifier loss: 0.429483; batch adversarial loss: 0.659780\n",
      "epoch 26; iter: 200; batch classifier loss: 0.419432; batch adversarial loss: 0.636588\n",
      "epoch 27; iter: 0; batch classifier loss: 0.398100; batch adversarial loss: 0.585102\n",
      "epoch 27; iter: 200; batch classifier loss: 0.385585; batch adversarial loss: 0.642889\n",
      "epoch 28; iter: 0; batch classifier loss: 0.442762; batch adversarial loss: 0.629504\n",
      "epoch 28; iter: 200; batch classifier loss: 0.365847; batch adversarial loss: 0.664926\n",
      "epoch 29; iter: 0; batch classifier loss: 0.457370; batch adversarial loss: 0.539342\n",
      "epoch 29; iter: 200; batch classifier loss: 0.483981; batch adversarial loss: 0.589216\n",
      "epoch 30; iter: 0; batch classifier loss: 0.404558; batch adversarial loss: 0.595403\n",
      "epoch 30; iter: 200; batch classifier loss: 0.454858; batch adversarial loss: 0.616995\n",
      "epoch 31; iter: 0; batch classifier loss: 0.382159; batch adversarial loss: 0.604710\n",
      "epoch 31; iter: 200; batch classifier loss: 0.401824; batch adversarial loss: 0.581200\n",
      "epoch 32; iter: 0; batch classifier loss: 0.471999; batch adversarial loss: 0.587837\n",
      "epoch 32; iter: 200; batch classifier loss: 0.523244; batch adversarial loss: 0.646913\n",
      "epoch 33; iter: 0; batch classifier loss: 0.518670; batch adversarial loss: 0.598872\n",
      "epoch 33; iter: 200; batch classifier loss: 0.497306; batch adversarial loss: 0.594682\n",
      "epoch 34; iter: 0; batch classifier loss: 0.417630; batch adversarial loss: 0.621112\n",
      "epoch 34; iter: 200; batch classifier loss: 0.396842; batch adversarial loss: 0.629645\n",
      "epoch 35; iter: 0; batch classifier loss: 0.388142; batch adversarial loss: 0.639475\n",
      "epoch 35; iter: 200; batch classifier loss: 0.400925; batch adversarial loss: 0.618869\n",
      "epoch 36; iter: 0; batch classifier loss: 0.409126; batch adversarial loss: 0.640536\n",
      "epoch 36; iter: 200; batch classifier loss: 0.377408; batch adversarial loss: 0.683649\n",
      "epoch 37; iter: 0; batch classifier loss: 0.463271; batch adversarial loss: 0.612439\n",
      "epoch 37; iter: 200; batch classifier loss: 0.482671; batch adversarial loss: 0.627472\n",
      "epoch 38; iter: 0; batch classifier loss: 0.403778; batch adversarial loss: 0.630087\n",
      "epoch 38; iter: 200; batch classifier loss: 0.525720; batch adversarial loss: 0.611608\n",
      "epoch 39; iter: 0; batch classifier loss: 0.492442; batch adversarial loss: 0.613896\n",
      "epoch 39; iter: 200; batch classifier loss: 0.433803; batch adversarial loss: 0.548522\n",
      "epoch 40; iter: 0; batch classifier loss: 0.458559; batch adversarial loss: 0.604011\n",
      "epoch 40; iter: 200; batch classifier loss: 0.413851; batch adversarial loss: 0.598954\n",
      "epoch 41; iter: 0; batch classifier loss: 0.453469; batch adversarial loss: 0.579599\n",
      "epoch 41; iter: 200; batch classifier loss: 0.448532; batch adversarial loss: 0.569081\n",
      "epoch 42; iter: 0; batch classifier loss: 0.382665; batch adversarial loss: 0.654492\n",
      "epoch 42; iter: 200; batch classifier loss: 0.463476; batch adversarial loss: 0.610979\n",
      "epoch 43; iter: 0; batch classifier loss: 0.363039; batch adversarial loss: 0.654013\n",
      "epoch 43; iter: 200; batch classifier loss: 0.365340; batch adversarial loss: 0.627046\n",
      "epoch 44; iter: 0; batch classifier loss: 0.453817; batch adversarial loss: 0.616724\n",
      "epoch 44; iter: 200; batch classifier loss: 0.412363; batch adversarial loss: 0.642423\n",
      "epoch 45; iter: 0; batch classifier loss: 0.471551; batch adversarial loss: 0.665267\n",
      "epoch 45; iter: 200; batch classifier loss: 0.534639; batch adversarial loss: 0.701237\n",
      "epoch 46; iter: 0; batch classifier loss: 0.503457; batch adversarial loss: 0.598118\n",
      "epoch 46; iter: 200; batch classifier loss: 0.535396; batch adversarial loss: 0.622823\n",
      "epoch 47; iter: 0; batch classifier loss: 0.434054; batch adversarial loss: 0.633731\n",
      "epoch 47; iter: 200; batch classifier loss: 0.432528; batch adversarial loss: 0.585791\n",
      "epoch 48; iter: 0; batch classifier loss: 0.451949; batch adversarial loss: 0.582069\n",
      "epoch 48; iter: 200; batch classifier loss: 0.382337; batch adversarial loss: 0.635904\n",
      "epoch 49; iter: 0; batch classifier loss: 0.456416; batch adversarial loss: 0.650367\n",
      "epoch 49; iter: 200; batch classifier loss: 0.488799; batch adversarial loss: 0.651251\n",
      "epoch 50; iter: 0; batch classifier loss: 0.559545; batch adversarial loss: 0.572229\n",
      "epoch 50; iter: 200; batch classifier loss: 0.513972; batch adversarial loss: 0.584347\n",
      "epoch 51; iter: 0; batch classifier loss: 0.396555; batch adversarial loss: 0.642636\n",
      "epoch 51; iter: 200; batch classifier loss: 0.484942; batch adversarial loss: 0.568756\n",
      "epoch 52; iter: 0; batch classifier loss: 0.459888; batch adversarial loss: 0.668720\n",
      "epoch 52; iter: 200; batch classifier loss: 0.413433; batch adversarial loss: 0.632922\n",
      "epoch 53; iter: 0; batch classifier loss: 0.301256; batch adversarial loss: 0.615175\n",
      "epoch 53; iter: 200; batch classifier loss: 0.385151; batch adversarial loss: 0.551092\n",
      "epoch 54; iter: 0; batch classifier loss: 0.388900; batch adversarial loss: 0.604346\n",
      "epoch 54; iter: 200; batch classifier loss: 0.450712; batch adversarial loss: 0.691543\n",
      "epoch 55; iter: 0; batch classifier loss: 0.382043; batch adversarial loss: 0.641082\n",
      "epoch 55; iter: 200; batch classifier loss: 0.431167; batch adversarial loss: 0.578586\n",
      "epoch 56; iter: 0; batch classifier loss: 0.418273; batch adversarial loss: 0.630683\n",
      "epoch 56; iter: 200; batch classifier loss: 0.388185; batch adversarial loss: 0.573191\n",
      "epoch 57; iter: 0; batch classifier loss: 0.441883; batch adversarial loss: 0.644276\n",
      "epoch 57; iter: 200; batch classifier loss: 0.512168; batch adversarial loss: 0.607797\n",
      "epoch 58; iter: 0; batch classifier loss: 0.365474; batch adversarial loss: 0.599217\n",
      "epoch 58; iter: 200; batch classifier loss: 0.452265; batch adversarial loss: 0.557996\n",
      "epoch 59; iter: 0; batch classifier loss: 0.337374; batch adversarial loss: 0.613827\n",
      "epoch 59; iter: 200; batch classifier loss: 0.429858; batch adversarial loss: 0.605284\n",
      "epoch 60; iter: 0; batch classifier loss: 0.409037; batch adversarial loss: 0.606797\n",
      "epoch 60; iter: 200; batch classifier loss: 0.447047; batch adversarial loss: 0.646318\n",
      "epoch 61; iter: 0; batch classifier loss: 0.496960; batch adversarial loss: 0.560645\n",
      "epoch 61; iter: 200; batch classifier loss: 0.445645; batch adversarial loss: 0.601411\n",
      "epoch 62; iter: 0; batch classifier loss: 0.349048; batch adversarial loss: 0.560244\n",
      "epoch 62; iter: 200; batch classifier loss: 0.415634; batch adversarial loss: 0.539749\n",
      "epoch 63; iter: 0; batch classifier loss: 0.414099; batch adversarial loss: 0.666725\n",
      "epoch 63; iter: 200; batch classifier loss: 0.419431; batch adversarial loss: 0.641940\n",
      "epoch 64; iter: 0; batch classifier loss: 0.393765; batch adversarial loss: 0.612248\n",
      "epoch 64; iter: 200; batch classifier loss: 0.406121; batch adversarial loss: 0.572870\n",
      "epoch 65; iter: 0; batch classifier loss: 0.408548; batch adversarial loss: 0.657297\n",
      "epoch 65; iter: 200; batch classifier loss: 0.382813; batch adversarial loss: 0.580595\n",
      "epoch 66; iter: 0; batch classifier loss: 0.471290; batch adversarial loss: 0.585742\n",
      "epoch 66; iter: 200; batch classifier loss: 0.414276; batch adversarial loss: 0.589066\n",
      "epoch 67; iter: 0; batch classifier loss: 0.427463; batch adversarial loss: 0.654399\n",
      "epoch 67; iter: 200; batch classifier loss: 0.403311; batch adversarial loss: 0.623293\n",
      "epoch 68; iter: 0; batch classifier loss: 0.455910; batch adversarial loss: 0.587084\n",
      "epoch 68; iter: 200; batch classifier loss: 0.417912; batch adversarial loss: 0.580383\n",
      "epoch 69; iter: 0; batch classifier loss: 0.363642; batch adversarial loss: 0.618227\n",
      "epoch 69; iter: 200; batch classifier loss: 0.479700; batch adversarial loss: 0.572084\n",
      "epoch 70; iter: 0; batch classifier loss: 0.546857; batch adversarial loss: 0.588357\n",
      "epoch 70; iter: 200; batch classifier loss: 0.445080; batch adversarial loss: 0.606802\n",
      "epoch 71; iter: 0; batch classifier loss: 0.399064; batch adversarial loss: 0.662341\n",
      "epoch 71; iter: 200; batch classifier loss: 0.345773; batch adversarial loss: 0.659364\n",
      "epoch 72; iter: 0; batch classifier loss: 0.412134; batch adversarial loss: 0.576771\n",
      "epoch 72; iter: 200; batch classifier loss: 0.436380; batch adversarial loss: 0.672896\n",
      "epoch 73; iter: 0; batch classifier loss: 0.467722; batch adversarial loss: 0.617787\n",
      "epoch 73; iter: 200; batch classifier loss: 0.464325; batch adversarial loss: 0.583377\n",
      "epoch 74; iter: 0; batch classifier loss: 0.465740; batch adversarial loss: 0.598381\n",
      "epoch 74; iter: 200; batch classifier loss: 0.501595; batch adversarial loss: 0.598274\n",
      "epoch 75; iter: 0; batch classifier loss: 0.406036; batch adversarial loss: 0.627648\n",
      "epoch 75; iter: 200; batch classifier loss: 0.442017; batch adversarial loss: 0.591774\n",
      "epoch 76; iter: 0; batch classifier loss: 0.374215; batch adversarial loss: 0.594812\n",
      "epoch 76; iter: 200; batch classifier loss: 0.377268; batch adversarial loss: 0.573564\n",
      "epoch 77; iter: 0; batch classifier loss: 0.383344; batch adversarial loss: 0.663682\n",
      "epoch 77; iter: 200; batch classifier loss: 0.398363; batch adversarial loss: 0.680031\n",
      "epoch 78; iter: 0; batch classifier loss: 0.485438; batch adversarial loss: 0.616897\n",
      "epoch 78; iter: 200; batch classifier loss: 0.367546; batch adversarial loss: 0.637048\n",
      "epoch 79; iter: 0; batch classifier loss: 0.444905; batch adversarial loss: 0.604155\n",
      "epoch 79; iter: 200; batch classifier loss: 0.449376; batch adversarial loss: 0.645278\n",
      "epoch 80; iter: 0; batch classifier loss: 0.448958; batch adversarial loss: 0.580988\n",
      "epoch 80; iter: 200; batch classifier loss: 0.393160; batch adversarial loss: 0.582346\n",
      "epoch 81; iter: 0; batch classifier loss: 0.399902; batch adversarial loss: 0.579708\n",
      "epoch 81; iter: 200; batch classifier loss: 0.497864; batch adversarial loss: 0.561532\n",
      "epoch 82; iter: 0; batch classifier loss: 0.511814; batch adversarial loss: 0.606127\n",
      "epoch 82; iter: 200; batch classifier loss: 0.437616; batch adversarial loss: 0.625483\n",
      "epoch 83; iter: 0; batch classifier loss: 0.417310; batch adversarial loss: 0.608543\n",
      "epoch 83; iter: 200; batch classifier loss: 0.476242; batch adversarial loss: 0.581453\n",
      "epoch 84; iter: 0; batch classifier loss: 0.364580; batch adversarial loss: 0.669028\n",
      "epoch 84; iter: 200; batch classifier loss: 0.542399; batch adversarial loss: 0.625938\n",
      "epoch 85; iter: 0; batch classifier loss: 0.497824; batch adversarial loss: 0.613108\n",
      "epoch 85; iter: 200; batch classifier loss: 0.425214; batch adversarial loss: 0.631355\n",
      "epoch 86; iter: 0; batch classifier loss: 0.561774; batch adversarial loss: 0.640520\n",
      "epoch 86; iter: 200; batch classifier loss: 0.469330; batch adversarial loss: 0.609426\n",
      "epoch 87; iter: 0; batch classifier loss: 0.413200; batch adversarial loss: 0.627638\n",
      "epoch 87; iter: 200; batch classifier loss: 0.366478; batch adversarial loss: 0.619443\n",
      "epoch 88; iter: 0; batch classifier loss: 0.473618; batch adversarial loss: 0.635955\n",
      "epoch 88; iter: 200; batch classifier loss: 0.414554; batch adversarial loss: 0.634368\n",
      "epoch 89; iter: 0; batch classifier loss: 0.439440; batch adversarial loss: 0.530228\n",
      "epoch 89; iter: 200; batch classifier loss: 0.519630; batch adversarial loss: 0.623293\n",
      "epoch 90; iter: 0; batch classifier loss: 0.445250; batch adversarial loss: 0.592747\n",
      "epoch 90; iter: 200; batch classifier loss: 0.369751; batch adversarial loss: 0.622406\n",
      "epoch 91; iter: 0; batch classifier loss: 0.366535; batch adversarial loss: 0.654559\n",
      "epoch 91; iter: 200; batch classifier loss: 0.471684; batch adversarial loss: 0.650516\n",
      "epoch 92; iter: 0; batch classifier loss: 0.429679; batch adversarial loss: 0.578314\n",
      "epoch 92; iter: 200; batch classifier loss: 0.452831; batch adversarial loss: 0.592239\n",
      "epoch 93; iter: 0; batch classifier loss: 0.384830; batch adversarial loss: 0.605424\n",
      "epoch 93; iter: 200; batch classifier loss: 0.490173; batch adversarial loss: 0.562133\n",
      "epoch 94; iter: 0; batch classifier loss: 0.441087; batch adversarial loss: 0.538703\n",
      "epoch 94; iter: 200; batch classifier loss: 0.370832; batch adversarial loss: 0.614445\n",
      "epoch 95; iter: 0; batch classifier loss: 0.432110; batch adversarial loss: 0.677969\n",
      "epoch 95; iter: 200; batch classifier loss: 0.390303; batch adversarial loss: 0.637808\n",
      "epoch 96; iter: 0; batch classifier loss: 0.414228; batch adversarial loss: 0.602886\n",
      "epoch 96; iter: 200; batch classifier loss: 0.393239; batch adversarial loss: 0.566417\n",
      "epoch 97; iter: 0; batch classifier loss: 0.402785; batch adversarial loss: 0.615197\n",
      "epoch 97; iter: 200; batch classifier loss: 0.446341; batch adversarial loss: 0.606882\n",
      "epoch 98; iter: 0; batch classifier loss: 0.462442; batch adversarial loss: 0.579605\n",
      "epoch 98; iter: 200; batch classifier loss: 0.510280; batch adversarial loss: 0.618213\n",
      "epoch 99; iter: 0; batch classifier loss: 0.421809; batch adversarial loss: 0.636106\n",
      "epoch 99; iter: 200; batch classifier loss: 0.587953; batch adversarial loss: 0.608109\n"
     ]
    },
    {
     "data": {
      "text/plain": "<aif360.algorithms.inprocessing.adversarial_debiasing_dnn5.AdversarialDebiasingDnn5 at 0x204ba88b348>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debiased_model.fit(dataset_orig_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the plain model to test data\n",
    "dataset_debiasing_train = debiased_model.predict(dataset_orig_train)\n",
    "dataset_debiasing_test = debiased_model.predict(dataset_orig_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Plain model - without debiasing - dataset metrics"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Difference in mean outcomes between unprivileged and privileged groups = 0.000000\n",
      "Test set: Difference in mean outcomes between unprivileged and privileged groups = 0.000000\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Model - with debiasing - dataset metrics"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Difference in mean outcomes between unprivileged and privileged groups = 0.000000\n",
      "Test set: Difference in mean outcomes between unprivileged and privileged groups = 0.000000\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Plain model - without debiasing - classification metrics"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Classification accuracy = 0.234722\n",
      "Test set: Balanced classification accuracy = 0.500000\n",
      "Test set: Disparate impact = 1.000000\n",
      "Test set: Equal opportunity difference = 0.000000\n",
      "Test set: Average odds difference = 0.000000\n",
      "Test set: Theil_index = 0.032674\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Model - with debiasing - classification metrics"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Classification accuracy = 0.234722\n",
      "Test set: Balanced classification accuracy = 0.500000\n",
      "Test set: Disparate impact = 1.000000\n",
      "Test set: Equal opportunity difference = 0.000000\n",
      "Test set: Average odds difference = 0.000000\n",
      "Test set: Theil_index = 0.032674\n"
     ]
    }
   ],
   "source": [
    "# Metrics for the dataset from plain model (without debiasing)\n",
    "display(Markdown(\"#### Plain model - without debiasing - dataset metrics\"))\n",
    "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_nodebiasing_train.mean_difference())\n",
    "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_nodebiasing_test.mean_difference())\n",
    "\n",
    "# Metrics for the dataset from model with debiasing\n",
    "display(Markdown(\"#### Model - with debiasing - dataset metrics\"))\n",
    "metric_dataset_debiasing_train = BinaryLabelDatasetMetric(dataset_debiasing_train, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "\n",
    "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_debiasing_train.mean_difference())\n",
    "\n",
    "metric_dataset_debiasing_test = BinaryLabelDatasetMetric(dataset_debiasing_test, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "\n",
    "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_debiasing_test.mean_difference())\n",
    "\n",
    "\n",
    "\n",
    "display(Markdown(\"#### Plain model - without debiasing - classification metrics\"))\n",
    "print(\"Test set: Classification accuracy = %f\" % classified_metric_nodebiasing_test.accuracy())\n",
    "TPR = classified_metric_nodebiasing_test.true_positive_rate()\n",
    "TNR = classified_metric_nodebiasing_test.true_negative_rate()\n",
    "bal_acc_nodebiasing_test = 0.5*(TPR+TNR)\n",
    "print(\"Test set: Balanced classification accuracy = %f\" % bal_acc_nodebiasing_test)\n",
    "print(\"Test set: Disparate impact = %f\" % classified_metric_nodebiasing_test.disparate_impact())\n",
    "print(\"Test set: Equal opportunity difference = %f\" % classified_metric_nodebiasing_test.equal_opportunity_difference())\n",
    "print(\"Test set: Average odds difference = %f\" % classified_metric_nodebiasing_test.average_odds_difference())\n",
    "print(\"Test set: Theil_index = %f\" % classified_metric_nodebiasing_test.theil_index())\n",
    "\n",
    "\n",
    "\n",
    "display(Markdown(\"#### Model - with debiasing - classification metrics\"))\n",
    "classified_metric_debiasing_test = ClassificationMetric(dataset_orig_test, \n",
    "                                                 dataset_debiasing_test,\n",
    "                                                 unprivileged_groups=unprivileged_groups,\n",
    "                                                 privileged_groups=privileged_groups)\n",
    "print(\"Test set: Classification accuracy = %f\" % classified_metric_debiasing_test.accuracy())\n",
    "TPR = classified_metric_debiasing_test.true_positive_rate()\n",
    "TNR = classified_metric_debiasing_test.true_negative_rate()\n",
    "bal_acc_debiasing_test = 0.5*(TPR+TNR)\n",
    "print(\"Test set: Balanced classification accuracy = %f\" % bal_acc_debiasing_test)\n",
    "print(\"Test set: Disparate impact = %f\" % classified_metric_debiasing_test.disparate_impact())\n",
    "print(\"Test set: Equal opportunity difference = %f\" % classified_metric_debiasing_test.equal_opportunity_difference())\n",
    "print(\"Test set: Average odds difference = %f\" % classified_metric_debiasing_test.average_odds_difference())\n",
    "print(\"Test set: Theil_index = %f\" % classified_metric_debiasing_test.theil_index())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "\n",
    "    References:\n",
    "    [1] B. H. Zhang, B. Lemoine, and M. Mitchell, \"Mitigating UnwantedBiases with Adversarial Learning,\" \n",
    "    AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society, 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}