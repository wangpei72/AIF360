{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# Load all necessary packages\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.datasets import AdultDataset, GermanDataset, CompasDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from aif360.metrics.utils import compute_boolean_conditioning_vector\n",
    "\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions import load_preproc_data_adult, load_preproc_data_compas, load_preproc_data_german\n",
    "\n",
    "from aif360.algorithms.inprocessing.adversarial_debiasing_dnn5 import AdversarialDebiasingDnn5\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataset and split into train and test\n",
    "dataset_orig = load_preproc_data_adult()\n",
    "\n",
    "privileged_groups = [{'sex': 1}]\n",
    "unprivileged_groups = [{'sex': 0}]\n",
    "\n",
    "dataset_orig_train, dataset_orig_test = dataset_orig.split([0.8], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Training Dataset shape"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39073, 18)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Favorable and unfavorable labels"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Protected attribute names"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sex', 'race']\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Privileged and unprivileged protected attribute values"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1.]), array([1.])] [array([0.]), array([0.])]\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Dataset feature names"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['race', 'sex', 'Age (decade)=10', 'Age (decade)=20', 'Age (decade)=30', 'Age (decade)=40', 'Age (decade)=50', 'Age (decade)=60', 'Age (decade)=>=70', 'Education Years=6', 'Education Years=7', 'Education Years=8', 'Education Years=9', 'Education Years=10', 'Education Years=11', 'Education Years=12', 'Education Years=<6', 'Education Years=>12']\n"
     ]
    }
   ],
   "source": [
    "# print out some labels, names, etc.\n",
    "display(Markdown(\"#### Training Dataset shape\"))\n",
    "print(dataset_orig_train.features.shape)\n",
    "display(Markdown(\"#### Favorable and unfavorable labels\"))\n",
    "print(dataset_orig_train.favorable_label, dataset_orig_train.unfavorable_label)\n",
    "display(Markdown(\"#### Protected attribute names\"))\n",
    "print(dataset_orig_train.protected_attribute_names)\n",
    "display(Markdown(\"#### Privileged and unprivileged protected attribute values\"))\n",
    "print(dataset_orig_train.privileged_protected_attributes, \n",
    "      dataset_orig_train.unprivileged_protected_attributes)\n",
    "display(Markdown(\"#### Dataset feature names\"))\n",
    "print(dataset_orig_train.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metric for original training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Original training dataset"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Difference in mean outcomes between unprivileged and privileged groups = -0.193634\n",
      "Test set: Difference in mean outcomes between unprivileged and privileged groups = -0.198030\n"
     ]
    }
   ],
   "source": [
    "# Metric for the original dataset\n",
    "metric_orig_train = BinaryLabelDatasetMetric(dataset_orig_train, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Original training dataset\"))\n",
    "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())\n",
    "metric_orig_test = BinaryLabelDatasetMetric(dataset_orig_test, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_test.mean_difference())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Scaled dataset - Verify that the scaling does not affect the group label statistics"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Difference in mean outcomes between unprivileged and privileged groups = -0.193634\n",
      "Test set: Difference in mean outcomes between unprivileged and privileged groups = -0.198030\n"
     ]
    }
   ],
   "source": [
    "min_max_scaler = MaxAbsScaler()\n",
    "dataset_orig_train.features = min_max_scaler.fit_transform(dataset_orig_train.features)\n",
    "dataset_orig_test.features = min_max_scaler.transform(dataset_orig_test.features)\n",
    "metric_scaled_train = BinaryLabelDatasetMetric(dataset_orig_train, \n",
    "                             unprivileged_groups=unprivileged_groups,\n",
    "                             privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Scaled dataset - Verify that the scaling does not affect the group label statistics\"))\n",
    "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_scaled_train.mean_difference())\n",
    "metric_scaled_test = BinaryLabelDatasetMetric(dataset_orig_test, \n",
    "                             unprivileged_groups=unprivileged_groups,\n",
    "                             privileged_groups=privileged_groups)\n",
    "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_scaled_test.mean_difference())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn plan classifier without debiasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load post-processing algorithm that equalizes the odds\n",
    "# Learn parameters with debias set to False\n",
    "sess = tf.Session()\n",
    "plain_model = AdversarialDebiasingDnn5(privileged_groups = privileged_groups,\n",
    "                          unprivileged_groups = unprivileged_groups,\n",
    "                          scope_name='plain_classifier',\n",
    "                          debias=False,\n",
    "                          sess=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\wp\\PycharmProjects\\AIF360\\aif360\\algorithms\\inprocessing\\adversarial_debiasing_dnn5.py:134: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\wp\\PycharmProjects\\AIF360\\aif360\\algorithms\\inprocessing\\adversarial_debiasing_dnn5.py:143: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\wp\\PycharmProjects\\AIF360\\aif360\\load_model\\layer.py:23: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\wp\\PycharmProjects\\AIF360\\aif360\\load_model\\layer.py:25: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From D:\\wp\\PycharmProjects\\AIF360\\aif360\\algorithms\\inprocessing\\adversarial_debiasing_dnn5.py:174: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\wp\\PycharmProjects\\AIF360\\aif360\\algorithms\\inprocessing\\adversarial_debiasing_dnn5.py:176: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\wp\\PycharmProjects\\AIF360\\aif360\\algorithms\\inprocessing\\adversarial_debiasing_dnn5.py:182: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\wp\\PycharmProjects\\AIF360\\aif360\\algorithms\\inprocessing\\adversarial_debiasing_dnn5.py:222: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\wp\\PycharmProjects\\AIF360\\aif360\\algorithms\\inprocessing\\adversarial_debiasing_dnn5.py:223: The name tf.local_variables_initializer is deprecated. Please use tf.compat.v1.local_variables_initializer instead.\n",
      "\n",
      "epoch 0; iter: 0; batch classifier loss: 0.719478\n",
      "epoch 0; iter: 200; batch classifier loss: 0.465207\n",
      "epoch 1; iter: 0; batch classifier loss: 0.472532\n",
      "epoch 1; iter: 200; batch classifier loss: 0.370881\n",
      "epoch 2; iter: 0; batch classifier loss: 0.384909\n",
      "epoch 2; iter: 200; batch classifier loss: 0.393073\n",
      "epoch 3; iter: 0; batch classifier loss: 0.405540\n",
      "epoch 3; iter: 200; batch classifier loss: 0.456807\n",
      "epoch 4; iter: 0; batch classifier loss: 0.429248\n",
      "epoch 4; iter: 200; batch classifier loss: 0.422016\n",
      "epoch 5; iter: 0; batch classifier loss: 0.404516\n",
      "epoch 5; iter: 200; batch classifier loss: 0.353134\n",
      "epoch 6; iter: 0; batch classifier loss: 0.423623\n",
      "epoch 6; iter: 200; batch classifier loss: 0.411595\n",
      "epoch 7; iter: 0; batch classifier loss: 0.449697\n",
      "epoch 7; iter: 200; batch classifier loss: 0.374251\n",
      "epoch 8; iter: 0; batch classifier loss: 0.457756\n",
      "epoch 8; iter: 200; batch classifier loss: 0.357061\n",
      "epoch 9; iter: 0; batch classifier loss: 0.317386\n",
      "epoch 9; iter: 200; batch classifier loss: 0.365206\n",
      "epoch 10; iter: 0; batch classifier loss: 0.492857\n",
      "epoch 10; iter: 200; batch classifier loss: 0.430619\n",
      "epoch 11; iter: 0; batch classifier loss: 0.510566\n",
      "epoch 11; iter: 200; batch classifier loss: 0.388000\n",
      "epoch 12; iter: 0; batch classifier loss: 0.364234\n",
      "epoch 12; iter: 200; batch classifier loss: 0.432046\n",
      "epoch 13; iter: 0; batch classifier loss: 0.362003\n",
      "epoch 13; iter: 200; batch classifier loss: 0.490584\n",
      "epoch 14; iter: 0; batch classifier loss: 0.375481\n",
      "epoch 14; iter: 200; batch classifier loss: 0.454983\n",
      "epoch 15; iter: 0; batch classifier loss: 0.463953\n",
      "epoch 15; iter: 200; batch classifier loss: 0.398335\n",
      "epoch 16; iter: 0; batch classifier loss: 0.396248\n",
      "epoch 16; iter: 200; batch classifier loss: 0.433801\n",
      "epoch 17; iter: 0; batch classifier loss: 0.503739\n",
      "epoch 17; iter: 200; batch classifier loss: 0.421140\n",
      "epoch 18; iter: 0; batch classifier loss: 0.451353\n",
      "epoch 18; iter: 200; batch classifier loss: 0.464490\n",
      "epoch 19; iter: 0; batch classifier loss: 0.430467\n",
      "epoch 19; iter: 200; batch classifier loss: 0.412801\n",
      "epoch 20; iter: 0; batch classifier loss: 0.440182\n",
      "epoch 20; iter: 200; batch classifier loss: 0.377974\n",
      "epoch 21; iter: 0; batch classifier loss: 0.435591\n",
      "epoch 21; iter: 200; batch classifier loss: 0.442498\n",
      "epoch 22; iter: 0; batch classifier loss: 0.387196\n",
      "epoch 22; iter: 200; batch classifier loss: 0.486647\n",
      "epoch 23; iter: 0; batch classifier loss: 0.419967\n",
      "epoch 23; iter: 200; batch classifier loss: 0.412308\n",
      "epoch 24; iter: 0; batch classifier loss: 0.480408\n",
      "epoch 24; iter: 200; batch classifier loss: 0.481896\n",
      "epoch 25; iter: 0; batch classifier loss: 0.559832\n",
      "epoch 25; iter: 200; batch classifier loss: 0.371437\n",
      "epoch 26; iter: 0; batch classifier loss: 0.462770\n",
      "epoch 26; iter: 200; batch classifier loss: 0.405809\n",
      "epoch 27; iter: 0; batch classifier loss: 0.417063\n",
      "epoch 27; iter: 200; batch classifier loss: 0.517224\n",
      "epoch 28; iter: 0; batch classifier loss: 0.403406\n",
      "epoch 28; iter: 200; batch classifier loss: 0.412443\n",
      "epoch 29; iter: 0; batch classifier loss: 0.406086\n",
      "epoch 29; iter: 200; batch classifier loss: 0.395219\n",
      "epoch 30; iter: 0; batch classifier loss: 0.392673\n",
      "epoch 30; iter: 200; batch classifier loss: 0.401337\n",
      "epoch 31; iter: 0; batch classifier loss: 0.331243\n",
      "epoch 31; iter: 200; batch classifier loss: 0.371275\n",
      "epoch 32; iter: 0; batch classifier loss: 0.349189\n",
      "epoch 32; iter: 200; batch classifier loss: 0.463181\n",
      "epoch 33; iter: 0; batch classifier loss: 0.339910\n",
      "epoch 33; iter: 200; batch classifier loss: 0.390731\n",
      "epoch 34; iter: 0; batch classifier loss: 0.522236\n",
      "epoch 34; iter: 200; batch classifier loss: 0.457262\n",
      "epoch 35; iter: 0; batch classifier loss: 0.439647\n",
      "epoch 35; iter: 200; batch classifier loss: 0.354243\n",
      "epoch 36; iter: 0; batch classifier loss: 0.416322\n",
      "epoch 36; iter: 200; batch classifier loss: 0.380147\n",
      "epoch 37; iter: 0; batch classifier loss: 0.493190\n",
      "epoch 37; iter: 200; batch classifier loss: 0.355243\n",
      "epoch 38; iter: 0; batch classifier loss: 0.448644\n",
      "epoch 38; iter: 200; batch classifier loss: 0.372733\n",
      "epoch 39; iter: 0; batch classifier loss: 0.378785\n",
      "epoch 39; iter: 200; batch classifier loss: 0.426970\n",
      "epoch 40; iter: 0; batch classifier loss: 0.449890\n",
      "epoch 40; iter: 200; batch classifier loss: 0.456119\n",
      "epoch 41; iter: 0; batch classifier loss: 0.385120\n",
      "epoch 41; iter: 200; batch classifier loss: 0.347980\n",
      "epoch 42; iter: 0; batch classifier loss: 0.438494\n",
      "epoch 42; iter: 200; batch classifier loss: 0.398878\n",
      "epoch 43; iter: 0; batch classifier loss: 0.367869\n",
      "epoch 43; iter: 200; batch classifier loss: 0.396602\n",
      "epoch 44; iter: 0; batch classifier loss: 0.341909\n",
      "epoch 44; iter: 200; batch classifier loss: 0.473888\n",
      "epoch 45; iter: 0; batch classifier loss: 0.357056\n",
      "epoch 45; iter: 200; batch classifier loss: 0.392067\n",
      "epoch 46; iter: 0; batch classifier loss: 0.479611\n",
      "epoch 46; iter: 200; batch classifier loss: 0.413742\n",
      "epoch 47; iter: 0; batch classifier loss: 0.362009\n",
      "epoch 47; iter: 200; batch classifier loss: 0.432895\n",
      "epoch 48; iter: 0; batch classifier loss: 0.410989\n",
      "epoch 48; iter: 200; batch classifier loss: 0.429573\n",
      "epoch 49; iter: 0; batch classifier loss: 0.409450\n",
      "epoch 49; iter: 200; batch classifier loss: 0.423234\n",
      "epoch 50; iter: 0; batch classifier loss: 0.371905\n",
      "epoch 50; iter: 200; batch classifier loss: 0.451876\n",
      "epoch 51; iter: 0; batch classifier loss: 0.431229\n",
      "epoch 51; iter: 200; batch classifier loss: 0.476214\n",
      "epoch 52; iter: 0; batch classifier loss: 0.393884\n",
      "epoch 52; iter: 200; batch classifier loss: 0.460090\n",
      "epoch 53; iter: 0; batch classifier loss: 0.350645\n",
      "epoch 53; iter: 200; batch classifier loss: 0.355031\n",
      "epoch 54; iter: 0; batch classifier loss: 0.382158\n",
      "epoch 54; iter: 200; batch classifier loss: 0.384274\n",
      "epoch 55; iter: 0; batch classifier loss: 0.363645\n",
      "epoch 55; iter: 200; batch classifier loss: 0.432667\n",
      "epoch 56; iter: 0; batch classifier loss: 0.377491\n",
      "epoch 56; iter: 200; batch classifier loss: 0.419148\n",
      "epoch 57; iter: 0; batch classifier loss: 0.440604\n",
      "epoch 57; iter: 200; batch classifier loss: 0.371776\n",
      "epoch 58; iter: 0; batch classifier loss: 0.420688\n",
      "epoch 58; iter: 200; batch classifier loss: 0.394219\n",
      "epoch 59; iter: 0; batch classifier loss: 0.425922\n",
      "epoch 59; iter: 200; batch classifier loss: 0.492496\n",
      "epoch 60; iter: 0; batch classifier loss: 0.447685\n",
      "epoch 60; iter: 200; batch classifier loss: 0.518695\n",
      "epoch 61; iter: 0; batch classifier loss: 0.418751\n",
      "epoch 61; iter: 200; batch classifier loss: 0.451624\n",
      "epoch 62; iter: 0; batch classifier loss: 0.336325\n",
      "epoch 62; iter: 200; batch classifier loss: 0.403850\n",
      "epoch 63; iter: 0; batch classifier loss: 0.427266\n",
      "epoch 63; iter: 200; batch classifier loss: 0.374148\n",
      "epoch 64; iter: 0; batch classifier loss: 0.319929\n",
      "epoch 64; iter: 200; batch classifier loss: 0.465281\n",
      "epoch 65; iter: 0; batch classifier loss: 0.421567\n",
      "epoch 65; iter: 200; batch classifier loss: 0.507553\n",
      "epoch 66; iter: 0; batch classifier loss: 0.348334\n",
      "epoch 66; iter: 200; batch classifier loss: 0.429053\n",
      "epoch 67; iter: 0; batch classifier loss: 0.403202\n",
      "epoch 67; iter: 200; batch classifier loss: 0.402940\n",
      "epoch 68; iter: 0; batch classifier loss: 0.453975\n",
      "epoch 68; iter: 200; batch classifier loss: 0.499056\n",
      "epoch 69; iter: 0; batch classifier loss: 0.466152\n",
      "epoch 69; iter: 200; batch classifier loss: 0.431328\n",
      "epoch 70; iter: 0; batch classifier loss: 0.421510\n",
      "epoch 70; iter: 200; batch classifier loss: 0.398788\n",
      "epoch 71; iter: 0; batch classifier loss: 0.507488\n",
      "epoch 71; iter: 200; batch classifier loss: 0.430659\n",
      "epoch 72; iter: 0; batch classifier loss: 0.470550\n",
      "epoch 72; iter: 200; batch classifier loss: 0.451808\n",
      "epoch 73; iter: 0; batch classifier loss: 0.362115\n",
      "epoch 73; iter: 200; batch classifier loss: 0.387220\n",
      "epoch 74; iter: 0; batch classifier loss: 0.294031\n",
      "epoch 74; iter: 200; batch classifier loss: 0.337671\n",
      "epoch 75; iter: 0; batch classifier loss: 0.394736\n",
      "epoch 75; iter: 200; batch classifier loss: 0.457868\n",
      "epoch 76; iter: 0; batch classifier loss: 0.456631\n",
      "epoch 76; iter: 200; batch classifier loss: 0.435102\n",
      "epoch 77; iter: 0; batch classifier loss: 0.410809\n",
      "epoch 77; iter: 200; batch classifier loss: 0.426859\n",
      "epoch 78; iter: 0; batch classifier loss: 0.389162\n",
      "epoch 78; iter: 200; batch classifier loss: 0.361375\n",
      "epoch 79; iter: 0; batch classifier loss: 0.477686\n",
      "epoch 79; iter: 200; batch classifier loss: 0.386659\n",
      "epoch 80; iter: 0; batch classifier loss: 0.428555\n",
      "epoch 80; iter: 200; batch classifier loss: 0.416915\n",
      "epoch 81; iter: 0; batch classifier loss: 0.360734\n",
      "epoch 81; iter: 200; batch classifier loss: 0.464603\n",
      "epoch 82; iter: 0; batch classifier loss: 0.434629\n",
      "epoch 82; iter: 200; batch classifier loss: 0.322794\n",
      "epoch 83; iter: 0; batch classifier loss: 0.408561\n",
      "epoch 83; iter: 200; batch classifier loss: 0.375048\n",
      "epoch 84; iter: 0; batch classifier loss: 0.427522\n",
      "epoch 84; iter: 200; batch classifier loss: 0.397153\n",
      "epoch 85; iter: 0; batch classifier loss: 0.506552\n",
      "epoch 85; iter: 200; batch classifier loss: 0.404131\n",
      "epoch 86; iter: 0; batch classifier loss: 0.435631\n",
      "epoch 86; iter: 200; batch classifier loss: 0.343117\n",
      "epoch 87; iter: 0; batch classifier loss: 0.481782\n",
      "epoch 87; iter: 200; batch classifier loss: 0.433171\n",
      "epoch 88; iter: 0; batch classifier loss: 0.422514\n",
      "epoch 88; iter: 200; batch classifier loss: 0.473175\n",
      "epoch 89; iter: 0; batch classifier loss: 0.431595\n",
      "epoch 89; iter: 200; batch classifier loss: 0.415186\n",
      "epoch 90; iter: 0; batch classifier loss: 0.346346\n",
      "epoch 90; iter: 200; batch classifier loss: 0.427176\n",
      "epoch 91; iter: 0; batch classifier loss: 0.414260\n",
      "epoch 91; iter: 200; batch classifier loss: 0.441789\n",
      "epoch 92; iter: 0; batch classifier loss: 0.370970\n",
      "epoch 92; iter: 200; batch classifier loss: 0.370069\n",
      "epoch 93; iter: 0; batch classifier loss: 0.358524\n",
      "epoch 93; iter: 200; batch classifier loss: 0.440315\n",
      "epoch 94; iter: 0; batch classifier loss: 0.366745\n",
      "epoch 94; iter: 200; batch classifier loss: 0.457901\n",
      "epoch 95; iter: 0; batch classifier loss: 0.398339\n",
      "epoch 95; iter: 200; batch classifier loss: 0.386466\n",
      "epoch 96; iter: 0; batch classifier loss: 0.336418\n",
      "epoch 96; iter: 200; batch classifier loss: 0.319960\n",
      "epoch 97; iter: 0; batch classifier loss: 0.388175\n",
      "epoch 97; iter: 200; batch classifier loss: 0.386664\n",
      "epoch 98; iter: 0; batch classifier loss: 0.530905\n",
      "epoch 98; iter: 200; batch classifier loss: 0.383213\n",
      "epoch 99; iter: 0; batch classifier loss: 0.483520\n",
      "epoch 99; iter: 200; batch classifier loss: 0.337738\n",
      "epoch 100; iter: 0; batch classifier loss: 0.422533\n",
      "epoch 100; iter: 200; batch classifier loss: 0.368136\n",
      "epoch 101; iter: 0; batch classifier loss: 0.374320\n",
      "epoch 101; iter: 200; batch classifier loss: 0.380527\n",
      "epoch 102; iter: 0; batch classifier loss: 0.390940\n",
      "epoch 102; iter: 200; batch classifier loss: 0.466595\n",
      "epoch 103; iter: 0; batch classifier loss: 0.439102\n",
      "epoch 103; iter: 200; batch classifier loss: 0.410096\n",
      "epoch 104; iter: 0; batch classifier loss: 0.405552\n",
      "epoch 104; iter: 200; batch classifier loss: 0.385831\n",
      "epoch 105; iter: 0; batch classifier loss: 0.380104\n",
      "epoch 105; iter: 200; batch classifier loss: 0.339635\n",
      "epoch 106; iter: 0; batch classifier loss: 0.442257\n",
      "epoch 106; iter: 200; batch classifier loss: 0.409970\n",
      "epoch 107; iter: 0; batch classifier loss: 0.334663\n",
      "epoch 107; iter: 200; batch classifier loss: 0.401557\n",
      "epoch 108; iter: 0; batch classifier loss: 0.432123\n",
      "epoch 108; iter: 200; batch classifier loss: 0.484709\n",
      "epoch 109; iter: 0; batch classifier loss: 0.438343\n",
      "epoch 109; iter: 200; batch classifier loss: 0.497018\n",
      "epoch 110; iter: 0; batch classifier loss: 0.350323\n",
      "epoch 110; iter: 200; batch classifier loss: 0.373226\n",
      "epoch 111; iter: 0; batch classifier loss: 0.520270\n",
      "epoch 111; iter: 200; batch classifier loss: 0.489472\n",
      "epoch 112; iter: 0; batch classifier loss: 0.397828\n",
      "epoch 112; iter: 200; batch classifier loss: 0.409608\n",
      "epoch 113; iter: 0; batch classifier loss: 0.346994\n",
      "epoch 113; iter: 200; batch classifier loss: 0.403132\n",
      "epoch 114; iter: 0; batch classifier loss: 0.511278\n",
      "epoch 114; iter: 200; batch classifier loss: 0.476194\n",
      "epoch 115; iter: 0; batch classifier loss: 0.307958\n",
      "epoch 115; iter: 200; batch classifier loss: 0.399655\n",
      "epoch 116; iter: 0; batch classifier loss: 0.390274\n",
      "epoch 116; iter: 200; batch classifier loss: 0.394875\n",
      "epoch 117; iter: 0; batch classifier loss: 0.457880\n",
      "epoch 117; iter: 200; batch classifier loss: 0.371357\n",
      "epoch 118; iter: 0; batch classifier loss: 0.483149\n",
      "epoch 118; iter: 200; batch classifier loss: 0.488645\n",
      "epoch 119; iter: 0; batch classifier loss: 0.445740\n",
      "epoch 119; iter: 200; batch classifier loss: 0.400109\n",
      "epoch 120; iter: 0; batch classifier loss: 0.338010\n",
      "epoch 120; iter: 200; batch classifier loss: 0.405102\n",
      "epoch 121; iter: 0; batch classifier loss: 0.395701\n",
      "epoch 121; iter: 200; batch classifier loss: 0.496097\n",
      "epoch 122; iter: 0; batch classifier loss: 0.406860\n",
      "epoch 122; iter: 200; batch classifier loss: 0.401600\n",
      "epoch 123; iter: 0; batch classifier loss: 0.416003\n",
      "epoch 123; iter: 200; batch classifier loss: 0.390160\n",
      "epoch 124; iter: 0; batch classifier loss: 0.417980\n",
      "epoch 124; iter: 200; batch classifier loss: 0.481959\n",
      "epoch 125; iter: 0; batch classifier loss: 0.404351\n",
      "epoch 125; iter: 200; batch classifier loss: 0.541188\n",
      "epoch 126; iter: 0; batch classifier loss: 0.419068\n",
      "epoch 126; iter: 200; batch classifier loss: 0.425183\n",
      "epoch 127; iter: 0; batch classifier loss: 0.423603\n",
      "epoch 127; iter: 200; batch classifier loss: 0.342889\n",
      "epoch 128; iter: 0; batch classifier loss: 0.311084\n",
      "epoch 128; iter: 200; batch classifier loss: 0.436584\n",
      "epoch 129; iter: 0; batch classifier loss: 0.393043\n",
      "epoch 129; iter: 200; batch classifier loss: 0.389151\n",
      "epoch 130; iter: 0; batch classifier loss: 0.370139\n",
      "epoch 130; iter: 200; batch classifier loss: 0.461471\n",
      "epoch 131; iter: 0; batch classifier loss: 0.442304\n",
      "epoch 131; iter: 200; batch classifier loss: 0.376777\n",
      "epoch 132; iter: 0; batch classifier loss: 0.415675\n",
      "epoch 132; iter: 200; batch classifier loss: 0.468213\n",
      "epoch 133; iter: 0; batch classifier loss: 0.367367\n",
      "epoch 133; iter: 200; batch classifier loss: 0.362203\n",
      "epoch 134; iter: 0; batch classifier loss: 0.467728\n",
      "epoch 134; iter: 200; batch classifier loss: 0.445982\n",
      "epoch 135; iter: 0; batch classifier loss: 0.432364\n",
      "epoch 135; iter: 200; batch classifier loss: 0.442894\n",
      "epoch 136; iter: 0; batch classifier loss: 0.409994\n",
      "epoch 136; iter: 200; batch classifier loss: 0.356960\n",
      "epoch 137; iter: 0; batch classifier loss: 0.390389\n",
      "epoch 137; iter: 200; batch classifier loss: 0.381458\n",
      "epoch 138; iter: 0; batch classifier loss: 0.405899\n",
      "epoch 138; iter: 200; batch classifier loss: 0.461134\n",
      "epoch 139; iter: 0; batch classifier loss: 0.445804\n",
      "epoch 139; iter: 200; batch classifier loss: 0.372862\n",
      "epoch 140; iter: 0; batch classifier loss: 0.373264\n",
      "epoch 140; iter: 200; batch classifier loss: 0.349124\n",
      "epoch 141; iter: 0; batch classifier loss: 0.488395\n",
      "epoch 141; iter: 200; batch classifier loss: 0.392702\n",
      "epoch 142; iter: 0; batch classifier loss: 0.460137\n",
      "epoch 142; iter: 200; batch classifier loss: 0.387845\n",
      "epoch 143; iter: 0; batch classifier loss: 0.429924\n",
      "epoch 143; iter: 200; batch classifier loss: 0.387633\n",
      "epoch 144; iter: 0; batch classifier loss: 0.407973\n",
      "epoch 144; iter: 200; batch classifier loss: 0.402366\n",
      "epoch 145; iter: 0; batch classifier loss: 0.407068\n",
      "epoch 145; iter: 200; batch classifier loss: 0.379543\n",
      "epoch 146; iter: 0; batch classifier loss: 0.361669\n",
      "epoch 146; iter: 200; batch classifier loss: 0.391976\n",
      "epoch 147; iter: 0; batch classifier loss: 0.408560\n",
      "epoch 147; iter: 200; batch classifier loss: 0.411326\n",
      "epoch 148; iter: 0; batch classifier loss: 0.472275\n",
      "epoch 148; iter: 200; batch classifier loss: 0.412167\n",
      "epoch 149; iter: 0; batch classifier loss: 0.439506\n",
      "epoch 149; iter: 200; batch classifier loss: 0.457973\n",
      "epoch 150; iter: 0; batch classifier loss: 0.319284\n",
      "epoch 150; iter: 200; batch classifier loss: 0.451346\n",
      "epoch 151; iter: 0; batch classifier loss: 0.413951\n",
      "epoch 151; iter: 200; batch classifier loss: 0.368477\n",
      "epoch 152; iter: 0; batch classifier loss: 0.442783\n",
      "epoch 152; iter: 200; batch classifier loss: 0.459299\n",
      "epoch 153; iter: 0; batch classifier loss: 0.391089\n",
      "epoch 153; iter: 200; batch classifier loss: 0.389464\n",
      "epoch 154; iter: 0; batch classifier loss: 0.469738\n",
      "epoch 154; iter: 200; batch classifier loss: 0.430865\n",
      "epoch 155; iter: 0; batch classifier loss: 0.403045\n",
      "epoch 155; iter: 200; batch classifier loss: 0.515292\n",
      "epoch 156; iter: 0; batch classifier loss: 0.442297\n",
      "epoch 156; iter: 200; batch classifier loss: 0.442864\n",
      "epoch 157; iter: 0; batch classifier loss: 0.374939\n",
      "epoch 157; iter: 200; batch classifier loss: 0.483429\n",
      "epoch 158; iter: 0; batch classifier loss: 0.405033\n",
      "epoch 158; iter: 200; batch classifier loss: 0.423447\n",
      "epoch 159; iter: 0; batch classifier loss: 0.351067\n",
      "epoch 159; iter: 200; batch classifier loss: 0.527695\n",
      "epoch 160; iter: 0; batch classifier loss: 0.389841\n",
      "epoch 160; iter: 200; batch classifier loss: 0.396261\n",
      "epoch 161; iter: 0; batch classifier loss: 0.485954\n",
      "epoch 161; iter: 200; batch classifier loss: 0.415150\n",
      "epoch 162; iter: 0; batch classifier loss: 0.408011\n",
      "epoch 162; iter: 200; batch classifier loss: 0.430144\n",
      "epoch 163; iter: 0; batch classifier loss: 0.466124\n",
      "epoch 163; iter: 200; batch classifier loss: 0.499012\n",
      "epoch 164; iter: 0; batch classifier loss: 0.386978\n",
      "epoch 164; iter: 200; batch classifier loss: 0.492843\n",
      "epoch 165; iter: 0; batch classifier loss: 0.465068\n",
      "epoch 165; iter: 200; batch classifier loss: 0.472219\n",
      "epoch 166; iter: 0; batch classifier loss: 0.458952\n",
      "epoch 166; iter: 200; batch classifier loss: 0.386478\n",
      "epoch 167; iter: 0; batch classifier loss: 0.398609\n",
      "epoch 167; iter: 200; batch classifier loss: 0.352657\n",
      "epoch 168; iter: 0; batch classifier loss: 0.418166\n",
      "epoch 168; iter: 200; batch classifier loss: 0.447936\n",
      "epoch 169; iter: 0; batch classifier loss: 0.368272\n",
      "epoch 169; iter: 200; batch classifier loss: 0.332460\n",
      "epoch 170; iter: 0; batch classifier loss: 0.311264\n",
      "epoch 170; iter: 200; batch classifier loss: 0.400104\n",
      "epoch 171; iter: 0; batch classifier loss: 0.399941\n",
      "epoch 171; iter: 200; batch classifier loss: 0.403932\n",
      "epoch 172; iter: 0; batch classifier loss: 0.471857\n",
      "epoch 172; iter: 200; batch classifier loss: 0.365444\n",
      "epoch 173; iter: 0; batch classifier loss: 0.385997\n",
      "epoch 173; iter: 200; batch classifier loss: 0.386488\n",
      "epoch 174; iter: 0; batch classifier loss: 0.386947\n",
      "epoch 174; iter: 200; batch classifier loss: 0.418899\n",
      "epoch 175; iter: 0; batch classifier loss: 0.429242\n",
      "epoch 175; iter: 200; batch classifier loss: 0.422217\n",
      "epoch 176; iter: 0; batch classifier loss: 0.511598\n",
      "epoch 176; iter: 200; batch classifier loss: 0.408934\n",
      "epoch 177; iter: 0; batch classifier loss: 0.344040\n",
      "epoch 177; iter: 200; batch classifier loss: 0.430949\n",
      "epoch 178; iter: 0; batch classifier loss: 0.347703\n",
      "epoch 178; iter: 200; batch classifier loss: 0.331617\n",
      "epoch 179; iter: 0; batch classifier loss: 0.393199\n",
      "epoch 179; iter: 200; batch classifier loss: 0.461321\n",
      "epoch 180; iter: 0; batch classifier loss: 0.454239\n",
      "epoch 180; iter: 200; batch classifier loss: 0.349063\n",
      "epoch 181; iter: 0; batch classifier loss: 0.393550\n",
      "epoch 181; iter: 200; batch classifier loss: 0.476361\n",
      "epoch 182; iter: 0; batch classifier loss: 0.505880\n",
      "epoch 182; iter: 200; batch classifier loss: 0.442800\n",
      "epoch 183; iter: 0; batch classifier loss: 0.359235\n",
      "epoch 183; iter: 200; batch classifier loss: 0.408448\n",
      "epoch 184; iter: 0; batch classifier loss: 0.414828\n",
      "epoch 184; iter: 200; batch classifier loss: 0.526619\n",
      "epoch 185; iter: 0; batch classifier loss: 0.418807\n",
      "epoch 185; iter: 200; batch classifier loss: 0.510659\n",
      "epoch 186; iter: 0; batch classifier loss: 0.540830\n",
      "epoch 186; iter: 200; batch classifier loss: 0.405093\n",
      "epoch 187; iter: 0; batch classifier loss: 0.402530\n",
      "epoch 187; iter: 200; batch classifier loss: 0.371818\n",
      "epoch 188; iter: 0; batch classifier loss: 0.376402\n",
      "epoch 188; iter: 200; batch classifier loss: 0.367501\n",
      "epoch 189; iter: 0; batch classifier loss: 0.450753\n",
      "epoch 189; iter: 200; batch classifier loss: 0.481145\n",
      "epoch 190; iter: 0; batch classifier loss: 0.420028\n",
      "epoch 190; iter: 200; batch classifier loss: 0.449608\n",
      "epoch 191; iter: 0; batch classifier loss: 0.460583\n",
      "epoch 191; iter: 200; batch classifier loss: 0.438652\n",
      "epoch 192; iter: 0; batch classifier loss: 0.326219\n",
      "epoch 192; iter: 200; batch classifier loss: 0.348031\n",
      "epoch 193; iter: 0; batch classifier loss: 0.423201\n",
      "epoch 193; iter: 200; batch classifier loss: 0.452141\n",
      "epoch 194; iter: 0; batch classifier loss: 0.402454\n",
      "epoch 194; iter: 200; batch classifier loss: 0.460127\n",
      "epoch 195; iter: 0; batch classifier loss: 0.381718\n",
      "epoch 195; iter: 200; batch classifier loss: 0.435583\n",
      "epoch 196; iter: 0; batch classifier loss: 0.389373\n",
      "epoch 196; iter: 200; batch classifier loss: 0.431934\n",
      "epoch 197; iter: 0; batch classifier loss: 0.438755\n",
      "epoch 197; iter: 200; batch classifier loss: 0.495293\n",
      "epoch 198; iter: 0; batch classifier loss: 0.412204\n",
      "epoch 198; iter: 200; batch classifier loss: 0.358519\n",
      "epoch 199; iter: 0; batch classifier loss: 0.385611\n",
      "epoch 199; iter: 200; batch classifier loss: 0.392355\n",
      "epoch 200; iter: 0; batch classifier loss: 0.394495\n",
      "epoch 200; iter: 200; batch classifier loss: 0.329779\n",
      "epoch 201; iter: 0; batch classifier loss: 0.476211\n",
      "epoch 201; iter: 200; batch classifier loss: 0.383202\n",
      "epoch 202; iter: 0; batch classifier loss: 0.467688\n",
      "epoch 202; iter: 200; batch classifier loss: 0.383450\n",
      "epoch 203; iter: 0; batch classifier loss: 0.418679\n",
      "epoch 203; iter: 200; batch classifier loss: 0.388501\n",
      "epoch 204; iter: 0; batch classifier loss: 0.346921\n",
      "epoch 204; iter: 200; batch classifier loss: 0.426469\n",
      "epoch 205; iter: 0; batch classifier loss: 0.325818\n",
      "epoch 205; iter: 200; batch classifier loss: 0.490458\n",
      "epoch 206; iter: 0; batch classifier loss: 0.502628\n",
      "epoch 206; iter: 200; batch classifier loss: 0.382478\n",
      "epoch 207; iter: 0; batch classifier loss: 0.443356\n",
      "epoch 207; iter: 200; batch classifier loss: 0.363385\n",
      "epoch 208; iter: 0; batch classifier loss: 0.484882\n",
      "epoch 208; iter: 200; batch classifier loss: 0.431360\n",
      "epoch 209; iter: 0; batch classifier loss: 0.348302\n",
      "epoch 209; iter: 200; batch classifier loss: 0.487828\n",
      "epoch 210; iter: 0; batch classifier loss: 0.438901\n",
      "epoch 210; iter: 200; batch classifier loss: 0.433350\n",
      "epoch 211; iter: 0; batch classifier loss: 0.332745\n",
      "epoch 211; iter: 200; batch classifier loss: 0.452658\n",
      "epoch 212; iter: 0; batch classifier loss: 0.540669\n",
      "epoch 212; iter: 200; batch classifier loss: 0.408091\n",
      "epoch 213; iter: 0; batch classifier loss: 0.446524\n",
      "epoch 213; iter: 200; batch classifier loss: 0.432536\n",
      "epoch 214; iter: 0; batch classifier loss: 0.390581\n",
      "epoch 214; iter: 200; batch classifier loss: 0.367179\n",
      "epoch 215; iter: 0; batch classifier loss: 0.429781\n",
      "epoch 215; iter: 200; batch classifier loss: 0.504760\n",
      "epoch 216; iter: 0; batch classifier loss: 0.445343\n",
      "epoch 216; iter: 200; batch classifier loss: 0.369086\n",
      "epoch 217; iter: 0; batch classifier loss: 0.488623\n",
      "epoch 217; iter: 200; batch classifier loss: 0.363545\n",
      "epoch 218; iter: 0; batch classifier loss: 0.262753\n",
      "epoch 218; iter: 200; batch classifier loss: 0.428468\n",
      "epoch 219; iter: 0; batch classifier loss: 0.383374\n",
      "epoch 219; iter: 200; batch classifier loss: 0.469392\n",
      "epoch 220; iter: 0; batch classifier loss: 0.464343\n",
      "epoch 220; iter: 200; batch classifier loss: 0.384552\n",
      "epoch 221; iter: 0; batch classifier loss: 0.385708\n",
      "epoch 221; iter: 200; batch classifier loss: 0.313826\n",
      "epoch 222; iter: 0; batch classifier loss: 0.419416\n",
      "epoch 222; iter: 200; batch classifier loss: 0.498773\n",
      "epoch 223; iter: 0; batch classifier loss: 0.476617\n",
      "epoch 223; iter: 200; batch classifier loss: 0.416027\n",
      "epoch 224; iter: 0; batch classifier loss: 0.371526\n",
      "epoch 224; iter: 200; batch classifier loss: 0.371683\n",
      "epoch 225; iter: 0; batch classifier loss: 0.392795\n",
      "epoch 225; iter: 200; batch classifier loss: 0.483837\n",
      "epoch 226; iter: 0; batch classifier loss: 0.452540\n",
      "epoch 226; iter: 200; batch classifier loss: 0.376942\n",
      "epoch 227; iter: 0; batch classifier loss: 0.383062\n",
      "epoch 227; iter: 200; batch classifier loss: 0.385813\n",
      "epoch 228; iter: 0; batch classifier loss: 0.400565\n",
      "epoch 228; iter: 200; batch classifier loss: 0.274637\n",
      "epoch 229; iter: 0; batch classifier loss: 0.446343\n",
      "epoch 229; iter: 200; batch classifier loss: 0.379656\n",
      "epoch 230; iter: 0; batch classifier loss: 0.481716\n",
      "epoch 230; iter: 200; batch classifier loss: 0.422950\n",
      "epoch 231; iter: 0; batch classifier loss: 0.320427\n",
      "epoch 231; iter: 200; batch classifier loss: 0.447278\n",
      "epoch 232; iter: 0; batch classifier loss: 0.470355\n",
      "epoch 232; iter: 200; batch classifier loss: 0.434280\n",
      "epoch 233; iter: 0; batch classifier loss: 0.448673\n",
      "epoch 233; iter: 200; batch classifier loss: 0.373207\n",
      "epoch 234; iter: 0; batch classifier loss: 0.369411\n",
      "epoch 234; iter: 200; batch classifier loss: 0.451353\n",
      "epoch 235; iter: 0; batch classifier loss: 0.420677\n",
      "epoch 235; iter: 200; batch classifier loss: 0.426231\n",
      "epoch 236; iter: 0; batch classifier loss: 0.472723\n",
      "epoch 236; iter: 200; batch classifier loss: 0.427777\n",
      "epoch 237; iter: 0; batch classifier loss: 0.507146\n",
      "epoch 237; iter: 200; batch classifier loss: 0.374106\n",
      "epoch 238; iter: 0; batch classifier loss: 0.382500\n",
      "epoch 238; iter: 200; batch classifier loss: 0.359174\n",
      "epoch 239; iter: 0; batch classifier loss: 0.380298\n",
      "epoch 239; iter: 200; batch classifier loss: 0.354584\n",
      "epoch 240; iter: 0; batch classifier loss: 0.470305\n",
      "epoch 240; iter: 200; batch classifier loss: 0.364380\n",
      "epoch 241; iter: 0; batch classifier loss: 0.433807\n",
      "epoch 241; iter: 200; batch classifier loss: 0.425746\n",
      "epoch 242; iter: 0; batch classifier loss: 0.479133\n",
      "epoch 242; iter: 200; batch classifier loss: 0.390928\n",
      "epoch 243; iter: 0; batch classifier loss: 0.464287\n",
      "epoch 243; iter: 200; batch classifier loss: 0.454820\n",
      "epoch 244; iter: 0; batch classifier loss: 0.516509\n",
      "epoch 244; iter: 200; batch classifier loss: 0.483709\n",
      "epoch 245; iter: 0; batch classifier loss: 0.386392\n",
      "epoch 245; iter: 200; batch classifier loss: 0.480371\n",
      "epoch 246; iter: 0; batch classifier loss: 0.399489\n",
      "epoch 246; iter: 200; batch classifier loss: 0.436293\n",
      "epoch 247; iter: 0; batch classifier loss: 0.476483\n",
      "epoch 247; iter: 200; batch classifier loss: 0.450475\n",
      "epoch 248; iter: 0; batch classifier loss: 0.449739\n",
      "epoch 248; iter: 200; batch classifier loss: 0.371282\n",
      "epoch 249; iter: 0; batch classifier loss: 0.384085\n",
      "epoch 249; iter: 200; batch classifier loss: 0.439101\n",
      "epoch 250; iter: 0; batch classifier loss: 0.420910\n",
      "epoch 250; iter: 200; batch classifier loss: 0.366658\n",
      "epoch 251; iter: 0; batch classifier loss: 0.370852\n",
      "epoch 251; iter: 200; batch classifier loss: 0.419794\n",
      "epoch 252; iter: 0; batch classifier loss: 0.461091\n",
      "epoch 252; iter: 200; batch classifier loss: 0.405462\n",
      "epoch 253; iter: 0; batch classifier loss: 0.446941\n",
      "epoch 253; iter: 200; batch classifier loss: 0.416072\n",
      "epoch 254; iter: 0; batch classifier loss: 0.392040\n",
      "epoch 254; iter: 200; batch classifier loss: 0.396894\n",
      "epoch 255; iter: 0; batch classifier loss: 0.388821\n",
      "epoch 255; iter: 200; batch classifier loss: 0.435257\n",
      "epoch 256; iter: 0; batch classifier loss: 0.408256\n",
      "epoch 256; iter: 200; batch classifier loss: 0.449762\n",
      "epoch 257; iter: 0; batch classifier loss: 0.375677\n",
      "epoch 257; iter: 200; batch classifier loss: 0.368435\n",
      "epoch 258; iter: 0; batch classifier loss: 0.452625\n",
      "epoch 258; iter: 200; batch classifier loss: 0.361846\n",
      "epoch 259; iter: 0; batch classifier loss: 0.444493\n",
      "epoch 259; iter: 200; batch classifier loss: 0.412455\n",
      "epoch 260; iter: 0; batch classifier loss: 0.477607\n",
      "epoch 260; iter: 200; batch classifier loss: 0.398892\n",
      "epoch 261; iter: 0; batch classifier loss: 0.387479\n",
      "epoch 261; iter: 200; batch classifier loss: 0.442738\n",
      "epoch 262; iter: 0; batch classifier loss: 0.375093\n",
      "epoch 262; iter: 200; batch classifier loss: 0.368915\n",
      "epoch 263; iter: 0; batch classifier loss: 0.421062\n",
      "epoch 263; iter: 200; batch classifier loss: 0.432961\n",
      "epoch 264; iter: 0; batch classifier loss: 0.357237\n",
      "epoch 264; iter: 200; batch classifier loss: 0.433939\n",
      "epoch 265; iter: 0; batch classifier loss: 0.380072\n",
      "epoch 265; iter: 200; batch classifier loss: 0.381894\n",
      "epoch 266; iter: 0; batch classifier loss: 0.354198\n",
      "epoch 266; iter: 200; batch classifier loss: 0.412529\n",
      "epoch 267; iter: 0; batch classifier loss: 0.443436\n",
      "epoch 267; iter: 200; batch classifier loss: 0.456534\n",
      "epoch 268; iter: 0; batch classifier loss: 0.358089\n",
      "epoch 268; iter: 200; batch classifier loss: 0.385958\n",
      "epoch 269; iter: 0; batch classifier loss: 0.462163\n",
      "epoch 269; iter: 200; batch classifier loss: 0.482843\n",
      "epoch 270; iter: 0; batch classifier loss: 0.398310\n",
      "epoch 270; iter: 200; batch classifier loss: 0.409913\n",
      "epoch 271; iter: 0; batch classifier loss: 0.454894\n",
      "epoch 271; iter: 200; batch classifier loss: 0.402005\n",
      "epoch 272; iter: 0; batch classifier loss: 0.425813\n",
      "epoch 272; iter: 200; batch classifier loss: 0.436661\n",
      "epoch 273; iter: 0; batch classifier loss: 0.414761\n",
      "epoch 273; iter: 200; batch classifier loss: 0.361407\n",
      "epoch 274; iter: 0; batch classifier loss: 0.398348\n",
      "epoch 274; iter: 200; batch classifier loss: 0.441394\n",
      "epoch 275; iter: 0; batch classifier loss: 0.431094\n",
      "epoch 275; iter: 200; batch classifier loss: 0.387306\n",
      "epoch 276; iter: 0; batch classifier loss: 0.360277\n",
      "epoch 276; iter: 200; batch classifier loss: 0.397185\n",
      "epoch 277; iter: 0; batch classifier loss: 0.345859\n",
      "epoch 277; iter: 200; batch classifier loss: 0.423330\n",
      "epoch 278; iter: 0; batch classifier loss: 0.398625\n",
      "epoch 278; iter: 200; batch classifier loss: 0.466828\n",
      "epoch 279; iter: 0; batch classifier loss: 0.416681\n",
      "epoch 279; iter: 200; batch classifier loss: 0.392066\n",
      "epoch 280; iter: 0; batch classifier loss: 0.418801\n",
      "epoch 280; iter: 200; batch classifier loss: 0.442390\n",
      "epoch 281; iter: 0; batch classifier loss: 0.371093\n",
      "epoch 281; iter: 200; batch classifier loss: 0.416066\n",
      "epoch 282; iter: 0; batch classifier loss: 0.425381\n",
      "epoch 282; iter: 200; batch classifier loss: 0.502400\n",
      "epoch 283; iter: 0; batch classifier loss: 0.350434\n",
      "epoch 283; iter: 200; batch classifier loss: 0.455235\n",
      "epoch 284; iter: 0; batch classifier loss: 0.461377\n",
      "epoch 284; iter: 200; batch classifier loss: 0.490175\n",
      "epoch 285; iter: 0; batch classifier loss: 0.413601\n",
      "epoch 285; iter: 200; batch classifier loss: 0.431144\n",
      "epoch 286; iter: 0; batch classifier loss: 0.505741\n",
      "epoch 286; iter: 200; batch classifier loss: 0.455220\n",
      "epoch 287; iter: 0; batch classifier loss: 0.363375\n",
      "epoch 287; iter: 200; batch classifier loss: 0.421348\n",
      "epoch 288; iter: 0; batch classifier loss: 0.391970\n",
      "epoch 288; iter: 200; batch classifier loss: 0.430219\n",
      "epoch 289; iter: 0; batch classifier loss: 0.443138\n",
      "epoch 289; iter: 200; batch classifier loss: 0.458077\n",
      "epoch 290; iter: 0; batch classifier loss: 0.482183\n",
      "epoch 290; iter: 200; batch classifier loss: 0.356353\n",
      "epoch 291; iter: 0; batch classifier loss: 0.511636\n",
      "epoch 291; iter: 200; batch classifier loss: 0.430801\n",
      "epoch 292; iter: 0; batch classifier loss: 0.354435\n",
      "epoch 292; iter: 200; batch classifier loss: 0.417961\n",
      "epoch 293; iter: 0; batch classifier loss: 0.383485\n",
      "epoch 293; iter: 200; batch classifier loss: 0.474316\n",
      "epoch 294; iter: 0; batch classifier loss: 0.390104\n",
      "epoch 294; iter: 200; batch classifier loss: 0.427313\n",
      "epoch 295; iter: 0; batch classifier loss: 0.401174\n",
      "epoch 295; iter: 200; batch classifier loss: 0.438272\n",
      "epoch 296; iter: 0; batch classifier loss: 0.399673\n",
      "epoch 296; iter: 200; batch classifier loss: 0.390673\n",
      "epoch 297; iter: 0; batch classifier loss: 0.437875\n",
      "epoch 297; iter: 200; batch classifier loss: 0.467286\n",
      "epoch 298; iter: 0; batch classifier loss: 0.394456\n",
      "epoch 298; iter: 200; batch classifier loss: 0.335853\n",
      "epoch 299; iter: 0; batch classifier loss: 0.394866\n",
      "epoch 299; iter: 200; batch classifier loss: 0.335662\n",
      "epoch 300; iter: 0; batch classifier loss: 0.405953\n",
      "epoch 300; iter: 200; batch classifier loss: 0.423741\n",
      "epoch 301; iter: 0; batch classifier loss: 0.384849\n",
      "epoch 301; iter: 200; batch classifier loss: 0.444091\n",
      "epoch 302; iter: 0; batch classifier loss: 0.377640\n",
      "epoch 302; iter: 200; batch classifier loss: 0.417474\n",
      "epoch 303; iter: 0; batch classifier loss: 0.328793\n",
      "epoch 303; iter: 200; batch classifier loss: 0.387089\n",
      "epoch 304; iter: 0; batch classifier loss: 0.383715\n",
      "epoch 304; iter: 200; batch classifier loss: 0.412973\n",
      "epoch 305; iter: 0; batch classifier loss: 0.426726\n",
      "epoch 305; iter: 200; batch classifier loss: 0.388836\n",
      "epoch 306; iter: 0; batch classifier loss: 0.406929\n",
      "epoch 306; iter: 200; batch classifier loss: 0.370228\n",
      "epoch 307; iter: 0; batch classifier loss: 0.470729\n",
      "epoch 307; iter: 200; batch classifier loss: 0.392015\n",
      "epoch 308; iter: 0; batch classifier loss: 0.485271\n",
      "epoch 308; iter: 200; batch classifier loss: 0.413647\n",
      "epoch 309; iter: 0; batch classifier loss: 0.470158\n",
      "epoch 309; iter: 200; batch classifier loss: 0.436522\n",
      "epoch 310; iter: 0; batch classifier loss: 0.424470\n",
      "epoch 310; iter: 200; batch classifier loss: 0.417173\n",
      "epoch 311; iter: 0; batch classifier loss: 0.457507\n",
      "epoch 311; iter: 200; batch classifier loss: 0.380669\n",
      "epoch 312; iter: 0; batch classifier loss: 0.499300\n",
      "epoch 312; iter: 200; batch classifier loss: 0.480547\n",
      "epoch 313; iter: 0; batch classifier loss: 0.425390\n",
      "epoch 313; iter: 200; batch classifier loss: 0.370773\n",
      "epoch 314; iter: 0; batch classifier loss: 0.465442\n",
      "epoch 314; iter: 200; batch classifier loss: 0.452458\n",
      "epoch 315; iter: 0; batch classifier loss: 0.407286\n",
      "epoch 315; iter: 200; batch classifier loss: 0.417186\n",
      "epoch 316; iter: 0; batch classifier loss: 0.456385\n",
      "epoch 316; iter: 200; batch classifier loss: 0.474642\n",
      "epoch 317; iter: 0; batch classifier loss: 0.340490\n",
      "epoch 317; iter: 200; batch classifier loss: 0.426937\n",
      "epoch 318; iter: 0; batch classifier loss: 0.528790\n",
      "epoch 318; iter: 200; batch classifier loss: 0.371488\n",
      "epoch 319; iter: 0; batch classifier loss: 0.359202\n",
      "epoch 319; iter: 200; batch classifier loss: 0.370416\n",
      "epoch 320; iter: 0; batch classifier loss: 0.502471\n",
      "epoch 320; iter: 200; batch classifier loss: 0.510006\n",
      "epoch 321; iter: 0; batch classifier loss: 0.376492\n",
      "epoch 321; iter: 200; batch classifier loss: 0.438585\n",
      "epoch 322; iter: 0; batch classifier loss: 0.363390\n",
      "epoch 322; iter: 200; batch classifier loss: 0.392963\n",
      "epoch 323; iter: 0; batch classifier loss: 0.493972\n",
      "epoch 323; iter: 200; batch classifier loss: 0.347692\n",
      "epoch 324; iter: 0; batch classifier loss: 0.454824\n",
      "epoch 324; iter: 200; batch classifier loss: 0.487025\n",
      "epoch 325; iter: 0; batch classifier loss: 0.385244\n",
      "epoch 325; iter: 200; batch classifier loss: 0.370373\n",
      "epoch 326; iter: 0; batch classifier loss: 0.434103\n",
      "epoch 326; iter: 200; batch classifier loss: 0.421612\n",
      "epoch 327; iter: 0; batch classifier loss: 0.366919\n",
      "epoch 327; iter: 200; batch classifier loss: 0.326723\n",
      "epoch 328; iter: 0; batch classifier loss: 0.413975\n",
      "epoch 328; iter: 200; batch classifier loss: 0.406192\n",
      "epoch 329; iter: 0; batch classifier loss: 0.436891\n",
      "epoch 329; iter: 200; batch classifier loss: 0.448034\n",
      "epoch 330; iter: 0; batch classifier loss: 0.365101\n",
      "epoch 330; iter: 200; batch classifier loss: 0.404291\n",
      "epoch 331; iter: 0; batch classifier loss: 0.400963\n",
      "epoch 331; iter: 200; batch classifier loss: 0.391935\n",
      "epoch 332; iter: 0; batch classifier loss: 0.445653\n",
      "epoch 332; iter: 200; batch classifier loss: 0.428139\n",
      "epoch 333; iter: 0; batch classifier loss: 0.466584\n",
      "epoch 333; iter: 200; batch classifier loss: 0.388355\n",
      "epoch 334; iter: 0; batch classifier loss: 0.406130\n",
      "epoch 334; iter: 200; batch classifier loss: 0.474703\n",
      "epoch 335; iter: 0; batch classifier loss: 0.443677\n",
      "epoch 335; iter: 200; batch classifier loss: 0.409455\n",
      "epoch 336; iter: 0; batch classifier loss: 0.441041\n",
      "epoch 336; iter: 200; batch classifier loss: 0.375043\n",
      "epoch 337; iter: 0; batch classifier loss: 0.402544\n",
      "epoch 337; iter: 200; batch classifier loss: 0.478145\n",
      "epoch 338; iter: 0; batch classifier loss: 0.361411\n",
      "epoch 338; iter: 200; batch classifier loss: 0.378267\n",
      "epoch 339; iter: 0; batch classifier loss: 0.480876\n",
      "epoch 339; iter: 200; batch classifier loss: 0.493325\n",
      "epoch 340; iter: 0; batch classifier loss: 0.361822\n",
      "epoch 340; iter: 200; batch classifier loss: 0.451838\n",
      "epoch 341; iter: 0; batch classifier loss: 0.376998\n",
      "epoch 341; iter: 200; batch classifier loss: 0.357118\n",
      "epoch 342; iter: 0; batch classifier loss: 0.433090\n",
      "epoch 342; iter: 200; batch classifier loss: 0.362308\n",
      "epoch 343; iter: 0; batch classifier loss: 0.380727\n",
      "epoch 343; iter: 200; batch classifier loss: 0.366098\n",
      "epoch 344; iter: 0; batch classifier loss: 0.416311\n",
      "epoch 344; iter: 200; batch classifier loss: 0.411965\n",
      "epoch 345; iter: 0; batch classifier loss: 0.434014\n",
      "epoch 345; iter: 200; batch classifier loss: 0.373276\n",
      "epoch 346; iter: 0; batch classifier loss: 0.375497\n",
      "epoch 346; iter: 200; batch classifier loss: 0.439513\n",
      "epoch 347; iter: 0; batch classifier loss: 0.391118\n",
      "epoch 347; iter: 200; batch classifier loss: 0.480757\n",
      "epoch 348; iter: 0; batch classifier loss: 0.437972\n",
      "epoch 348; iter: 200; batch classifier loss: 0.491517\n",
      "epoch 349; iter: 0; batch classifier loss: 0.406693\n",
      "epoch 349; iter: 200; batch classifier loss: 0.438814\n",
      "epoch 350; iter: 0; batch classifier loss: 0.400115\n",
      "epoch 350; iter: 200; batch classifier loss: 0.534672\n",
      "epoch 351; iter: 0; batch classifier loss: 0.369700\n",
      "epoch 351; iter: 200; batch classifier loss: 0.380973\n",
      "epoch 352; iter: 0; batch classifier loss: 0.429936\n",
      "epoch 352; iter: 200; batch classifier loss: 0.398015\n",
      "epoch 353; iter: 0; batch classifier loss: 0.411201\n",
      "epoch 353; iter: 200; batch classifier loss: 0.347906\n",
      "epoch 354; iter: 0; batch classifier loss: 0.479778\n",
      "epoch 354; iter: 200; batch classifier loss: 0.406388\n",
      "epoch 355; iter: 0; batch classifier loss: 0.406201\n",
      "epoch 355; iter: 200; batch classifier loss: 0.491339\n",
      "epoch 356; iter: 0; batch classifier loss: 0.485178\n",
      "epoch 356; iter: 200; batch classifier loss: 0.368136\n",
      "epoch 357; iter: 0; batch classifier loss: 0.316081\n",
      "epoch 357; iter: 200; batch classifier loss: 0.392540\n",
      "epoch 358; iter: 0; batch classifier loss: 0.401723\n",
      "epoch 358; iter: 200; batch classifier loss: 0.341187\n",
      "epoch 359; iter: 0; batch classifier loss: 0.455359\n",
      "epoch 359; iter: 200; batch classifier loss: 0.448054\n",
      "epoch 360; iter: 0; batch classifier loss: 0.403306\n",
      "epoch 360; iter: 200; batch classifier loss: 0.422740\n",
      "epoch 361; iter: 0; batch classifier loss: 0.391699\n",
      "epoch 361; iter: 200; batch classifier loss: 0.472667\n",
      "epoch 362; iter: 0; batch classifier loss: 0.409705\n",
      "epoch 362; iter: 200; batch classifier loss: 0.485674\n",
      "epoch 363; iter: 0; batch classifier loss: 0.436792\n",
      "epoch 363; iter: 200; batch classifier loss: 0.455049\n",
      "epoch 364; iter: 0; batch classifier loss: 0.412383\n",
      "epoch 364; iter: 200; batch classifier loss: 0.452598\n",
      "epoch 365; iter: 0; batch classifier loss: 0.349640\n",
      "epoch 365; iter: 200; batch classifier loss: 0.391428\n",
      "epoch 366; iter: 0; batch classifier loss: 0.377337\n",
      "epoch 366; iter: 200; batch classifier loss: 0.361745\n",
      "epoch 367; iter: 0; batch classifier loss: 0.471794\n",
      "epoch 367; iter: 200; batch classifier loss: 0.416886\n",
      "epoch 368; iter: 0; batch classifier loss: 0.352574\n",
      "epoch 368; iter: 200; batch classifier loss: 0.428411\n",
      "epoch 369; iter: 0; batch classifier loss: 0.331353\n",
      "epoch 369; iter: 200; batch classifier loss: 0.415969\n",
      "epoch 370; iter: 0; batch classifier loss: 0.408145\n",
      "epoch 370; iter: 200; batch classifier loss: 0.375944\n",
      "epoch 371; iter: 0; batch classifier loss: 0.480333\n",
      "epoch 371; iter: 200; batch classifier loss: 0.415750\n",
      "epoch 372; iter: 0; batch classifier loss: 0.430453\n",
      "epoch 372; iter: 200; batch classifier loss: 0.381999\n",
      "epoch 373; iter: 0; batch classifier loss: 0.348003\n",
      "epoch 373; iter: 200; batch classifier loss: 0.364111\n",
      "epoch 374; iter: 0; batch classifier loss: 0.392936\n",
      "epoch 374; iter: 200; batch classifier loss: 0.382381\n",
      "epoch 375; iter: 0; batch classifier loss: 0.448015\n",
      "epoch 375; iter: 200; batch classifier loss: 0.474160\n",
      "epoch 376; iter: 0; batch classifier loss: 0.383636\n",
      "epoch 376; iter: 200; batch classifier loss: 0.410428\n",
      "epoch 377; iter: 0; batch classifier loss: 0.362228\n",
      "epoch 377; iter: 200; batch classifier loss: 0.379607\n",
      "epoch 378; iter: 0; batch classifier loss: 0.390718\n",
      "epoch 378; iter: 200; batch classifier loss: 0.410120\n",
      "epoch 379; iter: 0; batch classifier loss: 0.450450\n",
      "epoch 379; iter: 200; batch classifier loss: 0.420902\n",
      "epoch 380; iter: 0; batch classifier loss: 0.575406\n",
      "epoch 380; iter: 200; batch classifier loss: 0.438333\n",
      "epoch 381; iter: 0; batch classifier loss: 0.341394\n",
      "epoch 381; iter: 200; batch classifier loss: 0.468319\n",
      "epoch 382; iter: 0; batch classifier loss: 0.358229\n",
      "epoch 382; iter: 200; batch classifier loss: 0.511569\n",
      "epoch 383; iter: 0; batch classifier loss: 0.407932\n",
      "epoch 383; iter: 200; batch classifier loss: 0.415847\n",
      "epoch 384; iter: 0; batch classifier loss: 0.331878\n",
      "epoch 384; iter: 200; batch classifier loss: 0.458938\n",
      "epoch 385; iter: 0; batch classifier loss: 0.413061\n",
      "epoch 385; iter: 200; batch classifier loss: 0.396349\n",
      "epoch 386; iter: 0; batch classifier loss: 0.349497\n",
      "epoch 386; iter: 200; batch classifier loss: 0.442914\n",
      "epoch 387; iter: 0; batch classifier loss: 0.430592\n",
      "epoch 387; iter: 200; batch classifier loss: 0.417553\n",
      "epoch 388; iter: 0; batch classifier loss: 0.433231\n",
      "epoch 388; iter: 200; batch classifier loss: 0.385398\n",
      "epoch 389; iter: 0; batch classifier loss: 0.367975\n",
      "epoch 389; iter: 200; batch classifier loss: 0.393634\n",
      "epoch 390; iter: 0; batch classifier loss: 0.451327\n",
      "epoch 390; iter: 200; batch classifier loss: 0.402354\n",
      "epoch 391; iter: 0; batch classifier loss: 0.504824\n",
      "epoch 391; iter: 200; batch classifier loss: 0.424600\n",
      "epoch 392; iter: 0; batch classifier loss: 0.387173\n",
      "epoch 392; iter: 200; batch classifier loss: 0.405968\n",
      "epoch 393; iter: 0; batch classifier loss: 0.468661\n",
      "epoch 393; iter: 200; batch classifier loss: 0.334495\n",
      "epoch 394; iter: 0; batch classifier loss: 0.457594\n",
      "epoch 394; iter: 200; batch classifier loss: 0.437377\n",
      "epoch 395; iter: 0; batch classifier loss: 0.410489\n",
      "epoch 395; iter: 200; batch classifier loss: 0.471240\n",
      "epoch 396; iter: 0; batch classifier loss: 0.375700\n",
      "epoch 396; iter: 200; batch classifier loss: 0.387277\n",
      "epoch 397; iter: 0; batch classifier loss: 0.471535\n",
      "epoch 397; iter: 200; batch classifier loss: 0.389723\n",
      "epoch 398; iter: 0; batch classifier loss: 0.371265\n",
      "epoch 398; iter: 200; batch classifier loss: 0.440323\n",
      "epoch 399; iter: 0; batch classifier loss: 0.402871\n",
      "epoch 399; iter: 200; batch classifier loss: 0.501782\n",
      "epoch 400; iter: 0; batch classifier loss: 0.433006\n",
      "epoch 400; iter: 200; batch classifier loss: 0.418222\n",
      "epoch 401; iter: 0; batch classifier loss: 0.393609\n",
      "epoch 401; iter: 200; batch classifier loss: 0.425433\n",
      "epoch 402; iter: 0; batch classifier loss: 0.360239\n",
      "epoch 402; iter: 200; batch classifier loss: 0.443574\n",
      "epoch 403; iter: 0; batch classifier loss: 0.449969\n",
      "epoch 403; iter: 200; batch classifier loss: 0.423991\n",
      "epoch 404; iter: 0; batch classifier loss: 0.396305\n",
      "epoch 404; iter: 200; batch classifier loss: 0.392257\n",
      "epoch 405; iter: 0; batch classifier loss: 0.459238\n",
      "epoch 405; iter: 200; batch classifier loss: 0.406736\n",
      "epoch 406; iter: 0; batch classifier loss: 0.436519\n",
      "epoch 406; iter: 200; batch classifier loss: 0.383554\n",
      "epoch 407; iter: 0; batch classifier loss: 0.411944\n",
      "epoch 407; iter: 200; batch classifier loss: 0.418977\n",
      "epoch 408; iter: 0; batch classifier loss: 0.364857\n",
      "epoch 408; iter: 200; batch classifier loss: 0.405796\n",
      "epoch 409; iter: 0; batch classifier loss: 0.407086\n",
      "epoch 409; iter: 200; batch classifier loss: 0.385590\n",
      "epoch 410; iter: 0; batch classifier loss: 0.370858\n",
      "epoch 410; iter: 200; batch classifier loss: 0.449224\n",
      "epoch 411; iter: 0; batch classifier loss: 0.501596\n",
      "epoch 411; iter: 200; batch classifier loss: 0.349216\n",
      "epoch 412; iter: 0; batch classifier loss: 0.380158\n",
      "epoch 412; iter: 200; batch classifier loss: 0.404584\n",
      "epoch 413; iter: 0; batch classifier loss: 0.405434\n",
      "epoch 413; iter: 200; batch classifier loss: 0.432231\n",
      "epoch 414; iter: 0; batch classifier loss: 0.422716\n",
      "epoch 414; iter: 200; batch classifier loss: 0.429793\n",
      "epoch 415; iter: 0; batch classifier loss: 0.429185\n",
      "epoch 415; iter: 200; batch classifier loss: 0.406382\n",
      "epoch 416; iter: 0; batch classifier loss: 0.357549\n",
      "epoch 416; iter: 200; batch classifier loss: 0.379400\n",
      "epoch 417; iter: 0; batch classifier loss: 0.312120\n",
      "epoch 417; iter: 200; batch classifier loss: 0.321626\n",
      "epoch 418; iter: 0; batch classifier loss: 0.378197\n",
      "epoch 418; iter: 200; batch classifier loss: 0.392229\n",
      "epoch 419; iter: 0; batch classifier loss: 0.378243\n",
      "epoch 419; iter: 200; batch classifier loss: 0.378129\n",
      "epoch 420; iter: 0; batch classifier loss: 0.453372\n",
      "epoch 420; iter: 200; batch classifier loss: 0.391521\n",
      "epoch 421; iter: 0; batch classifier loss: 0.359284\n",
      "epoch 421; iter: 200; batch classifier loss: 0.426689\n",
      "epoch 422; iter: 0; batch classifier loss: 0.425301\n",
      "epoch 422; iter: 200; batch classifier loss: 0.406340\n",
      "epoch 423; iter: 0; batch classifier loss: 0.462299\n",
      "epoch 423; iter: 200; batch classifier loss: 0.410239\n",
      "epoch 424; iter: 0; batch classifier loss: 0.357183\n",
      "epoch 424; iter: 200; batch classifier loss: 0.414908\n",
      "epoch 425; iter: 0; batch classifier loss: 0.423000\n",
      "epoch 425; iter: 200; batch classifier loss: 0.486544\n",
      "epoch 426; iter: 0; batch classifier loss: 0.412011\n",
      "epoch 426; iter: 200; batch classifier loss: 0.369248\n",
      "epoch 427; iter: 0; batch classifier loss: 0.381075\n",
      "epoch 427; iter: 200; batch classifier loss: 0.382774\n",
      "epoch 428; iter: 0; batch classifier loss: 0.455893\n",
      "epoch 428; iter: 200; batch classifier loss: 0.426892\n",
      "epoch 429; iter: 0; batch classifier loss: 0.416640\n",
      "epoch 429; iter: 200; batch classifier loss: 0.488650\n",
      "epoch 430; iter: 0; batch classifier loss: 0.363074\n",
      "epoch 430; iter: 200; batch classifier loss: 0.475847\n",
      "epoch 431; iter: 0; batch classifier loss: 0.444271\n",
      "epoch 431; iter: 200; batch classifier loss: 0.417441\n",
      "epoch 432; iter: 0; batch classifier loss: 0.474583\n",
      "epoch 432; iter: 200; batch classifier loss: 0.397229\n",
      "epoch 433; iter: 0; batch classifier loss: 0.485690\n",
      "epoch 433; iter: 200; batch classifier loss: 0.407331\n",
      "epoch 434; iter: 0; batch classifier loss: 0.437028\n",
      "epoch 434; iter: 200; batch classifier loss: 0.348708\n",
      "epoch 435; iter: 0; batch classifier loss: 0.387899\n",
      "epoch 435; iter: 200; batch classifier loss: 0.444234\n",
      "epoch 436; iter: 0; batch classifier loss: 0.406659\n",
      "epoch 436; iter: 200; batch classifier loss: 0.411436\n",
      "epoch 437; iter: 0; batch classifier loss: 0.490084\n",
      "epoch 437; iter: 200; batch classifier loss: 0.478311\n",
      "epoch 438; iter: 0; batch classifier loss: 0.439075\n",
      "epoch 438; iter: 200; batch classifier loss: 0.447882\n",
      "epoch 439; iter: 0; batch classifier loss: 0.463490\n",
      "epoch 439; iter: 200; batch classifier loss: 0.408084\n",
      "epoch 440; iter: 0; batch classifier loss: 0.398814\n",
      "epoch 440; iter: 200; batch classifier loss: 0.355382\n",
      "epoch 441; iter: 0; batch classifier loss: 0.391089\n",
      "epoch 441; iter: 200; batch classifier loss: 0.447619\n",
      "epoch 442; iter: 0; batch classifier loss: 0.411652\n",
      "epoch 442; iter: 200; batch classifier loss: 0.431412\n",
      "epoch 443; iter: 0; batch classifier loss: 0.415731\n",
      "epoch 443; iter: 200; batch classifier loss: 0.424319\n",
      "epoch 444; iter: 0; batch classifier loss: 0.526724\n",
      "epoch 444; iter: 200; batch classifier loss: 0.336501\n",
      "epoch 445; iter: 0; batch classifier loss: 0.467275\n",
      "epoch 445; iter: 200; batch classifier loss: 0.418764\n",
      "epoch 446; iter: 0; batch classifier loss: 0.354594\n",
      "epoch 446; iter: 200; batch classifier loss: 0.364809\n",
      "epoch 447; iter: 0; batch classifier loss: 0.421910\n",
      "epoch 447; iter: 200; batch classifier loss: 0.329925\n",
      "epoch 448; iter: 0; batch classifier loss: 0.339790\n",
      "epoch 448; iter: 200; batch classifier loss: 0.422955\n",
      "epoch 449; iter: 0; batch classifier loss: 0.450448\n",
      "epoch 449; iter: 200; batch classifier loss: 0.431923\n",
      "epoch 450; iter: 0; batch classifier loss: 0.477369\n",
      "epoch 450; iter: 200; batch classifier loss: 0.351630\n",
      "epoch 451; iter: 0; batch classifier loss: 0.394290\n",
      "epoch 451; iter: 200; batch classifier loss: 0.506017\n",
      "epoch 452; iter: 0; batch classifier loss: 0.407669\n",
      "epoch 452; iter: 200; batch classifier loss: 0.510947\n",
      "epoch 453; iter: 0; batch classifier loss: 0.373619\n",
      "epoch 453; iter: 200; batch classifier loss: 0.399670\n",
      "epoch 454; iter: 0; batch classifier loss: 0.359323\n",
      "epoch 454; iter: 200; batch classifier loss: 0.484698\n",
      "epoch 455; iter: 0; batch classifier loss: 0.387236\n",
      "epoch 455; iter: 200; batch classifier loss: 0.486388\n",
      "epoch 456; iter: 0; batch classifier loss: 0.421717\n",
      "epoch 456; iter: 200; batch classifier loss: 0.406899\n",
      "epoch 457; iter: 0; batch classifier loss: 0.464858\n",
      "epoch 457; iter: 200; batch classifier loss: 0.414170\n",
      "epoch 458; iter: 0; batch classifier loss: 0.417492\n",
      "epoch 458; iter: 200; batch classifier loss: 0.396474\n",
      "epoch 459; iter: 0; batch classifier loss: 0.428339\n",
      "epoch 459; iter: 200; batch classifier loss: 0.485120\n",
      "epoch 460; iter: 0; batch classifier loss: 0.371208\n",
      "epoch 460; iter: 200; batch classifier loss: 0.481792\n",
      "epoch 461; iter: 0; batch classifier loss: 0.471896\n",
      "epoch 461; iter: 200; batch classifier loss: 0.518900\n",
      "epoch 462; iter: 0; batch classifier loss: 0.413938\n",
      "epoch 462; iter: 200; batch classifier loss: 0.403263\n",
      "epoch 463; iter: 0; batch classifier loss: 0.414843\n",
      "epoch 463; iter: 200; batch classifier loss: 0.415086\n",
      "epoch 464; iter: 0; batch classifier loss: 0.410035\n",
      "epoch 464; iter: 200; batch classifier loss: 0.438044\n",
      "epoch 465; iter: 0; batch classifier loss: 0.398205\n",
      "epoch 465; iter: 200; batch classifier loss: 0.365712\n",
      "epoch 466; iter: 0; batch classifier loss: 0.430214\n",
      "epoch 466; iter: 200; batch classifier loss: 0.337805\n",
      "epoch 467; iter: 0; batch classifier loss: 0.472190\n",
      "epoch 467; iter: 200; batch classifier loss: 0.467939\n",
      "epoch 468; iter: 0; batch classifier loss: 0.443675\n",
      "epoch 468; iter: 200; batch classifier loss: 0.391874\n",
      "epoch 469; iter: 0; batch classifier loss: 0.386774\n",
      "epoch 469; iter: 200; batch classifier loss: 0.475743\n",
      "epoch 470; iter: 0; batch classifier loss: 0.513810\n",
      "epoch 470; iter: 200; batch classifier loss: 0.400836\n",
      "epoch 471; iter: 0; batch classifier loss: 0.420309\n",
      "epoch 471; iter: 200; batch classifier loss: 0.414251\n",
      "epoch 472; iter: 0; batch classifier loss: 0.478778\n",
      "epoch 472; iter: 200; batch classifier loss: 0.493796\n",
      "epoch 473; iter: 0; batch classifier loss: 0.404381\n",
      "epoch 473; iter: 200; batch classifier loss: 0.461212\n",
      "epoch 474; iter: 0; batch classifier loss: 0.306862\n",
      "epoch 474; iter: 200; batch classifier loss: 0.390322\n",
      "epoch 475; iter: 0; batch classifier loss: 0.476778\n",
      "epoch 475; iter: 200; batch classifier loss: 0.416509\n",
      "epoch 476; iter: 0; batch classifier loss: 0.454022\n",
      "epoch 476; iter: 200; batch classifier loss: 0.414209\n",
      "epoch 477; iter: 0; batch classifier loss: 0.375092\n",
      "epoch 477; iter: 200; batch classifier loss: 0.346882\n",
      "epoch 478; iter: 0; batch classifier loss: 0.410640\n",
      "epoch 478; iter: 200; batch classifier loss: 0.394597\n",
      "epoch 479; iter: 0; batch classifier loss: 0.428494\n",
      "epoch 479; iter: 200; batch classifier loss: 0.451735\n",
      "epoch 480; iter: 0; batch classifier loss: 0.420559\n",
      "epoch 480; iter: 200; batch classifier loss: 0.425708\n",
      "epoch 481; iter: 0; batch classifier loss: 0.467147\n",
      "epoch 481; iter: 200; batch classifier loss: 0.377200\n",
      "epoch 482; iter: 0; batch classifier loss: 0.427709\n",
      "epoch 482; iter: 200; batch classifier loss: 0.404878\n",
      "epoch 483; iter: 0; batch classifier loss: 0.390715\n",
      "epoch 483; iter: 200; batch classifier loss: 0.435734\n",
      "epoch 484; iter: 0; batch classifier loss: 0.442433\n",
      "epoch 484; iter: 200; batch classifier loss: 0.470721\n",
      "epoch 485; iter: 0; batch classifier loss: 0.384442\n",
      "epoch 485; iter: 200; batch classifier loss: 0.380192\n",
      "epoch 486; iter: 0; batch classifier loss: 0.408372\n",
      "epoch 486; iter: 200; batch classifier loss: 0.407784\n",
      "epoch 487; iter: 0; batch classifier loss: 0.420964\n",
      "epoch 487; iter: 200; batch classifier loss: 0.438662\n",
      "epoch 488; iter: 0; batch classifier loss: 0.400832\n",
      "epoch 488; iter: 200; batch classifier loss: 0.378569\n",
      "epoch 489; iter: 0; batch classifier loss: 0.445269\n",
      "epoch 489; iter: 200; batch classifier loss: 0.416476\n",
      "epoch 490; iter: 0; batch classifier loss: 0.503275\n",
      "epoch 490; iter: 200; batch classifier loss: 0.477752\n",
      "epoch 491; iter: 0; batch classifier loss: 0.443003\n",
      "epoch 491; iter: 200; batch classifier loss: 0.447677\n",
      "epoch 492; iter: 0; batch classifier loss: 0.409923\n",
      "epoch 492; iter: 200; batch classifier loss: 0.402021\n",
      "epoch 493; iter: 0; batch classifier loss: 0.435807\n",
      "epoch 493; iter: 200; batch classifier loss: 0.414260\n",
      "epoch 494; iter: 0; batch classifier loss: 0.434484\n",
      "epoch 494; iter: 200; batch classifier loss: 0.449786\n",
      "epoch 495; iter: 0; batch classifier loss: 0.410412\n",
      "epoch 495; iter: 200; batch classifier loss: 0.399444\n",
      "epoch 496; iter: 0; batch classifier loss: 0.394460\n",
      "epoch 496; iter: 200; batch classifier loss: 0.376957\n",
      "epoch 497; iter: 0; batch classifier loss: 0.527675\n",
      "epoch 497; iter: 200; batch classifier loss: 0.467206\n",
      "epoch 498; iter: 0; batch classifier loss: 0.471545\n",
      "epoch 498; iter: 200; batch classifier loss: 0.444205\n",
      "epoch 499; iter: 0; batch classifier loss: 0.529488\n",
      "epoch 499; iter: 200; batch classifier loss: 0.503160\n",
      "epoch 500; iter: 0; batch classifier loss: 0.459997\n",
      "epoch 500; iter: 200; batch classifier loss: 0.426324\n",
      "epoch 501; iter: 0; batch classifier loss: 0.359113\n",
      "epoch 501; iter: 200; batch classifier loss: 0.529242\n",
      "epoch 502; iter: 0; batch classifier loss: 0.372119\n",
      "epoch 502; iter: 200; batch classifier loss: 0.370761\n",
      "epoch 503; iter: 0; batch classifier loss: 0.385410\n",
      "epoch 503; iter: 200; batch classifier loss: 0.395915\n",
      "epoch 504; iter: 0; batch classifier loss: 0.363960\n",
      "epoch 504; iter: 200; batch classifier loss: 0.324418\n",
      "epoch 505; iter: 0; batch classifier loss: 0.355979\n",
      "epoch 505; iter: 200; batch classifier loss: 0.443371\n",
      "epoch 506; iter: 0; batch classifier loss: 0.561526\n",
      "epoch 506; iter: 200; batch classifier loss: 0.413157\n",
      "epoch 507; iter: 0; batch classifier loss: 0.373843\n",
      "epoch 507; iter: 200; batch classifier loss: 0.341070\n",
      "epoch 508; iter: 0; batch classifier loss: 0.460270\n",
      "epoch 508; iter: 200; batch classifier loss: 0.441742\n",
      "epoch 509; iter: 0; batch classifier loss: 0.470324\n",
      "epoch 509; iter: 200; batch classifier loss: 0.369932\n",
      "epoch 510; iter: 0; batch classifier loss: 0.467529\n",
      "epoch 510; iter: 200; batch classifier loss: 0.471487\n",
      "epoch 511; iter: 0; batch classifier loss: 0.347419\n",
      "epoch 511; iter: 200; batch classifier loss: 0.449481\n",
      "epoch 512; iter: 0; batch classifier loss: 0.422134\n",
      "epoch 512; iter: 200; batch classifier loss: 0.402998\n",
      "epoch 513; iter: 0; batch classifier loss: 0.349446\n",
      "epoch 513; iter: 200; batch classifier loss: 0.418518\n",
      "epoch 514; iter: 0; batch classifier loss: 0.462721\n",
      "epoch 514; iter: 200; batch classifier loss: 0.364372\n",
      "epoch 515; iter: 0; batch classifier loss: 0.387890\n",
      "epoch 515; iter: 200; batch classifier loss: 0.458697\n",
      "epoch 516; iter: 0; batch classifier loss: 0.422406\n",
      "epoch 516; iter: 200; batch classifier loss: 0.390864\n",
      "epoch 517; iter: 0; batch classifier loss: 0.435009\n",
      "epoch 517; iter: 200; batch classifier loss: 0.419041\n",
      "epoch 518; iter: 0; batch classifier loss: 0.391314\n",
      "epoch 518; iter: 200; batch classifier loss: 0.423571\n",
      "epoch 519; iter: 0; batch classifier loss: 0.451206\n",
      "epoch 519; iter: 200; batch classifier loss: 0.482126\n",
      "epoch 520; iter: 0; batch classifier loss: 0.364534\n",
      "epoch 520; iter: 200; batch classifier loss: 0.423112\n",
      "epoch 521; iter: 0; batch classifier loss: 0.355790\n",
      "epoch 521; iter: 200; batch classifier loss: 0.423150\n",
      "epoch 522; iter: 0; batch classifier loss: 0.425124\n",
      "epoch 522; iter: 200; batch classifier loss: 0.389251\n",
      "epoch 523; iter: 0; batch classifier loss: 0.404065\n",
      "epoch 523; iter: 200; batch classifier loss: 0.506175\n",
      "epoch 524; iter: 0; batch classifier loss: 0.432118\n",
      "epoch 524; iter: 200; batch classifier loss: 0.458429\n",
      "epoch 525; iter: 0; batch classifier loss: 0.371013\n",
      "epoch 525; iter: 200; batch classifier loss: 0.442626\n",
      "epoch 526; iter: 0; batch classifier loss: 0.450000\n",
      "epoch 526; iter: 200; batch classifier loss: 0.401515\n",
      "epoch 527; iter: 0; batch classifier loss: 0.484215\n",
      "epoch 527; iter: 200; batch classifier loss: 0.489076\n",
      "epoch 528; iter: 0; batch classifier loss: 0.461228\n",
      "epoch 528; iter: 200; batch classifier loss: 0.391506\n",
      "epoch 529; iter: 0; batch classifier loss: 0.460554\n",
      "epoch 529; iter: 200; batch classifier loss: 0.428513\n",
      "epoch 530; iter: 0; batch classifier loss: 0.366961\n",
      "epoch 530; iter: 200; batch classifier loss: 0.477608\n",
      "epoch 531; iter: 0; batch classifier loss: 0.401639\n",
      "epoch 531; iter: 200; batch classifier loss: 0.451768\n",
      "epoch 532; iter: 0; batch classifier loss: 0.401265\n",
      "epoch 532; iter: 200; batch classifier loss: 0.395561\n",
      "epoch 533; iter: 0; batch classifier loss: 0.474337\n",
      "epoch 533; iter: 200; batch classifier loss: 0.510336\n",
      "epoch 534; iter: 0; batch classifier loss: 0.495855\n",
      "epoch 534; iter: 200; batch classifier loss: 0.387561\n",
      "epoch 535; iter: 0; batch classifier loss: 0.454976\n",
      "epoch 535; iter: 200; batch classifier loss: 0.564180\n",
      "epoch 536; iter: 0; batch classifier loss: 0.386114\n",
      "epoch 536; iter: 200; batch classifier loss: 0.330875\n",
      "epoch 537; iter: 0; batch classifier loss: 0.465692\n",
      "epoch 537; iter: 200; batch classifier loss: 0.349343\n",
      "epoch 538; iter: 0; batch classifier loss: 0.414269\n",
      "epoch 538; iter: 200; batch classifier loss: 0.394763\n",
      "epoch 539; iter: 0; batch classifier loss: 0.362655\n",
      "epoch 539; iter: 200; batch classifier loss: 0.404535\n",
      "epoch 540; iter: 0; batch classifier loss: 0.413281\n",
      "epoch 540; iter: 200; batch classifier loss: 0.407859\n",
      "epoch 541; iter: 0; batch classifier loss: 0.342063\n",
      "epoch 541; iter: 200; batch classifier loss: 0.434851\n",
      "epoch 542; iter: 0; batch classifier loss: 0.372386\n",
      "epoch 542; iter: 200; batch classifier loss: 0.537309\n",
      "epoch 543; iter: 0; batch classifier loss: 0.479839\n",
      "epoch 543; iter: 200; batch classifier loss: 0.381586\n",
      "epoch 544; iter: 0; batch classifier loss: 0.371166\n",
      "epoch 544; iter: 200; batch classifier loss: 0.383108\n",
      "epoch 545; iter: 0; batch classifier loss: 0.399382\n",
      "epoch 545; iter: 200; batch classifier loss: 0.368195\n",
      "epoch 546; iter: 0; batch classifier loss: 0.414070\n",
      "epoch 546; iter: 200; batch classifier loss: 0.333827\n",
      "epoch 547; iter: 0; batch classifier loss: 0.380703\n",
      "epoch 547; iter: 200; batch classifier loss: 0.369635\n",
      "epoch 548; iter: 0; batch classifier loss: 0.389747\n",
      "epoch 548; iter: 200; batch classifier loss: 0.507268\n",
      "epoch 549; iter: 0; batch classifier loss: 0.399434\n",
      "epoch 549; iter: 200; batch classifier loss: 0.458004\n",
      "epoch 550; iter: 0; batch classifier loss: 0.449667\n",
      "epoch 550; iter: 200; batch classifier loss: 0.437123\n",
      "epoch 551; iter: 0; batch classifier loss: 0.394638\n",
      "epoch 551; iter: 200; batch classifier loss: 0.385330\n",
      "epoch 552; iter: 0; batch classifier loss: 0.437838\n",
      "epoch 552; iter: 200; batch classifier loss: 0.386333\n",
      "epoch 553; iter: 0; batch classifier loss: 0.506501\n",
      "epoch 553; iter: 200; batch classifier loss: 0.390548\n",
      "epoch 554; iter: 0; batch classifier loss: 0.417588\n",
      "epoch 554; iter: 200; batch classifier loss: 0.381286\n",
      "epoch 555; iter: 0; batch classifier loss: 0.298005\n",
      "epoch 555; iter: 200; batch classifier loss: 0.502926\n",
      "epoch 556; iter: 0; batch classifier loss: 0.416274\n",
      "epoch 556; iter: 200; batch classifier loss: 0.476553\n",
      "epoch 557; iter: 0; batch classifier loss: 0.472459\n",
      "epoch 557; iter: 200; batch classifier loss: 0.509555\n",
      "epoch 558; iter: 0; batch classifier loss: 0.412885\n",
      "epoch 558; iter: 200; batch classifier loss: 0.466154\n",
      "epoch 559; iter: 0; batch classifier loss: 0.384463\n",
      "epoch 559; iter: 200; batch classifier loss: 0.410684\n",
      "epoch 560; iter: 0; batch classifier loss: 0.401488\n",
      "epoch 560; iter: 200; batch classifier loss: 0.415938\n",
      "epoch 561; iter: 0; batch classifier loss: 0.453541\n",
      "epoch 561; iter: 200; batch classifier loss: 0.387416\n",
      "epoch 562; iter: 0; batch classifier loss: 0.369945\n",
      "epoch 562; iter: 200; batch classifier loss: 0.418551\n",
      "epoch 563; iter: 0; batch classifier loss: 0.394555\n",
      "epoch 563; iter: 200; batch classifier loss: 0.452234\n",
      "epoch 564; iter: 0; batch classifier loss: 0.372820\n",
      "epoch 564; iter: 200; batch classifier loss: 0.404520\n",
      "epoch 565; iter: 0; batch classifier loss: 0.455018\n",
      "epoch 565; iter: 200; batch classifier loss: 0.463824\n",
      "epoch 566; iter: 0; batch classifier loss: 0.459587\n",
      "epoch 566; iter: 200; batch classifier loss: 0.451334\n",
      "epoch 567; iter: 0; batch classifier loss: 0.437569\n",
      "epoch 567; iter: 200; batch classifier loss: 0.386892\n",
      "epoch 568; iter: 0; batch classifier loss: 0.467812\n",
      "epoch 568; iter: 200; batch classifier loss: 0.344475\n",
      "epoch 569; iter: 0; batch classifier loss: 0.456610\n",
      "epoch 569; iter: 200; batch classifier loss: 0.445599\n",
      "epoch 570; iter: 0; batch classifier loss: 0.361796\n",
      "epoch 570; iter: 200; batch classifier loss: 0.439621\n",
      "epoch 571; iter: 0; batch classifier loss: 0.405179\n",
      "epoch 571; iter: 200; batch classifier loss: 0.533351\n",
      "epoch 572; iter: 0; batch classifier loss: 0.428258\n",
      "epoch 572; iter: 200; batch classifier loss: 0.475448\n",
      "epoch 573; iter: 0; batch classifier loss: 0.427158\n",
      "epoch 573; iter: 200; batch classifier loss: 0.412690\n",
      "epoch 574; iter: 0; batch classifier loss: 0.385351\n",
      "epoch 574; iter: 200; batch classifier loss: 0.389199\n",
      "epoch 575; iter: 0; batch classifier loss: 0.435152\n",
      "epoch 575; iter: 200; batch classifier loss: 0.453400\n",
      "epoch 576; iter: 0; batch classifier loss: 0.411968\n",
      "epoch 576; iter: 200; batch classifier loss: 0.438216\n",
      "epoch 577; iter: 0; batch classifier loss: 0.354724\n",
      "epoch 577; iter: 200; batch classifier loss: 0.397114\n",
      "epoch 578; iter: 0; batch classifier loss: 0.447723\n",
      "epoch 578; iter: 200; batch classifier loss: 0.485207\n",
      "epoch 579; iter: 0; batch classifier loss: 0.483168\n",
      "epoch 579; iter: 200; batch classifier loss: 0.418409\n",
      "epoch 580; iter: 0; batch classifier loss: 0.406335\n",
      "epoch 580; iter: 200; batch classifier loss: 0.387991\n",
      "epoch 581; iter: 0; batch classifier loss: 0.415628\n",
      "epoch 581; iter: 200; batch classifier loss: 0.327309\n",
      "epoch 582; iter: 0; batch classifier loss: 0.414611\n",
      "epoch 582; iter: 200; batch classifier loss: 0.411008\n",
      "epoch 583; iter: 0; batch classifier loss: 0.403557\n",
      "epoch 583; iter: 200; batch classifier loss: 0.477491\n",
      "epoch 584; iter: 0; batch classifier loss: 0.389814\n",
      "epoch 584; iter: 200; batch classifier loss: 0.425439\n",
      "epoch 585; iter: 0; batch classifier loss: 0.478810\n",
      "epoch 585; iter: 200; batch classifier loss: 0.392852\n",
      "epoch 586; iter: 0; batch classifier loss: 0.300864\n",
      "epoch 586; iter: 200; batch classifier loss: 0.438450\n",
      "epoch 587; iter: 0; batch classifier loss: 0.396828\n",
      "epoch 587; iter: 200; batch classifier loss: 0.478335\n",
      "epoch 588; iter: 0; batch classifier loss: 0.417529\n",
      "epoch 588; iter: 200; batch classifier loss: 0.493549\n",
      "epoch 589; iter: 0; batch classifier loss: 0.410723\n",
      "epoch 589; iter: 200; batch classifier loss: 0.395681\n",
      "epoch 590; iter: 0; batch classifier loss: 0.367685\n",
      "epoch 590; iter: 200; batch classifier loss: 0.374381\n",
      "epoch 591; iter: 0; batch classifier loss: 0.419735\n",
      "epoch 591; iter: 200; batch classifier loss: 0.412970\n",
      "epoch 592; iter: 0; batch classifier loss: 0.363253\n",
      "epoch 592; iter: 200; batch classifier loss: 0.431543\n",
      "epoch 593; iter: 0; batch classifier loss: 0.338576\n",
      "epoch 593; iter: 200; batch classifier loss: 0.441087\n",
      "epoch 594; iter: 0; batch classifier loss: 0.433723\n",
      "epoch 594; iter: 200; batch classifier loss: 0.400152\n",
      "epoch 595; iter: 0; batch classifier loss: 0.393099\n",
      "epoch 595; iter: 200; batch classifier loss: 0.462761\n",
      "epoch 596; iter: 0; batch classifier loss: 0.351315\n",
      "epoch 596; iter: 200; batch classifier loss: 0.396191\n",
      "epoch 597; iter: 0; batch classifier loss: 0.436388\n",
      "epoch 597; iter: 200; batch classifier loss: 0.371564\n",
      "epoch 598; iter: 0; batch classifier loss: 0.418585\n",
      "epoch 598; iter: 200; batch classifier loss: 0.433937\n",
      "epoch 599; iter: 0; batch classifier loss: 0.413379\n",
      "epoch 599; iter: 200; batch classifier loss: 0.460278\n",
      "epoch 600; iter: 0; batch classifier loss: 0.401308\n",
      "epoch 600; iter: 200; batch classifier loss: 0.446215\n",
      "epoch 601; iter: 0; batch classifier loss: 0.362478\n",
      "epoch 601; iter: 200; batch classifier loss: 0.326228\n",
      "epoch 602; iter: 0; batch classifier loss: 0.419952\n",
      "epoch 602; iter: 200; batch classifier loss: 0.506745\n",
      "epoch 603; iter: 0; batch classifier loss: 0.418437\n",
      "epoch 603; iter: 200; batch classifier loss: 0.338562\n",
      "epoch 604; iter: 0; batch classifier loss: 0.478777\n",
      "epoch 604; iter: 200; batch classifier loss: 0.457364\n",
      "epoch 605; iter: 0; batch classifier loss: 0.474462\n",
      "epoch 605; iter: 200; batch classifier loss: 0.375925\n",
      "epoch 606; iter: 0; batch classifier loss: 0.359634\n",
      "epoch 606; iter: 200; batch classifier loss: 0.433156\n",
      "epoch 607; iter: 0; batch classifier loss: 0.423874\n",
      "epoch 607; iter: 200; batch classifier loss: 0.473289\n",
      "epoch 608; iter: 0; batch classifier loss: 0.407868\n",
      "epoch 608; iter: 200; batch classifier loss: 0.418430\n",
      "epoch 609; iter: 0; batch classifier loss: 0.325397\n",
      "epoch 609; iter: 200; batch classifier loss: 0.499618\n",
      "epoch 610; iter: 0; batch classifier loss: 0.352976\n",
      "epoch 610; iter: 200; batch classifier loss: 0.432499\n",
      "epoch 611; iter: 0; batch classifier loss: 0.354759\n",
      "epoch 611; iter: 200; batch classifier loss: 0.498010\n",
      "epoch 612; iter: 0; batch classifier loss: 0.424270\n",
      "epoch 612; iter: 200; batch classifier loss: 0.436598\n",
      "epoch 613; iter: 0; batch classifier loss: 0.479401\n",
      "epoch 613; iter: 200; batch classifier loss: 0.440424\n",
      "epoch 614; iter: 0; batch classifier loss: 0.406062\n",
      "epoch 614; iter: 200; batch classifier loss: 0.474319\n",
      "epoch 615; iter: 0; batch classifier loss: 0.481898\n",
      "epoch 615; iter: 200; batch classifier loss: 0.443149\n",
      "epoch 616; iter: 0; batch classifier loss: 0.437534\n",
      "epoch 616; iter: 200; batch classifier loss: 0.390678\n",
      "epoch 617; iter: 0; batch classifier loss: 0.425434\n",
      "epoch 617; iter: 200; batch classifier loss: 0.402964\n",
      "epoch 618; iter: 0; batch classifier loss: 0.371834\n",
      "epoch 618; iter: 200; batch classifier loss: 0.342954\n",
      "epoch 619; iter: 0; batch classifier loss: 0.416839\n",
      "epoch 619; iter: 200; batch classifier loss: 0.410302\n",
      "epoch 620; iter: 0; batch classifier loss: 0.404130\n",
      "epoch 620; iter: 200; batch classifier loss: 0.288858\n",
      "epoch 621; iter: 0; batch classifier loss: 0.410591\n",
      "epoch 621; iter: 200; batch classifier loss: 0.390076\n",
      "epoch 622; iter: 0; batch classifier loss: 0.371622\n",
      "epoch 622; iter: 200; batch classifier loss: 0.438104\n",
      "epoch 623; iter: 0; batch classifier loss: 0.457100\n",
      "epoch 623; iter: 200; batch classifier loss: 0.390485\n",
      "epoch 624; iter: 0; batch classifier loss: 0.433117\n",
      "epoch 624; iter: 200; batch classifier loss: 0.361843\n",
      "epoch 625; iter: 0; batch classifier loss: 0.336084\n",
      "epoch 625; iter: 200; batch classifier loss: 0.452504\n",
      "epoch 626; iter: 0; batch classifier loss: 0.382455\n",
      "epoch 626; iter: 200; batch classifier loss: 0.383401\n",
      "epoch 627; iter: 0; batch classifier loss: 0.389804\n",
      "epoch 627; iter: 200; batch classifier loss: 0.354370\n",
      "epoch 628; iter: 0; batch classifier loss: 0.427757\n",
      "epoch 628; iter: 200; batch classifier loss: 0.456607\n",
      "epoch 629; iter: 0; batch classifier loss: 0.381248\n",
      "epoch 629; iter: 200; batch classifier loss: 0.455990\n",
      "epoch 630; iter: 0; batch classifier loss: 0.385398\n",
      "epoch 630; iter: 200; batch classifier loss: 0.527161\n",
      "epoch 631; iter: 0; batch classifier loss: 0.373138\n",
      "epoch 631; iter: 200; batch classifier loss: 0.461518\n",
      "epoch 632; iter: 0; batch classifier loss: 0.323593\n",
      "epoch 632; iter: 200; batch classifier loss: 0.452965\n",
      "epoch 633; iter: 0; batch classifier loss: 0.365597\n",
      "epoch 633; iter: 200; batch classifier loss: 0.363842\n",
      "epoch 634; iter: 0; batch classifier loss: 0.381777\n",
      "epoch 634; iter: 200; batch classifier loss: 0.400030\n",
      "epoch 635; iter: 0; batch classifier loss: 0.313110\n",
      "epoch 635; iter: 200; batch classifier loss: 0.407635\n",
      "epoch 636; iter: 0; batch classifier loss: 0.410073\n",
      "epoch 636; iter: 200; batch classifier loss: 0.393478\n",
      "epoch 637; iter: 0; batch classifier loss: 0.424933\n",
      "epoch 637; iter: 200; batch classifier loss: 0.376508\n",
      "epoch 638; iter: 0; batch classifier loss: 0.400636\n",
      "epoch 638; iter: 200; batch classifier loss: 0.504568\n",
      "epoch 639; iter: 0; batch classifier loss: 0.503966\n",
      "epoch 639; iter: 200; batch classifier loss: 0.356142\n",
      "epoch 640; iter: 0; batch classifier loss: 0.440934\n",
      "epoch 640; iter: 200; batch classifier loss: 0.449594\n",
      "epoch 641; iter: 0; batch classifier loss: 0.379970\n",
      "epoch 641; iter: 200; batch classifier loss: 0.375340\n",
      "epoch 642; iter: 0; batch classifier loss: 0.490578\n",
      "epoch 642; iter: 200; batch classifier loss: 0.427975\n",
      "epoch 643; iter: 0; batch classifier loss: 0.420306\n",
      "epoch 643; iter: 200; batch classifier loss: 0.448342\n",
      "epoch 644; iter: 0; batch classifier loss: 0.398673\n",
      "epoch 644; iter: 200; batch classifier loss: 0.386999\n",
      "epoch 645; iter: 0; batch classifier loss: 0.380865\n",
      "epoch 645; iter: 200; batch classifier loss: 0.401442\n",
      "epoch 646; iter: 0; batch classifier loss: 0.374921\n",
      "epoch 646; iter: 200; batch classifier loss: 0.354563\n",
      "epoch 647; iter: 0; batch classifier loss: 0.475277\n",
      "epoch 647; iter: 200; batch classifier loss: 0.330134\n",
      "epoch 648; iter: 0; batch classifier loss: 0.424388\n",
      "epoch 648; iter: 200; batch classifier loss: 0.357513\n",
      "epoch 649; iter: 0; batch classifier loss: 0.408957\n",
      "epoch 649; iter: 200; batch classifier loss: 0.421501\n",
      "epoch 650; iter: 0; batch classifier loss: 0.412472\n",
      "epoch 650; iter: 200; batch classifier loss: 0.352106\n",
      "epoch 651; iter: 0; batch classifier loss: 0.366447\n",
      "epoch 651; iter: 200; batch classifier loss: 0.416838\n",
      "epoch 652; iter: 0; batch classifier loss: 0.350156\n",
      "epoch 652; iter: 200; batch classifier loss: 0.447392\n",
      "epoch 653; iter: 0; batch classifier loss: 0.347792\n",
      "epoch 653; iter: 200; batch classifier loss: 0.404426\n",
      "epoch 654; iter: 0; batch classifier loss: 0.389268\n",
      "epoch 654; iter: 200; batch classifier loss: 0.326563\n",
      "epoch 655; iter: 0; batch classifier loss: 0.429480\n",
      "epoch 655; iter: 200; batch classifier loss: 0.374317\n",
      "epoch 656; iter: 0; batch classifier loss: 0.499819\n",
      "epoch 656; iter: 200; batch classifier loss: 0.374965\n",
      "epoch 657; iter: 0; batch classifier loss: 0.424767\n",
      "epoch 657; iter: 200; batch classifier loss: 0.424601\n",
      "epoch 658; iter: 0; batch classifier loss: 0.465903\n",
      "epoch 658; iter: 200; batch classifier loss: 0.383295\n",
      "epoch 659; iter: 0; batch classifier loss: 0.371288\n",
      "epoch 659; iter: 200; batch classifier loss: 0.456664\n",
      "epoch 660; iter: 0; batch classifier loss: 0.411725\n",
      "epoch 660; iter: 200; batch classifier loss: 0.425420\n",
      "epoch 661; iter: 0; batch classifier loss: 0.463697\n",
      "epoch 661; iter: 200; batch classifier loss: 0.510940\n",
      "epoch 662; iter: 0; batch classifier loss: 0.450697\n",
      "epoch 662; iter: 200; batch classifier loss: 0.393421\n",
      "epoch 663; iter: 0; batch classifier loss: 0.447882\n",
      "epoch 663; iter: 200; batch classifier loss: 0.421410\n",
      "epoch 664; iter: 0; batch classifier loss: 0.416291\n",
      "epoch 664; iter: 200; batch classifier loss: 0.393195\n",
      "epoch 665; iter: 0; batch classifier loss: 0.423890\n",
      "epoch 665; iter: 200; batch classifier loss: 0.345441\n",
      "epoch 666; iter: 0; batch classifier loss: 0.317256\n",
      "epoch 666; iter: 200; batch classifier loss: 0.393455\n",
      "epoch 667; iter: 0; batch classifier loss: 0.424360\n",
      "epoch 667; iter: 200; batch classifier loss: 0.519716\n",
      "epoch 668; iter: 0; batch classifier loss: 0.390701\n",
      "epoch 668; iter: 200; batch classifier loss: 0.466293\n",
      "epoch 669; iter: 0; batch classifier loss: 0.415320\n",
      "epoch 669; iter: 200; batch classifier loss: 0.412707\n",
      "epoch 670; iter: 0; batch classifier loss: 0.443388\n",
      "epoch 670; iter: 200; batch classifier loss: 0.413039\n",
      "epoch 671; iter: 0; batch classifier loss: 0.429917\n",
      "epoch 671; iter: 200; batch classifier loss: 0.395346\n",
      "epoch 672; iter: 0; batch classifier loss: 0.293732\n",
      "epoch 672; iter: 200; batch classifier loss: 0.363327\n",
      "epoch 673; iter: 0; batch classifier loss: 0.392918\n",
      "epoch 673; iter: 200; batch classifier loss: 0.424461\n",
      "epoch 674; iter: 0; batch classifier loss: 0.456281\n",
      "epoch 674; iter: 200; batch classifier loss: 0.369014\n",
      "epoch 675; iter: 0; batch classifier loss: 0.401368\n",
      "epoch 675; iter: 200; batch classifier loss: 0.464008\n",
      "epoch 676; iter: 0; batch classifier loss: 0.404377\n",
      "epoch 676; iter: 200; batch classifier loss: 0.339492\n",
      "epoch 677; iter: 0; batch classifier loss: 0.445588\n",
      "epoch 677; iter: 200; batch classifier loss: 0.355543\n",
      "epoch 678; iter: 0; batch classifier loss: 0.576113\n",
      "epoch 678; iter: 200; batch classifier loss: 0.352391\n",
      "epoch 679; iter: 0; batch classifier loss: 0.362282\n",
      "epoch 679; iter: 200; batch classifier loss: 0.419132\n",
      "epoch 680; iter: 0; batch classifier loss: 0.428310\n",
      "epoch 680; iter: 200; batch classifier loss: 0.423255\n",
      "epoch 681; iter: 0; batch classifier loss: 0.425326\n",
      "epoch 681; iter: 200; batch classifier loss: 0.420464\n",
      "epoch 682; iter: 0; batch classifier loss: 0.405370\n",
      "epoch 682; iter: 200; batch classifier loss: 0.407551\n",
      "epoch 683; iter: 0; batch classifier loss: 0.426553\n",
      "epoch 683; iter: 200; batch classifier loss: 0.448144\n",
      "epoch 684; iter: 0; batch classifier loss: 0.389141\n",
      "epoch 684; iter: 200; batch classifier loss: 0.354966\n",
      "epoch 685; iter: 0; batch classifier loss: 0.393557\n",
      "epoch 685; iter: 200; batch classifier loss: 0.468570\n",
      "epoch 686; iter: 0; batch classifier loss: 0.331290\n",
      "epoch 686; iter: 200; batch classifier loss: 0.463400\n",
      "epoch 687; iter: 0; batch classifier loss: 0.413109\n",
      "epoch 687; iter: 200; batch classifier loss: 0.409236\n",
      "epoch 688; iter: 0; batch classifier loss: 0.380953\n",
      "epoch 688; iter: 200; batch classifier loss: 0.496019\n",
      "epoch 689; iter: 0; batch classifier loss: 0.457668\n",
      "epoch 689; iter: 200; batch classifier loss: 0.365767\n",
      "epoch 690; iter: 0; batch classifier loss: 0.453196\n",
      "epoch 690; iter: 200; batch classifier loss: 0.383782\n",
      "epoch 691; iter: 0; batch classifier loss: 0.386196\n",
      "epoch 691; iter: 200; batch classifier loss: 0.380789\n",
      "epoch 692; iter: 0; batch classifier loss: 0.418110\n",
      "epoch 692; iter: 200; batch classifier loss: 0.372239\n",
      "epoch 693; iter: 0; batch classifier loss: 0.435601\n",
      "epoch 693; iter: 200; batch classifier loss: 0.406515\n",
      "epoch 694; iter: 0; batch classifier loss: 0.431630\n",
      "epoch 694; iter: 200; batch classifier loss: 0.447035\n",
      "epoch 695; iter: 0; batch classifier loss: 0.450695\n",
      "epoch 695; iter: 200; batch classifier loss: 0.392957\n",
      "epoch 696; iter: 0; batch classifier loss: 0.395249\n",
      "epoch 696; iter: 200; batch classifier loss: 0.429696\n",
      "epoch 697; iter: 0; batch classifier loss: 0.490534\n",
      "epoch 697; iter: 200; batch classifier loss: 0.421568\n",
      "epoch 698; iter: 0; batch classifier loss: 0.414166\n",
      "epoch 698; iter: 200; batch classifier loss: 0.437310\n",
      "epoch 699; iter: 0; batch classifier loss: 0.464481\n",
      "epoch 699; iter: 200; batch classifier loss: 0.427628\n",
      "epoch 700; iter: 0; batch classifier loss: 0.422729\n",
      "epoch 700; iter: 200; batch classifier loss: 0.355474\n",
      "epoch 701; iter: 0; batch classifier loss: 0.465184\n",
      "epoch 701; iter: 200; batch classifier loss: 0.395613\n",
      "epoch 702; iter: 0; batch classifier loss: 0.405252\n",
      "epoch 702; iter: 200; batch classifier loss: 0.398101\n",
      "epoch 703; iter: 0; batch classifier loss: 0.443718\n",
      "epoch 703; iter: 200; batch classifier loss: 0.478450\n",
      "epoch 704; iter: 0; batch classifier loss: 0.455736\n",
      "epoch 704; iter: 200; batch classifier loss: 0.303866\n",
      "epoch 705; iter: 0; batch classifier loss: 0.428442\n",
      "epoch 705; iter: 200; batch classifier loss: 0.426876\n",
      "epoch 706; iter: 0; batch classifier loss: 0.428782\n",
      "epoch 706; iter: 200; batch classifier loss: 0.448034\n",
      "epoch 707; iter: 0; batch classifier loss: 0.401852\n",
      "epoch 707; iter: 200; batch classifier loss: 0.415220\n",
      "epoch 708; iter: 0; batch classifier loss: 0.462955\n",
      "epoch 708; iter: 200; batch classifier loss: 0.450174\n",
      "epoch 709; iter: 0; batch classifier loss: 0.427896\n",
      "epoch 709; iter: 200; batch classifier loss: 0.401060\n",
      "epoch 710; iter: 0; batch classifier loss: 0.305458\n",
      "epoch 710; iter: 200; batch classifier loss: 0.418245\n",
      "epoch 711; iter: 0; batch classifier loss: 0.449904\n",
      "epoch 711; iter: 200; batch classifier loss: 0.470191\n",
      "epoch 712; iter: 0; batch classifier loss: 0.403014\n",
      "epoch 712; iter: 200; batch classifier loss: 0.450398\n",
      "epoch 713; iter: 0; batch classifier loss: 0.489557\n",
      "epoch 713; iter: 200; batch classifier loss: 0.409380\n",
      "epoch 714; iter: 0; batch classifier loss: 0.401412\n",
      "epoch 714; iter: 200; batch classifier loss: 0.322796\n",
      "epoch 715; iter: 0; batch classifier loss: 0.389628\n",
      "epoch 715; iter: 200; batch classifier loss: 0.383794\n",
      "epoch 716; iter: 0; batch classifier loss: 0.455587\n",
      "epoch 716; iter: 200; batch classifier loss: 0.377911\n",
      "epoch 717; iter: 0; batch classifier loss: 0.406608\n",
      "epoch 717; iter: 200; batch classifier loss: 0.364790\n",
      "epoch 718; iter: 0; batch classifier loss: 0.437518\n",
      "epoch 718; iter: 200; batch classifier loss: 0.425812\n",
      "epoch 719; iter: 0; batch classifier loss: 0.480655\n",
      "epoch 719; iter: 200; batch classifier loss: 0.414525\n",
      "epoch 720; iter: 0; batch classifier loss: 0.491724\n",
      "epoch 720; iter: 200; batch classifier loss: 0.367163\n",
      "epoch 721; iter: 0; batch classifier loss: 0.420142\n",
      "epoch 721; iter: 200; batch classifier loss: 0.444154\n",
      "epoch 722; iter: 0; batch classifier loss: 0.305520\n",
      "epoch 722; iter: 200; batch classifier loss: 0.471170\n",
      "epoch 723; iter: 0; batch classifier loss: 0.446813\n",
      "epoch 723; iter: 200; batch classifier loss: 0.457495\n",
      "epoch 724; iter: 0; batch classifier loss: 0.435598\n",
      "epoch 724; iter: 200; batch classifier loss: 0.347811\n",
      "epoch 725; iter: 0; batch classifier loss: 0.354052\n",
      "epoch 725; iter: 200; batch classifier loss: 0.418145\n",
      "epoch 726; iter: 0; batch classifier loss: 0.322055\n",
      "epoch 726; iter: 200; batch classifier loss: 0.461103\n",
      "epoch 727; iter: 0; batch classifier loss: 0.402705\n",
      "epoch 727; iter: 200; batch classifier loss: 0.423210\n",
      "epoch 728; iter: 0; batch classifier loss: 0.440199\n",
      "epoch 728; iter: 200; batch classifier loss: 0.435348\n",
      "epoch 729; iter: 0; batch classifier loss: 0.382550\n",
      "epoch 729; iter: 200; batch classifier loss: 0.498125\n",
      "epoch 730; iter: 0; batch classifier loss: 0.382351\n",
      "epoch 730; iter: 200; batch classifier loss: 0.466188\n",
      "epoch 731; iter: 0; batch classifier loss: 0.429619\n",
      "epoch 731; iter: 200; batch classifier loss: 0.402438\n",
      "epoch 732; iter: 0; batch classifier loss: 0.323645\n",
      "epoch 732; iter: 200; batch classifier loss: 0.369906\n",
      "epoch 733; iter: 0; batch classifier loss: 0.411313\n",
      "epoch 733; iter: 200; batch classifier loss: 0.395008\n",
      "epoch 734; iter: 0; batch classifier loss: 0.398032\n",
      "epoch 734; iter: 200; batch classifier loss: 0.434499\n",
      "epoch 735; iter: 0; batch classifier loss: 0.484790\n",
      "epoch 735; iter: 200; batch classifier loss: 0.397646\n",
      "epoch 736; iter: 0; batch classifier loss: 0.342371\n",
      "epoch 736; iter: 200; batch classifier loss: 0.372252\n",
      "epoch 737; iter: 0; batch classifier loss: 0.392344\n",
      "epoch 737; iter: 200; batch classifier loss: 0.418004\n",
      "epoch 738; iter: 0; batch classifier loss: 0.492110\n",
      "epoch 738; iter: 200; batch classifier loss: 0.486625\n",
      "epoch 739; iter: 0; batch classifier loss: 0.489866\n",
      "epoch 739; iter: 200; batch classifier loss: 0.442029\n",
      "epoch 740; iter: 0; batch classifier loss: 0.386907\n",
      "epoch 740; iter: 200; batch classifier loss: 0.357118\n",
      "epoch 741; iter: 0; batch classifier loss: 0.412732\n",
      "epoch 741; iter: 200; batch classifier loss: 0.447776\n",
      "epoch 742; iter: 0; batch classifier loss: 0.393510\n",
      "epoch 742; iter: 200; batch classifier loss: 0.392964\n",
      "epoch 743; iter: 0; batch classifier loss: 0.445817\n",
      "epoch 743; iter: 200; batch classifier loss: 0.416307\n",
      "epoch 744; iter: 0; batch classifier loss: 0.402577\n",
      "epoch 744; iter: 200; batch classifier loss: 0.367759\n",
      "epoch 745; iter: 0; batch classifier loss: 0.426286\n",
      "epoch 745; iter: 200; batch classifier loss: 0.443665\n",
      "epoch 746; iter: 0; batch classifier loss: 0.380087\n",
      "epoch 746; iter: 200; batch classifier loss: 0.415604\n",
      "epoch 747; iter: 0; batch classifier loss: 0.425316\n",
      "epoch 747; iter: 200; batch classifier loss: 0.380461\n",
      "epoch 748; iter: 0; batch classifier loss: 0.395151\n",
      "epoch 748; iter: 200; batch classifier loss: 0.420498\n",
      "epoch 749; iter: 0; batch classifier loss: 0.413046\n",
      "epoch 749; iter: 200; batch classifier loss: 0.406482\n",
      "epoch 750; iter: 0; batch classifier loss: 0.401898\n",
      "epoch 750; iter: 200; batch classifier loss: 0.428587\n",
      "epoch 751; iter: 0; batch classifier loss: 0.365676\n",
      "epoch 751; iter: 200; batch classifier loss: 0.521144\n",
      "epoch 752; iter: 0; batch classifier loss: 0.375661\n",
      "epoch 752; iter: 200; batch classifier loss: 0.419839\n",
      "epoch 753; iter: 0; batch classifier loss: 0.463840\n",
      "epoch 753; iter: 200; batch classifier loss: 0.393060\n",
      "epoch 754; iter: 0; batch classifier loss: 0.436766\n",
      "epoch 754; iter: 200; batch classifier loss: 0.446113\n",
      "epoch 755; iter: 0; batch classifier loss: 0.397094\n",
      "epoch 755; iter: 200; batch classifier loss: 0.430766\n",
      "epoch 756; iter: 0; batch classifier loss: 0.449337\n",
      "epoch 756; iter: 200; batch classifier loss: 0.367188\n",
      "epoch 757; iter: 0; batch classifier loss: 0.467839\n",
      "epoch 757; iter: 200; batch classifier loss: 0.473756\n",
      "epoch 758; iter: 0; batch classifier loss: 0.495880\n",
      "epoch 758; iter: 200; batch classifier loss: 0.402303\n",
      "epoch 759; iter: 0; batch classifier loss: 0.405766\n",
      "epoch 759; iter: 200; batch classifier loss: 0.459305\n",
      "epoch 760; iter: 0; batch classifier loss: 0.406085\n",
      "epoch 760; iter: 200; batch classifier loss: 0.401538\n",
      "epoch 761; iter: 0; batch classifier loss: 0.469521\n",
      "epoch 761; iter: 200; batch classifier loss: 0.372453\n",
      "epoch 762; iter: 0; batch classifier loss: 0.369063\n",
      "epoch 762; iter: 200; batch classifier loss: 0.469964\n",
      "epoch 763; iter: 0; batch classifier loss: 0.483606\n",
      "epoch 763; iter: 200; batch classifier loss: 0.440341\n",
      "epoch 764; iter: 0; batch classifier loss: 0.467355\n",
      "epoch 764; iter: 200; batch classifier loss: 0.319147\n",
      "epoch 765; iter: 0; batch classifier loss: 0.412437\n",
      "epoch 765; iter: 200; batch classifier loss: 0.365297\n",
      "epoch 766; iter: 0; batch classifier loss: 0.387956\n",
      "epoch 766; iter: 200; batch classifier loss: 0.390091\n",
      "epoch 767; iter: 0; batch classifier loss: 0.478241\n",
      "epoch 767; iter: 200; batch classifier loss: 0.365477\n",
      "epoch 768; iter: 0; batch classifier loss: 0.463167\n",
      "epoch 768; iter: 200; batch classifier loss: 0.380670\n",
      "epoch 769; iter: 0; batch classifier loss: 0.437890\n",
      "epoch 769; iter: 200; batch classifier loss: 0.299030\n",
      "epoch 770; iter: 0; batch classifier loss: 0.439031\n",
      "epoch 770; iter: 200; batch classifier loss: 0.448554\n",
      "epoch 771; iter: 0; batch classifier loss: 0.493021\n",
      "epoch 771; iter: 200; batch classifier loss: 0.434841\n",
      "epoch 772; iter: 0; batch classifier loss: 0.393081\n",
      "epoch 772; iter: 200; batch classifier loss: 0.433880\n",
      "epoch 773; iter: 0; batch classifier loss: 0.438178\n",
      "epoch 773; iter: 200; batch classifier loss: 0.477079\n",
      "epoch 774; iter: 0; batch classifier loss: 0.420395\n",
      "epoch 774; iter: 200; batch classifier loss: 0.394597\n",
      "epoch 775; iter: 0; batch classifier loss: 0.448465\n",
      "epoch 775; iter: 200; batch classifier loss: 0.403952\n",
      "epoch 776; iter: 0; batch classifier loss: 0.436048\n",
      "epoch 776; iter: 200; batch classifier loss: 0.465228\n",
      "epoch 777; iter: 0; batch classifier loss: 0.399434\n",
      "epoch 777; iter: 200; batch classifier loss: 0.503834\n",
      "epoch 778; iter: 0; batch classifier loss: 0.407220\n",
      "epoch 778; iter: 200; batch classifier loss: 0.427175\n",
      "epoch 779; iter: 0; batch classifier loss: 0.437038\n",
      "epoch 779; iter: 200; batch classifier loss: 0.419536\n",
      "epoch 780; iter: 0; batch classifier loss: 0.384342\n",
      "epoch 780; iter: 200; batch classifier loss: 0.396763\n",
      "epoch 781; iter: 0; batch classifier loss: 0.387927\n",
      "epoch 781; iter: 200; batch classifier loss: 0.442602\n",
      "epoch 782; iter: 0; batch classifier loss: 0.411447\n",
      "epoch 782; iter: 200; batch classifier loss: 0.446148\n",
      "epoch 783; iter: 0; batch classifier loss: 0.447768\n",
      "epoch 783; iter: 200; batch classifier loss: 0.392605\n",
      "epoch 784; iter: 0; batch classifier loss: 0.436136\n",
      "epoch 784; iter: 200; batch classifier loss: 0.369651\n",
      "epoch 785; iter: 0; batch classifier loss: 0.513567\n",
      "epoch 785; iter: 200; batch classifier loss: 0.424441\n",
      "epoch 786; iter: 0; batch classifier loss: 0.433838\n",
      "epoch 786; iter: 200; batch classifier loss: 0.347290\n",
      "epoch 787; iter: 0; batch classifier loss: 0.451102\n",
      "epoch 787; iter: 200; batch classifier loss: 0.393520\n",
      "epoch 788; iter: 0; batch classifier loss: 0.443267\n",
      "epoch 788; iter: 200; batch classifier loss: 0.482195\n",
      "epoch 789; iter: 0; batch classifier loss: 0.520890\n",
      "epoch 789; iter: 200; batch classifier loss: 0.400192\n",
      "epoch 790; iter: 0; batch classifier loss: 0.446467\n",
      "epoch 790; iter: 200; batch classifier loss: 0.421940\n",
      "epoch 791; iter: 0; batch classifier loss: 0.417706\n",
      "epoch 791; iter: 200; batch classifier loss: 0.449715\n",
      "epoch 792; iter: 0; batch classifier loss: 0.356886\n",
      "epoch 792; iter: 200; batch classifier loss: 0.416040\n",
      "epoch 793; iter: 0; batch classifier loss: 0.365058\n",
      "epoch 793; iter: 200; batch classifier loss: 0.433068\n",
      "epoch 794; iter: 0; batch classifier loss: 0.376803\n",
      "epoch 794; iter: 200; batch classifier loss: 0.478283\n",
      "epoch 795; iter: 0; batch classifier loss: 0.442033\n",
      "epoch 795; iter: 200; batch classifier loss: 0.436179\n",
      "epoch 796; iter: 0; batch classifier loss: 0.316679\n",
      "epoch 796; iter: 200; batch classifier loss: 0.451227\n",
      "epoch 797; iter: 0; batch classifier loss: 0.409760\n",
      "epoch 797; iter: 200; batch classifier loss: 0.403926\n",
      "epoch 798; iter: 0; batch classifier loss: 0.420171\n",
      "epoch 798; iter: 200; batch classifier loss: 0.400168\n",
      "epoch 799; iter: 0; batch classifier loss: 0.438511\n",
      "epoch 799; iter: 200; batch classifier loss: 0.424777\n",
      "epoch 800; iter: 0; batch classifier loss: 0.497599\n",
      "epoch 800; iter: 200; batch classifier loss: 0.389547\n",
      "epoch 801; iter: 0; batch classifier loss: 0.388681\n",
      "epoch 801; iter: 200; batch classifier loss: 0.393678\n",
      "epoch 802; iter: 0; batch classifier loss: 0.389319\n",
      "epoch 802; iter: 200; batch classifier loss: 0.396867\n",
      "epoch 803; iter: 0; batch classifier loss: 0.538582\n",
      "epoch 803; iter: 200; batch classifier loss: 0.439472\n",
      "epoch 804; iter: 0; batch classifier loss: 0.424372\n",
      "epoch 804; iter: 200; batch classifier loss: 0.344249\n",
      "epoch 805; iter: 0; batch classifier loss: 0.385077\n",
      "epoch 805; iter: 200; batch classifier loss: 0.394562\n",
      "epoch 806; iter: 0; batch classifier loss: 0.388616\n",
      "epoch 806; iter: 200; batch classifier loss: 0.394360\n",
      "epoch 807; iter: 0; batch classifier loss: 0.342642\n",
      "epoch 807; iter: 200; batch classifier loss: 0.349873\n",
      "epoch 808; iter: 0; batch classifier loss: 0.398664\n",
      "epoch 808; iter: 200; batch classifier loss: 0.439819\n",
      "epoch 809; iter: 0; batch classifier loss: 0.469873\n",
      "epoch 809; iter: 200; batch classifier loss: 0.390928\n",
      "epoch 810; iter: 0; batch classifier loss: 0.430556\n",
      "epoch 810; iter: 200; batch classifier loss: 0.346222\n",
      "epoch 811; iter: 0; batch classifier loss: 0.295543\n",
      "epoch 811; iter: 200; batch classifier loss: 0.429012\n",
      "epoch 812; iter: 0; batch classifier loss: 0.376229\n",
      "epoch 812; iter: 200; batch classifier loss: 0.345247\n",
      "epoch 813; iter: 0; batch classifier loss: 0.390569\n",
      "epoch 813; iter: 200; batch classifier loss: 0.373504\n",
      "epoch 814; iter: 0; batch classifier loss: 0.401029\n",
      "epoch 814; iter: 200; batch classifier loss: 0.424467\n",
      "epoch 815; iter: 0; batch classifier loss: 0.443233\n",
      "epoch 815; iter: 200; batch classifier loss: 0.471057\n",
      "epoch 816; iter: 0; batch classifier loss: 0.428290\n",
      "epoch 816; iter: 200; batch classifier loss: 0.444383\n",
      "epoch 817; iter: 0; batch classifier loss: 0.451618\n",
      "epoch 817; iter: 200; batch classifier loss: 0.366003\n",
      "epoch 818; iter: 0; batch classifier loss: 0.526352\n",
      "epoch 818; iter: 200; batch classifier loss: 0.380551\n",
      "epoch 819; iter: 0; batch classifier loss: 0.449999\n",
      "epoch 819; iter: 200; batch classifier loss: 0.395821\n",
      "epoch 820; iter: 0; batch classifier loss: 0.334455\n",
      "epoch 820; iter: 200; batch classifier loss: 0.469013\n",
      "epoch 821; iter: 0; batch classifier loss: 0.452897\n",
      "epoch 821; iter: 200; batch classifier loss: 0.431245\n",
      "epoch 822; iter: 0; batch classifier loss: 0.473966\n",
      "epoch 822; iter: 200; batch classifier loss: 0.403312\n",
      "epoch 823; iter: 0; batch classifier loss: 0.457483\n",
      "epoch 823; iter: 200; batch classifier loss: 0.353350\n",
      "epoch 824; iter: 0; batch classifier loss: 0.333073\n",
      "epoch 824; iter: 200; batch classifier loss: 0.470297\n",
      "epoch 825; iter: 0; batch classifier loss: 0.376431\n",
      "epoch 825; iter: 200; batch classifier loss: 0.430557\n",
      "epoch 826; iter: 0; batch classifier loss: 0.402816\n",
      "epoch 826; iter: 200; batch classifier loss: 0.445826\n",
      "epoch 827; iter: 0; batch classifier loss: 0.431523\n",
      "epoch 827; iter: 200; batch classifier loss: 0.369744\n",
      "epoch 828; iter: 0; batch classifier loss: 0.412595\n",
      "epoch 828; iter: 200; batch classifier loss: 0.432912\n",
      "epoch 829; iter: 0; batch classifier loss: 0.376999\n",
      "epoch 829; iter: 200; batch classifier loss: 0.451939\n",
      "epoch 830; iter: 0; batch classifier loss: 0.455548\n",
      "epoch 830; iter: 200; batch classifier loss: 0.443445\n",
      "epoch 831; iter: 0; batch classifier loss: 0.414896\n",
      "epoch 831; iter: 200; batch classifier loss: 0.483990\n",
      "epoch 832; iter: 0; batch classifier loss: 0.379442\n",
      "epoch 832; iter: 200; batch classifier loss: 0.456908\n",
      "epoch 833; iter: 0; batch classifier loss: 0.377748\n",
      "epoch 833; iter: 200; batch classifier loss: 0.407353\n",
      "epoch 834; iter: 0; batch classifier loss: 0.447403\n",
      "epoch 834; iter: 200; batch classifier loss: 0.452405\n",
      "epoch 835; iter: 0; batch classifier loss: 0.460584\n",
      "epoch 835; iter: 200; batch classifier loss: 0.379386\n",
      "epoch 836; iter: 0; batch classifier loss: 0.383946\n",
      "epoch 836; iter: 200; batch classifier loss: 0.396783\n",
      "epoch 837; iter: 0; batch classifier loss: 0.415021\n",
      "epoch 837; iter: 200; batch classifier loss: 0.385807\n",
      "epoch 838; iter: 0; batch classifier loss: 0.411565\n",
      "epoch 838; iter: 200; batch classifier loss: 0.454914\n",
      "epoch 839; iter: 0; batch classifier loss: 0.365342\n",
      "epoch 839; iter: 200; batch classifier loss: 0.418299\n",
      "epoch 840; iter: 0; batch classifier loss: 0.472437\n",
      "epoch 840; iter: 200; batch classifier loss: 0.454138\n",
      "epoch 841; iter: 0; batch classifier loss: 0.419113\n",
      "epoch 841; iter: 200; batch classifier loss: 0.407343\n",
      "epoch 842; iter: 0; batch classifier loss: 0.406177\n",
      "epoch 842; iter: 200; batch classifier loss: 0.370247\n",
      "epoch 843; iter: 0; batch classifier loss: 0.387667\n",
      "epoch 843; iter: 200; batch classifier loss: 0.371387\n",
      "epoch 844; iter: 0; batch classifier loss: 0.427824\n",
      "epoch 844; iter: 200; batch classifier loss: 0.411470\n",
      "epoch 845; iter: 0; batch classifier loss: 0.346524\n",
      "epoch 845; iter: 200; batch classifier loss: 0.390358\n",
      "epoch 846; iter: 0; batch classifier loss: 0.377721\n",
      "epoch 846; iter: 200; batch classifier loss: 0.322914\n",
      "epoch 847; iter: 0; batch classifier loss: 0.295054\n",
      "epoch 847; iter: 200; batch classifier loss: 0.359690\n",
      "epoch 848; iter: 0; batch classifier loss: 0.414577\n",
      "epoch 848; iter: 200; batch classifier loss: 0.440695\n",
      "epoch 849; iter: 0; batch classifier loss: 0.425978\n",
      "epoch 849; iter: 200; batch classifier loss: 0.398397\n",
      "epoch 850; iter: 0; batch classifier loss: 0.406342\n",
      "epoch 850; iter: 200; batch classifier loss: 0.404237\n",
      "epoch 851; iter: 0; batch classifier loss: 0.427844\n",
      "epoch 851; iter: 200; batch classifier loss: 0.378761\n",
      "epoch 852; iter: 0; batch classifier loss: 0.369032\n",
      "epoch 852; iter: 200; batch classifier loss: 0.381733\n",
      "epoch 853; iter: 0; batch classifier loss: 0.421680\n",
      "epoch 853; iter: 200; batch classifier loss: 0.399128\n",
      "epoch 854; iter: 0; batch classifier loss: 0.467669\n",
      "epoch 854; iter: 200; batch classifier loss: 0.394958\n",
      "epoch 855; iter: 0; batch classifier loss: 0.413591\n",
      "epoch 855; iter: 200; batch classifier loss: 0.437140\n",
      "epoch 856; iter: 0; batch classifier loss: 0.407511\n",
      "epoch 856; iter: 200; batch classifier loss: 0.479263\n",
      "epoch 857; iter: 0; batch classifier loss: 0.392858\n",
      "epoch 857; iter: 200; batch classifier loss: 0.375512\n",
      "epoch 858; iter: 0; batch classifier loss: 0.420172\n",
      "epoch 858; iter: 200; batch classifier loss: 0.384608\n",
      "epoch 859; iter: 0; batch classifier loss: 0.383137\n",
      "epoch 859; iter: 200; batch classifier loss: 0.473727\n",
      "epoch 860; iter: 0; batch classifier loss: 0.358795\n",
      "epoch 860; iter: 200; batch classifier loss: 0.431688\n",
      "epoch 861; iter: 0; batch classifier loss: 0.406676\n",
      "epoch 861; iter: 200; batch classifier loss: 0.553436\n",
      "epoch 862; iter: 0; batch classifier loss: 0.321075\n",
      "epoch 862; iter: 200; batch classifier loss: 0.469209\n",
      "epoch 863; iter: 0; batch classifier loss: 0.395028\n",
      "epoch 863; iter: 200; batch classifier loss: 0.381402\n",
      "epoch 864; iter: 0; batch classifier loss: 0.404621\n",
      "epoch 864; iter: 200; batch classifier loss: 0.537147\n",
      "epoch 865; iter: 0; batch classifier loss: 0.424828\n",
      "epoch 865; iter: 200; batch classifier loss: 0.330596\n",
      "epoch 866; iter: 0; batch classifier loss: 0.424595\n",
      "epoch 866; iter: 200; batch classifier loss: 0.451891\n",
      "epoch 867; iter: 0; batch classifier loss: 0.450787\n",
      "epoch 867; iter: 200; batch classifier loss: 0.440390\n",
      "epoch 868; iter: 0; batch classifier loss: 0.434777\n",
      "epoch 868; iter: 200; batch classifier loss: 0.361993\n",
      "epoch 869; iter: 0; batch classifier loss: 0.426687\n",
      "epoch 869; iter: 200; batch classifier loss: 0.444802\n",
      "epoch 870; iter: 0; batch classifier loss: 0.501363\n",
      "epoch 870; iter: 200; batch classifier loss: 0.399313\n",
      "epoch 871; iter: 0; batch classifier loss: 0.444088\n",
      "epoch 871; iter: 200; batch classifier loss: 0.379359\n",
      "epoch 872; iter: 0; batch classifier loss: 0.383357\n",
      "epoch 872; iter: 200; batch classifier loss: 0.384961\n",
      "epoch 873; iter: 0; batch classifier loss: 0.317192\n",
      "epoch 873; iter: 200; batch classifier loss: 0.466733\n",
      "epoch 874; iter: 0; batch classifier loss: 0.400949\n",
      "epoch 874; iter: 200; batch classifier loss: 0.425038\n",
      "epoch 875; iter: 0; batch classifier loss: 0.359076\n",
      "epoch 875; iter: 200; batch classifier loss: 0.370033\n",
      "epoch 876; iter: 0; batch classifier loss: 0.483036\n",
      "epoch 876; iter: 200; batch classifier loss: 0.434611\n",
      "epoch 877; iter: 0; batch classifier loss: 0.350090\n",
      "epoch 877; iter: 200; batch classifier loss: 0.377948\n",
      "epoch 878; iter: 0; batch classifier loss: 0.493667\n",
      "epoch 878; iter: 200; batch classifier loss: 0.392238\n",
      "epoch 879; iter: 0; batch classifier loss: 0.416073\n",
      "epoch 879; iter: 200; batch classifier loss: 0.423423\n",
      "epoch 880; iter: 0; batch classifier loss: 0.353469\n",
      "epoch 880; iter: 200; batch classifier loss: 0.342994\n",
      "epoch 881; iter: 0; batch classifier loss: 0.457509\n",
      "epoch 881; iter: 200; batch classifier loss: 0.496807\n",
      "epoch 882; iter: 0; batch classifier loss: 0.395665\n",
      "epoch 882; iter: 200; batch classifier loss: 0.435995\n",
      "epoch 883; iter: 0; batch classifier loss: 0.435234\n",
      "epoch 883; iter: 200; batch classifier loss: 0.484218\n",
      "epoch 884; iter: 0; batch classifier loss: 0.406796\n",
      "epoch 884; iter: 200; batch classifier loss: 0.381387\n",
      "epoch 885; iter: 0; batch classifier loss: 0.345359\n",
      "epoch 885; iter: 200; batch classifier loss: 0.400649\n",
      "epoch 886; iter: 0; batch classifier loss: 0.474851\n",
      "epoch 886; iter: 200; batch classifier loss: 0.370608\n",
      "epoch 887; iter: 0; batch classifier loss: 0.388225\n",
      "epoch 887; iter: 200; batch classifier loss: 0.400941\n",
      "epoch 888; iter: 0; batch classifier loss: 0.409117\n",
      "epoch 888; iter: 200; batch classifier loss: 0.399405\n",
      "epoch 889; iter: 0; batch classifier loss: 0.434997\n",
      "epoch 889; iter: 200; batch classifier loss: 0.457025\n",
      "epoch 890; iter: 0; batch classifier loss: 0.400847\n",
      "epoch 890; iter: 200; batch classifier loss: 0.379250\n",
      "epoch 891; iter: 0; batch classifier loss: 0.465902\n",
      "epoch 891; iter: 200; batch classifier loss: 0.457846\n",
      "epoch 892; iter: 0; batch classifier loss: 0.448154\n",
      "epoch 892; iter: 200; batch classifier loss: 0.475618\n",
      "epoch 893; iter: 0; batch classifier loss: 0.392014\n",
      "epoch 893; iter: 200; batch classifier loss: 0.401035\n",
      "epoch 894; iter: 0; batch classifier loss: 0.356833\n",
      "epoch 894; iter: 200; batch classifier loss: 0.471625\n",
      "epoch 895; iter: 0; batch classifier loss: 0.532513\n",
      "epoch 895; iter: 200; batch classifier loss: 0.434842\n",
      "epoch 896; iter: 0; batch classifier loss: 0.353900\n",
      "epoch 896; iter: 200; batch classifier loss: 0.454146\n",
      "epoch 897; iter: 0; batch classifier loss: 0.402708\n",
      "epoch 897; iter: 200; batch classifier loss: 0.462414\n",
      "epoch 898; iter: 0; batch classifier loss: 0.443506\n",
      "epoch 898; iter: 200; batch classifier loss: 0.348913\n",
      "epoch 899; iter: 0; batch classifier loss: 0.461422\n",
      "epoch 899; iter: 200; batch classifier loss: 0.344837\n",
      "epoch 900; iter: 0; batch classifier loss: 0.341932\n",
      "epoch 900; iter: 200; batch classifier loss: 0.521084\n",
      "epoch 901; iter: 0; batch classifier loss: 0.512585\n",
      "epoch 901; iter: 200; batch classifier loss: 0.506492\n",
      "epoch 902; iter: 0; batch classifier loss: 0.377639\n",
      "epoch 902; iter: 200; batch classifier loss: 0.429269\n",
      "epoch 903; iter: 0; batch classifier loss: 0.383330\n",
      "epoch 903; iter: 200; batch classifier loss: 0.416974\n",
      "epoch 904; iter: 0; batch classifier loss: 0.365026\n",
      "epoch 904; iter: 200; batch classifier loss: 0.431652\n",
      "epoch 905; iter: 0; batch classifier loss: 0.395008\n",
      "epoch 905; iter: 200; batch classifier loss: 0.468282\n",
      "epoch 906; iter: 0; batch classifier loss: 0.463342\n",
      "epoch 906; iter: 200; batch classifier loss: 0.425758\n",
      "epoch 907; iter: 0; batch classifier loss: 0.401493\n",
      "epoch 907; iter: 200; batch classifier loss: 0.383389\n",
      "epoch 908; iter: 0; batch classifier loss: 0.427507\n",
      "epoch 908; iter: 200; batch classifier loss: 0.374905\n",
      "epoch 909; iter: 0; batch classifier loss: 0.535726\n",
      "epoch 909; iter: 200; batch classifier loss: 0.450921\n",
      "epoch 910; iter: 0; batch classifier loss: 0.337110\n",
      "epoch 910; iter: 200; batch classifier loss: 0.422724\n",
      "epoch 911; iter: 0; batch classifier loss: 0.452400\n",
      "epoch 911; iter: 200; batch classifier loss: 0.446591\n",
      "epoch 912; iter: 0; batch classifier loss: 0.452767\n",
      "epoch 912; iter: 200; batch classifier loss: 0.457759\n",
      "epoch 913; iter: 0; batch classifier loss: 0.494324\n",
      "epoch 913; iter: 200; batch classifier loss: 0.473897\n",
      "epoch 914; iter: 0; batch classifier loss: 0.490755\n",
      "epoch 914; iter: 200; batch classifier loss: 0.505715\n",
      "epoch 915; iter: 0; batch classifier loss: 0.430464\n",
      "epoch 915; iter: 200; batch classifier loss: 0.415474\n",
      "epoch 916; iter: 0; batch classifier loss: 0.529151\n",
      "epoch 916; iter: 200; batch classifier loss: 0.412375\n",
      "epoch 917; iter: 0; batch classifier loss: 0.374931\n",
      "epoch 917; iter: 200; batch classifier loss: 0.500817\n",
      "epoch 918; iter: 0; batch classifier loss: 0.368978\n",
      "epoch 918; iter: 200; batch classifier loss: 0.453621\n",
      "epoch 919; iter: 0; batch classifier loss: 0.410044\n",
      "epoch 919; iter: 200; batch classifier loss: 0.372599\n",
      "epoch 920; iter: 0; batch classifier loss: 0.446944\n",
      "epoch 920; iter: 200; batch classifier loss: 0.463488\n",
      "epoch 921; iter: 0; batch classifier loss: 0.433691\n",
      "epoch 921; iter: 200; batch classifier loss: 0.471229\n",
      "epoch 922; iter: 0; batch classifier loss: 0.403145\n",
      "epoch 922; iter: 200; batch classifier loss: 0.421292\n",
      "epoch 923; iter: 0; batch classifier loss: 0.483044\n",
      "epoch 923; iter: 200; batch classifier loss: 0.384744\n",
      "epoch 924; iter: 0; batch classifier loss: 0.415816\n",
      "epoch 924; iter: 200; batch classifier loss: 0.398905\n",
      "epoch 925; iter: 0; batch classifier loss: 0.368610\n",
      "epoch 925; iter: 200; batch classifier loss: 0.391781\n",
      "epoch 926; iter: 0; batch classifier loss: 0.419032\n",
      "epoch 926; iter: 200; batch classifier loss: 0.433415\n",
      "epoch 927; iter: 0; batch classifier loss: 0.403683\n",
      "epoch 927; iter: 200; batch classifier loss: 0.350252\n",
      "epoch 928; iter: 0; batch classifier loss: 0.411312\n",
      "epoch 928; iter: 200; batch classifier loss: 0.377641\n",
      "epoch 929; iter: 0; batch classifier loss: 0.366832\n",
      "epoch 929; iter: 200; batch classifier loss: 0.415126\n",
      "epoch 930; iter: 0; batch classifier loss: 0.385899\n",
      "epoch 930; iter: 200; batch classifier loss: 0.416782\n",
      "epoch 931; iter: 0; batch classifier loss: 0.320200\n",
      "epoch 931; iter: 200; batch classifier loss: 0.393275\n",
      "epoch 932; iter: 0; batch classifier loss: 0.466606\n",
      "epoch 932; iter: 200; batch classifier loss: 0.356804\n",
      "epoch 933; iter: 0; batch classifier loss: 0.369650\n",
      "epoch 933; iter: 200; batch classifier loss: 0.489222\n",
      "epoch 934; iter: 0; batch classifier loss: 0.415049\n",
      "epoch 934; iter: 200; batch classifier loss: 0.439678\n",
      "epoch 935; iter: 0; batch classifier loss: 0.346016\n",
      "epoch 935; iter: 200; batch classifier loss: 0.374210\n",
      "epoch 936; iter: 0; batch classifier loss: 0.380732\n",
      "epoch 936; iter: 200; batch classifier loss: 0.444531\n",
      "epoch 937; iter: 0; batch classifier loss: 0.372443\n",
      "epoch 937; iter: 200; batch classifier loss: 0.470061\n",
      "epoch 938; iter: 0; batch classifier loss: 0.436130\n",
      "epoch 938; iter: 200; batch classifier loss: 0.475734\n",
      "epoch 939; iter: 0; batch classifier loss: 0.433233\n",
      "epoch 939; iter: 200; batch classifier loss: 0.352695\n",
      "epoch 940; iter: 0; batch classifier loss: 0.491401\n",
      "epoch 940; iter: 200; batch classifier loss: 0.435326\n",
      "epoch 941; iter: 0; batch classifier loss: 0.487130\n",
      "epoch 941; iter: 200; batch classifier loss: 0.452745\n",
      "epoch 942; iter: 0; batch classifier loss: 0.408073\n",
      "epoch 942; iter: 200; batch classifier loss: 0.446889\n",
      "epoch 943; iter: 0; batch classifier loss: 0.361187\n",
      "epoch 943; iter: 200; batch classifier loss: 0.351847\n",
      "epoch 944; iter: 0; batch classifier loss: 0.426197\n",
      "epoch 944; iter: 200; batch classifier loss: 0.403415\n",
      "epoch 945; iter: 0; batch classifier loss: 0.339724\n",
      "epoch 945; iter: 200; batch classifier loss: 0.393516\n",
      "epoch 946; iter: 0; batch classifier loss: 0.404354\n",
      "epoch 946; iter: 200; batch classifier loss: 0.415340\n",
      "epoch 947; iter: 0; batch classifier loss: 0.440164\n",
      "epoch 947; iter: 200; batch classifier loss: 0.372306\n",
      "epoch 948; iter: 0; batch classifier loss: 0.435007\n",
      "epoch 948; iter: 200; batch classifier loss: 0.427288\n",
      "epoch 949; iter: 0; batch classifier loss: 0.450755\n",
      "epoch 949; iter: 200; batch classifier loss: 0.462034\n",
      "epoch 950; iter: 0; batch classifier loss: 0.399279\n",
      "epoch 950; iter: 200; batch classifier loss: 0.468354\n",
      "epoch 951; iter: 0; batch classifier loss: 0.480190\n",
      "epoch 951; iter: 200; batch classifier loss: 0.454355\n",
      "epoch 952; iter: 0; batch classifier loss: 0.498304\n",
      "epoch 952; iter: 200; batch classifier loss: 0.433703\n",
      "epoch 953; iter: 0; batch classifier loss: 0.351819\n",
      "epoch 953; iter: 200; batch classifier loss: 0.428131\n",
      "epoch 954; iter: 0; batch classifier loss: 0.524373\n",
      "epoch 954; iter: 200; batch classifier loss: 0.411922\n",
      "epoch 955; iter: 0; batch classifier loss: 0.422040\n",
      "epoch 955; iter: 200; batch classifier loss: 0.369179\n",
      "epoch 956; iter: 0; batch classifier loss: 0.366385\n",
      "epoch 956; iter: 200; batch classifier loss: 0.298193\n",
      "epoch 957; iter: 0; batch classifier loss: 0.458408\n",
      "epoch 957; iter: 200; batch classifier loss: 0.419846\n",
      "epoch 958; iter: 0; batch classifier loss: 0.442329\n",
      "epoch 958; iter: 200; batch classifier loss: 0.466569\n",
      "epoch 959; iter: 0; batch classifier loss: 0.449231\n",
      "epoch 959; iter: 200; batch classifier loss: 0.443354\n",
      "epoch 960; iter: 0; batch classifier loss: 0.439875\n",
      "epoch 960; iter: 200; batch classifier loss: 0.419481\n",
      "epoch 961; iter: 0; batch classifier loss: 0.441998\n",
      "epoch 961; iter: 200; batch classifier loss: 0.408785\n",
      "epoch 962; iter: 0; batch classifier loss: 0.466428\n",
      "epoch 962; iter: 200; batch classifier loss: 0.435833\n",
      "epoch 963; iter: 0; batch classifier loss: 0.352533\n",
      "epoch 963; iter: 200; batch classifier loss: 0.407053\n",
      "epoch 964; iter: 0; batch classifier loss: 0.422383\n",
      "epoch 964; iter: 200; batch classifier loss: 0.385363\n",
      "epoch 965; iter: 0; batch classifier loss: 0.353896\n",
      "epoch 965; iter: 200; batch classifier loss: 0.413889\n",
      "epoch 966; iter: 0; batch classifier loss: 0.461794\n",
      "epoch 966; iter: 200; batch classifier loss: 0.446037\n",
      "epoch 967; iter: 0; batch classifier loss: 0.451984\n",
      "epoch 967; iter: 200; batch classifier loss: 0.490939\n",
      "epoch 968; iter: 0; batch classifier loss: 0.451558\n",
      "epoch 968; iter: 200; batch classifier loss: 0.395701\n",
      "epoch 969; iter: 0; batch classifier loss: 0.412397\n",
      "epoch 969; iter: 200; batch classifier loss: 0.480259\n",
      "epoch 970; iter: 0; batch classifier loss: 0.385115\n",
      "epoch 970; iter: 200; batch classifier loss: 0.391197\n",
      "epoch 971; iter: 0; batch classifier loss: 0.483761\n",
      "epoch 971; iter: 200; batch classifier loss: 0.451444\n",
      "epoch 972; iter: 0; batch classifier loss: 0.506136\n",
      "epoch 972; iter: 200; batch classifier loss: 0.417627\n",
      "epoch 973; iter: 0; batch classifier loss: 0.416219\n",
      "epoch 973; iter: 200; batch classifier loss: 0.348554\n",
      "epoch 974; iter: 0; batch classifier loss: 0.389014\n",
      "epoch 974; iter: 200; batch classifier loss: 0.364870\n",
      "epoch 975; iter: 0; batch classifier loss: 0.496598\n",
      "epoch 975; iter: 200; batch classifier loss: 0.404175\n",
      "epoch 976; iter: 0; batch classifier loss: 0.425938\n",
      "epoch 976; iter: 200; batch classifier loss: 0.396644\n",
      "epoch 977; iter: 0; batch classifier loss: 0.488559\n",
      "epoch 977; iter: 200; batch classifier loss: 0.458095\n",
      "epoch 978; iter: 0; batch classifier loss: 0.356562\n",
      "epoch 978; iter: 200; batch classifier loss: 0.401207\n",
      "epoch 979; iter: 0; batch classifier loss: 0.449843\n",
      "epoch 979; iter: 200; batch classifier loss: 0.425275\n",
      "epoch 980; iter: 0; batch classifier loss: 0.328385\n",
      "epoch 980; iter: 200; batch classifier loss: 0.442615\n",
      "epoch 981; iter: 0; batch classifier loss: 0.347110\n",
      "epoch 981; iter: 200; batch classifier loss: 0.421784\n",
      "epoch 982; iter: 0; batch classifier loss: 0.445816\n",
      "epoch 982; iter: 200; batch classifier loss: 0.336150\n",
      "epoch 983; iter: 0; batch classifier loss: 0.413916\n",
      "epoch 983; iter: 200; batch classifier loss: 0.473501\n",
      "epoch 984; iter: 0; batch classifier loss: 0.465773\n",
      "epoch 984; iter: 200; batch classifier loss: 0.352163\n",
      "epoch 985; iter: 0; batch classifier loss: 0.476373\n",
      "epoch 985; iter: 200; batch classifier loss: 0.469773\n",
      "epoch 986; iter: 0; batch classifier loss: 0.457526\n",
      "epoch 986; iter: 200; batch classifier loss: 0.427069\n",
      "epoch 987; iter: 0; batch classifier loss: 0.397413\n",
      "epoch 987; iter: 200; batch classifier loss: 0.392456\n",
      "epoch 988; iter: 0; batch classifier loss: 0.394466\n",
      "epoch 988; iter: 200; batch classifier loss: 0.377997\n",
      "epoch 989; iter: 0; batch classifier loss: 0.355731\n",
      "epoch 989; iter: 200; batch classifier loss: 0.346764\n",
      "epoch 990; iter: 0; batch classifier loss: 0.469997\n",
      "epoch 990; iter: 200; batch classifier loss: 0.368983\n",
      "epoch 991; iter: 0; batch classifier loss: 0.343775\n",
      "epoch 991; iter: 200; batch classifier loss: 0.390483\n",
      "epoch 992; iter: 0; batch classifier loss: 0.386510\n",
      "epoch 992; iter: 200; batch classifier loss: 0.532661\n",
      "epoch 993; iter: 0; batch classifier loss: 0.324251\n",
      "epoch 993; iter: 200; batch classifier loss: 0.446482\n",
      "epoch 994; iter: 0; batch classifier loss: 0.450521\n",
      "epoch 994; iter: 200; batch classifier loss: 0.459541\n",
      "epoch 995; iter: 0; batch classifier loss: 0.418452\n",
      "epoch 995; iter: 200; batch classifier loss: 0.398119\n",
      "epoch 996; iter: 0; batch classifier loss: 0.413853\n",
      "epoch 996; iter: 200; batch classifier loss: 0.361704\n",
      "epoch 997; iter: 0; batch classifier loss: 0.354411\n",
      "epoch 997; iter: 200; batch classifier loss: 0.455694\n",
      "epoch 998; iter: 0; batch classifier loss: 0.457412\n",
      "epoch 998; iter: 200; batch classifier loss: 0.416311\n",
      "epoch 999; iter: 0; batch classifier loss: 0.394171\n",
      "epoch 999; iter: 200; batch classifier loss: 0.362931\n",
      "WARNING:tensorflow:From D:\\wp\\PycharmProjects\\AIF360\\aif360\\algorithms\\inprocessing\\adversarial_debiasing_dnn5.py:98: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "Completed model training and saved at: ../org-model/adult/999\\test.model\n"
     ]
    },
    {
     "data": {
      "text/plain": "<aif360.algorithms.inprocessing.adversarial_debiasing_dnn5.AdversarialDebiasingDnn5 at 0x2015bd60948>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plain_model.fit(dataset_orig_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\wp\\PycharmProjects\\AIF360\\aif360\\algorithms\\inprocessing\\adversarial_debiasing_dnn5.py:286: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\wp\\PycharmProjects\\AIF360\\aif360\\algorithms\\inprocessing\\adversarial_debiasing_dnn5.py:290: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\wp\\PycharmProjects\\AIF360\\aif360\\load_model\\layer.py:23: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\wp\\PycharmProjects\\AIF360\\aif360\\load_model\\layer.py:25: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From D:\\wp\\PycharmProjects\\AIF360\\aif360\\algorithms\\inprocessing\\adversarial_debiasing_dnn5.py:307: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\wp\\PycharmProjects\\AIF360\\aif360\\algorithms\\inprocessing\\adversarial_debiasing_dnn5.py:309: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\wp\\PycharmProjects\\AIF360\\aif360\\algorithms\\inprocessing\\adversarial_debiasing_dnn5.py:313: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\wp\\PycharmProjects\\AIF360\\aif360\\algorithms\\inprocessing\\adversarial_debiasing_dnn5.py:340: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from ../org-model/adult/99/test.model\n",
      "INFO:tensorflow:Restoring parameters from ../org-model/adult/99/test.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1365, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1350, in _run_fn\n",
      "    target_list, run_metadata)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1443, in _call_tf_sessionrun\n",
      "    run_metadata)\n",
      "tensorflow.python.framework.errors_impl.NotFoundError: 2 root error(s) found.\n",
      "  (0) Not found: Key plain_classifier/plain_classifier_1/classifier_model/linear/bias/Adam not found in checkpoint\n",
      "\t [[{{node save_1/RestoreV2}}]]\n",
      "\t [[save_1/RestoreV2/_181]]\n",
      "  (1) Not found: Key plain_classifier/plain_classifier_1/classifier_model/linear/bias/Adam not found in checkpoint\n",
      "\t [[{{node save_1/RestoreV2}}]]\n",
      "0 successful operations.\n",
      "0 derived errors ignored.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 1290, in restore\n",
      "    {self.saver_def.filename_tensor_name: save_path})\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 956, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1180, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1359, in _do_run\n",
      "    run_metadata)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1384, in _do_call\n",
      "    raise type(e)(node_def, op, message)\n",
      "tensorflow.python.framework.errors_impl.NotFoundError: 2 root error(s) found.\n",
      "  (0) Not found: Key plain_classifier/plain_classifier_1/classifier_model/linear/bias/Adam not found in checkpoint\n",
      "\t [[node save_1/RestoreV2 (defined at F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) ]]\n",
      "\t [[save_1/RestoreV2/_181]]\n",
      "  (1) Not found: Key plain_classifier/plain_classifier_1/classifier_model/linear/bias/Adam not found in checkpoint\n",
      "\t [[node save_1/RestoreV2 (defined at F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) ]]\n",
      "0 successful operations.\n",
      "0 derived errors ignored.\n",
      "\n",
      "Original stack trace for 'save_1/RestoreV2':\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n",
      "    app.start()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n",
      "    self.io_loop.start()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\asyncio\\base_events.py\", line 541, in run_forever\n",
      "    self._run_once()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\asyncio\\base_events.py\", line 1786, in _run_once\n",
      "    handle._run()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 460, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 367, in dispatch_shell\n",
      "    await result\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 662, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 360, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2915, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2960, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 78, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3186, in run_cell_async\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3377, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3457, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\wp\\AppData\\Local\\Temp\\ipykernel_18028\\1550785440.py\", line 3, in <module>\n",
      "    dataset_nodebiasing_test = plain_model.predict(dataset_orig_test)\n",
      "  File \"D:\\wp\\PycharmProjects\\AIF360\\aif360\\algorithms\\transformer.py\", line 27, in wrapper\n",
      "    new_dataset = func(self, *args, **kwargs)\n",
      "  File \"D:\\wp\\PycharmProjects\\AIF360\\aif360\\algorithms\\inprocessing\\adversarial_debiasing_dnn5.py\", line 340, in predict\n",
      "    saver = tf.train.Saver()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 828, in __init__\n",
      "    self.build()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 840, in build\n",
      "    self._build(self._filename, build_save=True, build_restore=True)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 878, in _build\n",
      "    build_restore=build_restore)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 508, in _build_internal\n",
      "    restore_sequentially, reshape)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 328, in _AddRestoreOps\n",
      "    restore_sequentially)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 575, in bulk_restore\n",
      "    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_io_ops.py\", line 1696, in restore_v2\n",
      "    name=name)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\", line 794, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\", line 507, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3357, in create_op\n",
      "    attrs, op_def, compute_device)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3426, in _create_op_internal\n",
      "    op_def=op_def)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1748, in __init__\n",
      "    self._traceback = tf_stack.extract_stack()\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 1300, in restore\n",
      "    names_to_keys = object_graph_key_mapping(save_path)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 1618, in object_graph_key_mapping\n",
      "    object_graph_string = reader.get_tensor(trackable.OBJECT_GRAPH_PROTO_KEY)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 915, in get_tensor\n",
      "    return CheckpointReader_GetTensor(self, compat.as_bytes(tensor_str))\n",
      "tensorflow.python.framework.errors_impl.NotFoundError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3457, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\wp\\AppData\\Local\\Temp\\ipykernel_18028\\1550785440.py\", line 3, in <module>\n",
      "    dataset_nodebiasing_test = plain_model.predict(dataset_orig_test)\n",
      "  File \"D:\\wp\\PycharmProjects\\AIF360\\aif360\\algorithms\\transformer.py\", line 27, in wrapper\n",
      "    new_dataset = func(self, *args, **kwargs)\n",
      "  File \"D:\\wp\\PycharmProjects\\AIF360\\aif360\\algorithms\\inprocessing\\adversarial_debiasing_dnn5.py\", line 341, in predict\n",
      "    saver.restore(self.sess, model_path)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 1306, in restore\n",
      "    err, \"a Variable name or other graph key that is missing\")\n",
      "tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n",
      "\n",
      "2 root error(s) found.\n",
      "  (0) Not found: Key plain_classifier/plain_classifier_1/classifier_model/linear/bias/Adam not found in checkpoint\n",
      "\t [[node save_1/RestoreV2 (defined at F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) ]]\n",
      "\t [[save_1/RestoreV2/_181]]\n",
      "  (1) Not found: Key plain_classifier/plain_classifier_1/classifier_model/linear/bias/Adam not found in checkpoint\n",
      "\t [[node save_1/RestoreV2 (defined at F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) ]]\n",
      "0 successful operations.\n",
      "0 derived errors ignored.\n",
      "\n",
      "Original stack trace for 'save_1/RestoreV2':\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n",
      "    app.start()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n",
      "    self.io_loop.start()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\asyncio\\base_events.py\", line 541, in run_forever\n",
      "    self._run_once()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\asyncio\\base_events.py\", line 1786, in _run_once\n",
      "    handle._run()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 460, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 367, in dispatch_shell\n",
      "    await result\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 662, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 360, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2915, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2960, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 78, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3186, in run_cell_async\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3377, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3457, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\wp\\AppData\\Local\\Temp\\ipykernel_18028\\1550785440.py\", line 3, in <module>\n",
      "    dataset_nodebiasing_test = plain_model.predict(dataset_orig_test)\n",
      "  File \"D:\\wp\\PycharmProjects\\AIF360\\aif360\\algorithms\\transformer.py\", line 27, in wrapper\n",
      "    new_dataset = func(self, *args, **kwargs)\n",
      "  File \"D:\\wp\\PycharmProjects\\AIF360\\aif360\\algorithms\\inprocessing\\adversarial_debiasing_dnn5.py\", line 340, in predict\n",
      "    saver = tf.train.Saver()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 828, in __init__\n",
      "    self.build()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 840, in build\n",
      "    self._build(self._filename, build_save=True, build_restore=True)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 878, in _build\n",
      "    build_restore=build_restore)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 508, in _build_internal\n",
      "    restore_sequentially, reshape)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 328, in _AddRestoreOps\n",
      "    restore_sequentially)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 575, in bulk_restore\n",
      "    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_io_ops.py\", line 1696, in restore_v2\n",
      "    name=name)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\", line 794, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\", line 507, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3357, in create_op\n",
      "    attrs, op_def, compute_device)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3426, in _create_op_internal\n",
      "    op_def=op_def)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1748, in __init__\n",
      "    self._traceback = tf_stack.extract_stack()\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2077, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'NotFoundError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\contrib\\__init__.py\", line 39, in <module>\n",
      "    from tensorflow.contrib import compiler\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\contrib\\compiler\\__init__.py\", line 21, in <module>\n",
      "    from tensorflow.contrib.compiler import jit\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\contrib\\compiler\\__init__.py\", line 22, in <module>\n",
      "    from tensorflow.contrib.compiler import xla\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\contrib\\compiler\\xla.py\", line 22, in <module>\n",
      "    from tensorflow.python.estimator import model_fn as model_fn_lib\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\estimator\\model_fn.py\", line 26, in <module>\n",
      "    from tensorflow_estimator.python.estimator import model_fn\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_estimator\\__init__.py\", line 10, in <module>\n",
      "    from tensorflow_estimator._api.v1 import estimator\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_estimator\\_api\\v1\\estimator\\__init__.py\", line 10, in <module>\n",
      "    from tensorflow_estimator._api.v1.estimator import experimental\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_estimator\\_api\\v1\\estimator\\experimental\\__init__.py\", line 10, in <module>\n",
      "    from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\canned\\dnn.py\", line 27, in <module>\n",
      "    from tensorflow_estimator.python.estimator import estimator\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 36, in <module>\n",
      "    from tensorflow.python.profiler import trace\n",
      "ImportError: cannot import name 'trace' from 'tensorflow.python.profiler' (F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\profiler\\__init__.py)\n",
      "Traceback (most recent call last):\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1365, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1350, in _run_fn\n",
      "    target_list, run_metadata)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1443, in _call_tf_sessionrun\n",
      "    run_metadata)\n",
      "tensorflow.python.framework.errors_impl.NotFoundError: 2 root error(s) found.\n",
      "  (0) Not found: Key plain_classifier/plain_classifier_1/classifier_model/linear/bias/Adam not found in checkpoint\n",
      "\t [[{{node save_1/RestoreV2}}]]\n",
      "\t [[save_1/RestoreV2/_181]]\n",
      "  (1) Not found: Key plain_classifier/plain_classifier_1/classifier_model/linear/bias/Adam not found in checkpoint\n",
      "\t [[{{node save_1/RestoreV2}}]]\n",
      "0 successful operations.\n",
      "0 derived errors ignored.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 1290, in restore\n",
      "    {self.saver_def.filename_tensor_name: save_path})\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 956, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1180, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1359, in _do_run\n",
      "    run_metadata)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1384, in _do_call\n",
      "    raise type(e)(node_def, op, message)\n",
      "tensorflow.python.framework.errors_impl.NotFoundError: 2 root error(s) found.\n",
      "  (0) Not found: Key plain_classifier/plain_classifier_1/classifier_model/linear/bias/Adam not found in checkpoint\n",
      "\t [[node save_1/RestoreV2 (defined at F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) ]]\n",
      "\t [[save_1/RestoreV2/_181]]\n",
      "  (1) Not found: Key plain_classifier/plain_classifier_1/classifier_model/linear/bias/Adam not found in checkpoint\n",
      "\t [[node save_1/RestoreV2 (defined at F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) ]]\n",
      "0 successful operations.\n",
      "0 derived errors ignored.\n",
      "\n",
      "Original stack trace for 'save_1/RestoreV2':\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n",
      "    app.start()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n",
      "    self.io_loop.start()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\asyncio\\base_events.py\", line 541, in run_forever\n",
      "    self._run_once()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\asyncio\\base_events.py\", line 1786, in _run_once\n",
      "    handle._run()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 460, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 367, in dispatch_shell\n",
      "    await result\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 662, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 360, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2915, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2960, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 78, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3186, in run_cell_async\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3377, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3457, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\wp\\AppData\\Local\\Temp\\ipykernel_18028\\1550785440.py\", line 3, in <module>\n",
      "    dataset_nodebiasing_test = plain_model.predict(dataset_orig_test)\n",
      "  File \"D:\\wp\\PycharmProjects\\AIF360\\aif360\\algorithms\\transformer.py\", line 27, in wrapper\n",
      "    new_dataset = func(self, *args, **kwargs)\n",
      "  File \"D:\\wp\\PycharmProjects\\AIF360\\aif360\\algorithms\\inprocessing\\adversarial_debiasing_dnn5.py\", line 340, in predict\n",
      "    saver = tf.train.Saver()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 828, in __init__\n",
      "    self.build()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 840, in build\n",
      "    self._build(self._filename, build_save=True, build_restore=True)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 878, in _build\n",
      "    build_restore=build_restore)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 508, in _build_internal\n",
      "    restore_sequentially, reshape)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 328, in _AddRestoreOps\n",
      "    restore_sequentially)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 575, in bulk_restore\n",
      "    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_io_ops.py\", line 1696, in restore_v2\n",
      "    name=name)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\", line 794, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\", line 507, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3357, in create_op\n",
      "    attrs, op_def, compute_device)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3426, in _create_op_internal\n",
      "    op_def=op_def)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1748, in __init__\n",
      "    self._traceback = tf_stack.extract_stack()\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 1300, in restore\n",
      "    names_to_keys = object_graph_key_mapping(save_path)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 1618, in object_graph_key_mapping\n",
      "    object_graph_string = reader.get_tensor(trackable.OBJECT_GRAPH_PROTO_KEY)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 915, in get_tensor\n",
      "    return CheckpointReader_GetTensor(self, compat.as_bytes(tensor_str))\n",
      "tensorflow.python.framework.errors_impl.NotFoundError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3457, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\wp\\AppData\\Local\\Temp\\ipykernel_18028\\1550785440.py\", line 3, in <module>\n",
      "    dataset_nodebiasing_test = plain_model.predict(dataset_orig_test)\n",
      "  File \"D:\\wp\\PycharmProjects\\AIF360\\aif360\\algorithms\\transformer.py\", line 27, in wrapper\n",
      "    new_dataset = func(self, *args, **kwargs)\n",
      "  File \"D:\\wp\\PycharmProjects\\AIF360\\aif360\\algorithms\\inprocessing\\adversarial_debiasing_dnn5.py\", line 341, in predict\n",
      "    saver.restore(self.sess, model_path)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 1306, in restore\n",
      "    err, \"a Variable name or other graph key that is missing\")\n",
      "tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n",
      "\n",
      "2 root error(s) found.\n",
      "  (0) Not found: Key plain_classifier/plain_classifier_1/classifier_model/linear/bias/Adam not found in checkpoint\n",
      "\t [[node save_1/RestoreV2 (defined at F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) ]]\n",
      "\t [[save_1/RestoreV2/_181]]\n",
      "  (1) Not found: Key plain_classifier/plain_classifier_1/classifier_model/linear/bias/Adam not found in checkpoint\n",
      "\t [[node save_1/RestoreV2 (defined at F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) ]]\n",
      "0 successful operations.\n",
      "0 derived errors ignored.\n",
      "\n",
      "Original stack trace for 'save_1/RestoreV2':\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n",
      "    app.start()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n",
      "    self.io_loop.start()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\asyncio\\base_events.py\", line 541, in run_forever\n",
      "    self._run_once()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\asyncio\\base_events.py\", line 1786, in _run_once\n",
      "    handle._run()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 460, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 367, in dispatch_shell\n",
      "    await result\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 662, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 360, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2915, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2960, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 78, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3186, in run_cell_async\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3377, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3457, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\wp\\AppData\\Local\\Temp\\ipykernel_18028\\1550785440.py\", line 3, in <module>\n",
      "    dataset_nodebiasing_test = plain_model.predict(dataset_orig_test)\n",
      "  File \"D:\\wp\\PycharmProjects\\AIF360\\aif360\\algorithms\\transformer.py\", line 27, in wrapper\n",
      "    new_dataset = func(self, *args, **kwargs)\n",
      "  File \"D:\\wp\\PycharmProjects\\AIF360\\aif360\\algorithms\\inprocessing\\adversarial_debiasing_dnn5.py\", line 340, in predict\n",
      "    saver = tf.train.Saver()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 828, in __init__\n",
      "    self.build()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 840, in build\n",
      "    self._build(self._filename, build_save=True, build_restore=True)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 878, in _build\n",
      "    build_restore=build_restore)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 508, in _build_internal\n",
      "    restore_sequentially, reshape)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 328, in _AddRestoreOps\n",
      "    restore_sequentially)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 575, in bulk_restore\n",
      "    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_io_ops.py\", line 1696, in restore_v2\n",
      "    name=name)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\", line 794, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\", line 507, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3357, in create_op\n",
      "    attrs, op_def, compute_device)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3426, in _create_op_internal\n",
      "    op_def=op_def)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1748, in __init__\n",
      "    self._traceback = tf_stack.extract_stack()\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2077, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'NotFoundError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3377, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3474, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2080, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1368, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1268, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1125, in structured_traceback\n",
      "    tb_offset)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2077, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 965, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'tensorflow_core.estimator'\n",
      "Traceback (most recent call last):\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1365, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1350, in _run_fn\n",
      "    target_list, run_metadata)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1443, in _call_tf_sessionrun\n",
      "    run_metadata)\n",
      "tensorflow.python.framework.errors_impl.NotFoundError: 2 root error(s) found.\n",
      "  (0) Not found: Key plain_classifier/plain_classifier_1/classifier_model/linear/bias/Adam not found in checkpoint\n",
      "\t [[{{node save_1/RestoreV2}}]]\n",
      "\t [[save_1/RestoreV2/_181]]\n",
      "  (1) Not found: Key plain_classifier/plain_classifier_1/classifier_model/linear/bias/Adam not found in checkpoint\n",
      "\t [[{{node save_1/RestoreV2}}]]\n",
      "0 successful operations.\n",
      "0 derived errors ignored.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 1290, in restore\n",
      "    {self.saver_def.filename_tensor_name: save_path})\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 956, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1180, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1359, in _do_run\n",
      "    run_metadata)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1384, in _do_call\n",
      "    raise type(e)(node_def, op, message)\n",
      "tensorflow.python.framework.errors_impl.NotFoundError: 2 root error(s) found.\n",
      "  (0) Not found: Key plain_classifier/plain_classifier_1/classifier_model/linear/bias/Adam not found in checkpoint\n",
      "\t [[node save_1/RestoreV2 (defined at F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) ]]\n",
      "\t [[save_1/RestoreV2/_181]]\n",
      "  (1) Not found: Key plain_classifier/plain_classifier_1/classifier_model/linear/bias/Adam not found in checkpoint\n",
      "\t [[node save_1/RestoreV2 (defined at F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) ]]\n",
      "0 successful operations.\n",
      "0 derived errors ignored.\n",
      "\n",
      "Original stack trace for 'save_1/RestoreV2':\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n",
      "    app.start()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n",
      "    self.io_loop.start()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\asyncio\\base_events.py\", line 541, in run_forever\n",
      "    self._run_once()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\asyncio\\base_events.py\", line 1786, in _run_once\n",
      "    handle._run()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 460, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 367, in dispatch_shell\n",
      "    await result\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 662, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 360, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2915, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2960, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 78, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3186, in run_cell_async\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3377, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3457, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\wp\\AppData\\Local\\Temp\\ipykernel_18028\\1550785440.py\", line 3, in <module>\n",
      "    dataset_nodebiasing_test = plain_model.predict(dataset_orig_test)\n",
      "  File \"D:\\wp\\PycharmProjects\\AIF360\\aif360\\algorithms\\transformer.py\", line 27, in wrapper\n",
      "    new_dataset = func(self, *args, **kwargs)\n",
      "  File \"D:\\wp\\PycharmProjects\\AIF360\\aif360\\algorithms\\inprocessing\\adversarial_debiasing_dnn5.py\", line 340, in predict\n",
      "    saver = tf.train.Saver()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 828, in __init__\n",
      "    self.build()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 840, in build\n",
      "    self._build(self._filename, build_save=True, build_restore=True)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 878, in _build\n",
      "    build_restore=build_restore)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 508, in _build_internal\n",
      "    restore_sequentially, reshape)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 328, in _AddRestoreOps\n",
      "    restore_sequentially)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 575, in bulk_restore\n",
      "    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_io_ops.py\", line 1696, in restore_v2\n",
      "    name=name)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\", line 794, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\", line 507, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3357, in create_op\n",
      "    attrs, op_def, compute_device)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3426, in _create_op_internal\n",
      "    op_def=op_def)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1748, in __init__\n",
      "    self._traceback = tf_stack.extract_stack()\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 1300, in restore\n",
      "    names_to_keys = object_graph_key_mapping(save_path)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 1618, in object_graph_key_mapping\n",
      "    object_graph_string = reader.get_tensor(trackable.OBJECT_GRAPH_PROTO_KEY)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 915, in get_tensor\n",
      "    return CheckpointReader_GetTensor(self, compat.as_bytes(tensor_str))\n",
      "tensorflow.python.framework.errors_impl.NotFoundError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3457, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\wp\\AppData\\Local\\Temp\\ipykernel_18028\\1550785440.py\", line 3, in <module>\n",
      "    dataset_nodebiasing_test = plain_model.predict(dataset_orig_test)\n",
      "  File \"D:\\wp\\PycharmProjects\\AIF360\\aif360\\algorithms\\transformer.py\", line 27, in wrapper\n",
      "    new_dataset = func(self, *args, **kwargs)\n",
      "  File \"D:\\wp\\PycharmProjects\\AIF360\\aif360\\algorithms\\inprocessing\\adversarial_debiasing_dnn5.py\", line 341, in predict\n",
      "    saver.restore(self.sess, model_path)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 1306, in restore\n",
      "    err, \"a Variable name or other graph key that is missing\")\n",
      "tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n",
      "\n",
      "2 root error(s) found.\n",
      "  (0) Not found: Key plain_classifier/plain_classifier_1/classifier_model/linear/bias/Adam not found in checkpoint\n",
      "\t [[node save_1/RestoreV2 (defined at F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) ]]\n",
      "\t [[save_1/RestoreV2/_181]]\n",
      "  (1) Not found: Key plain_classifier/plain_classifier_1/classifier_model/linear/bias/Adam not found in checkpoint\n",
      "\t [[node save_1/RestoreV2 (defined at F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) ]]\n",
      "0 successful operations.\n",
      "0 derived errors ignored.\n",
      "\n",
      "Original stack trace for 'save_1/RestoreV2':\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n",
      "    app.start()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n",
      "    self.io_loop.start()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\asyncio\\base_events.py\", line 541, in run_forever\n",
      "    self._run_once()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\asyncio\\base_events.py\", line 1786, in _run_once\n",
      "    handle._run()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 460, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 367, in dispatch_shell\n",
      "    await result\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 662, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 360, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2915, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2960, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 78, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3186, in run_cell_async\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3377, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3457, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\wp\\AppData\\Local\\Temp\\ipykernel_18028\\1550785440.py\", line 3, in <module>\n",
      "    dataset_nodebiasing_test = plain_model.predict(dataset_orig_test)\n",
      "  File \"D:\\wp\\PycharmProjects\\AIF360\\aif360\\algorithms\\transformer.py\", line 27, in wrapper\n",
      "    new_dataset = func(self, *args, **kwargs)\n",
      "  File \"D:\\wp\\PycharmProjects\\AIF360\\aif360\\algorithms\\inprocessing\\adversarial_debiasing_dnn5.py\", line 340, in predict\n",
      "    saver = tf.train.Saver()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 828, in __init__\n",
      "    self.build()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 840, in build\n",
      "    self._build(self._filename, build_save=True, build_restore=True)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 878, in _build\n",
      "    build_restore=build_restore)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 508, in _build_internal\n",
      "    restore_sequentially, reshape)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 328, in _AddRestoreOps\n",
      "    restore_sequentially)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py\", line 575, in bulk_restore\n",
      "    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_io_ops.py\", line 1696, in restore_v2\n",
      "    name=name)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\", line 794, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\", line 507, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3357, in create_op\n",
      "    attrs, op_def, compute_device)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3426, in _create_op_internal\n",
      "    op_def=op_def)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1748, in __init__\n",
      "    self._traceback = tf_stack.extract_stack()\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2077, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'NotFoundError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3377, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3474, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2080, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1368, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1268, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1125, in structured_traceback\n",
      "    tb_offset)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2077, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2960, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 78, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3186, in run_cell_async\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3396, in run_ast_nodes\n",
      "    self.showtraceback()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2080, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1368, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1268, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1143, in structured_traceback\n",
      "    chained_exceptions_tb_offset)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2077, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"F:\\Software\\Anaconda3\\envs\\wptf115\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 965, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'tensorflow_core.estimator'\n"
     ]
    }
   ],
   "source": [
    "# Apply the plain model to test data\n",
    "dataset_nodebiasing_train = plain_model.predict(dataset_orig_train)\n",
    "dataset_nodebiasing_test = plain_model.predict(dataset_orig_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Plain model - without debiasing - dataset metrics"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Difference in mean outcomes between unprivileged and privileged groups = 0.000000\n",
      "Test set: Difference in mean outcomes between unprivileged and privileged groups = 0.000000\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Plain model - without debiasing - classification metrics"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Classification accuracy = 0.243321\n",
      "Test set: Balanced classification accuracy = 0.500000\n",
      "Test set: Disparate impact = 1.000000\n",
      "Test set: Equal opportunity difference = 0.000000\n",
      "Test set: Average odds difference = 0.000000\n",
      "Test set: Theil_index = 0.033713\n"
     ]
    }
   ],
   "source": [
    "# Metrics for the dataset from plain model (without debiasing)\n",
    "display(Markdown(\"#### Plain model - without debiasing - dataset metrics\"))\n",
    "metric_dataset_nodebiasing_train = BinaryLabelDatasetMetric(dataset_nodebiasing_train, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "\n",
    "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_nodebiasing_train.mean_difference())\n",
    "\n",
    "metric_dataset_nodebiasing_test = BinaryLabelDatasetMetric(dataset_nodebiasing_test, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "\n",
    "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_nodebiasing_test.mean_difference())\n",
    "\n",
    "display(Markdown(\"#### Plain model - without debiasing - classification metrics\"))\n",
    "classified_metric_nodebiasing_test = ClassificationMetric(dataset_orig_test, \n",
    "                                                 dataset_nodebiasing_test,\n",
    "                                                 unprivileged_groups=unprivileged_groups,\n",
    "                                                 privileged_groups=privileged_groups)\n",
    "print(\"Test set: Classification accuracy = %f\" % classified_metric_nodebiasing_test.accuracy())\n",
    "TPR = classified_metric_nodebiasing_test.true_positive_rate()\n",
    "TNR = classified_metric_nodebiasing_test.true_negative_rate()\n",
    "bal_acc_nodebiasing_test = 0.5*(TPR+TNR)\n",
    "print(\"Test set: Balanced classification accuracy = %f\" % bal_acc_nodebiasing_test)\n",
    "print(\"Test set: Disparate impact = %f\" % classified_metric_nodebiasing_test.disparate_impact())\n",
    "print(\"Test set: Equal opportunity difference = %f\" % classified_metric_nodebiasing_test.equal_opportunity_difference())\n",
    "print(\"Test set: Average odds difference = %f\" % classified_metric_nodebiasing_test.average_odds_difference())\n",
    "print(\"Test set: Theil_index = %f\" % classified_metric_nodebiasing_test.theil_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply in-processing algorithm based on adversarial learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn parameters with debias set to True\n",
    "debiased_model = AdversarialDebiasingDnn5(privileged_groups = privileged_groups,\n",
    "                          unprivileged_groups = unprivileged_groups,\n",
    "                          scope_name='debiased_classifier',\n",
    "                          debias=True,\n",
    "                          sess=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\wp\\PycharmProjects\\AIF360\\aif360\\algorithms\\inprocessing\\adversarial_debiasing_dnn5.py:79: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "epoch 0; iter: 0; batch classifier loss: 0.688283; batch adversarial loss: 0.726517\n",
      "epoch 0; iter: 200; batch classifier loss: 0.400095; batch adversarial loss: 0.676711\n",
      "epoch 1; iter: 0; batch classifier loss: 0.375739; batch adversarial loss: 0.653499\n",
      "epoch 1; iter: 200; batch classifier loss: 0.482282; batch adversarial loss: 0.632502\n",
      "epoch 2; iter: 0; batch classifier loss: 0.388942; batch adversarial loss: 0.580499\n",
      "epoch 2; iter: 200; batch classifier loss: 0.572508; batch adversarial loss: 0.581376\n"
     ]
    }
   ],
   "source": [
    "debiased_model.fit(dataset_orig_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the plain model to test data\n",
    "dataset_debiasing_train = debiased_model.predict(dataset_orig_train)\n",
    "dataset_debiasing_test = debiased_model.predict(dataset_orig_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Plain model - without debiasing - dataset metrics"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Difference in mean outcomes between unprivileged and privileged groups = 0.000000\n",
      "Test set: Difference in mean outcomes between unprivileged and privileged groups = 0.000000\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Model - with debiasing - dataset metrics"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Difference in mean outcomes between unprivileged and privileged groups = 0.000000\n",
      "Test set: Difference in mean outcomes between unprivileged and privileged groups = 0.000000\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Plain model - without debiasing - classification metrics"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Classification accuracy = 0.234722\n",
      "Test set: Balanced classification accuracy = 0.500000\n",
      "Test set: Disparate impact = 1.000000\n",
      "Test set: Equal opportunity difference = 0.000000\n",
      "Test set: Average odds difference = 0.000000\n",
      "Test set: Theil_index = 0.032674\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "#### Model - with debiasing - classification metrics"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Classification accuracy = 0.234722\n",
      "Test set: Balanced classification accuracy = 0.500000\n",
      "Test set: Disparate impact = 1.000000\n",
      "Test set: Equal opportunity difference = 0.000000\n",
      "Test set: Average odds difference = 0.000000\n",
      "Test set: Theil_index = 0.032674\n"
     ]
    }
   ],
   "source": [
    "# Metrics for the dataset from plain model (without debiasing)\n",
    "display(Markdown(\"#### Plain model - without debiasing - dataset metrics\"))\n",
    "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_nodebiasing_train.mean_difference())\n",
    "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_nodebiasing_test.mean_difference())\n",
    "\n",
    "# Metrics for the dataset from model with debiasing\n",
    "display(Markdown(\"#### Model - with debiasing - dataset metrics\"))\n",
    "metric_dataset_debiasing_train = BinaryLabelDatasetMetric(dataset_debiasing_train, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "\n",
    "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_debiasing_train.mean_difference())\n",
    "\n",
    "metric_dataset_debiasing_test = BinaryLabelDatasetMetric(dataset_debiasing_test, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "\n",
    "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_debiasing_test.mean_difference())\n",
    "\n",
    "\n",
    "\n",
    "display(Markdown(\"#### Plain model - without debiasing - classification metrics\"))\n",
    "print(\"Test set: Classification accuracy = %f\" % classified_metric_nodebiasing_test.accuracy())\n",
    "TPR = classified_metric_nodebiasing_test.true_positive_rate()\n",
    "TNR = classified_metric_nodebiasing_test.true_negative_rate()\n",
    "bal_acc_nodebiasing_test = 0.5*(TPR+TNR)\n",
    "print(\"Test set: Balanced classification accuracy = %f\" % bal_acc_nodebiasing_test)\n",
    "print(\"Test set: Disparate impact = %f\" % classified_metric_nodebiasing_test.disparate_impact())\n",
    "print(\"Test set: Equal opportunity difference = %f\" % classified_metric_nodebiasing_test.equal_opportunity_difference())\n",
    "print(\"Test set: Average odds difference = %f\" % classified_metric_nodebiasing_test.average_odds_difference())\n",
    "print(\"Test set: Theil_index = %f\" % classified_metric_nodebiasing_test.theil_index())\n",
    "\n",
    "\n",
    "\n",
    "display(Markdown(\"#### Model - with debiasing - classification metrics\"))\n",
    "classified_metric_debiasing_test = ClassificationMetric(dataset_orig_test, \n",
    "                                                 dataset_debiasing_test,\n",
    "                                                 unprivileged_groups=unprivileged_groups,\n",
    "                                                 privileged_groups=privileged_groups)\n",
    "print(\"Test set: Classification accuracy = %f\" % classified_metric_debiasing_test.accuracy())\n",
    "TPR = classified_metric_debiasing_test.true_positive_rate()\n",
    "TNR = classified_metric_debiasing_test.true_negative_rate()\n",
    "bal_acc_debiasing_test = 0.5*(TPR+TNR)\n",
    "print(\"Test set: Balanced classification accuracy = %f\" % bal_acc_debiasing_test)\n",
    "print(\"Test set: Disparate impact = %f\" % classified_metric_debiasing_test.disparate_impact())\n",
    "print(\"Test set: Equal opportunity difference = %f\" % classified_metric_debiasing_test.equal_opportunity_difference())\n",
    "print(\"Test set: Average odds difference = %f\" % classified_metric_debiasing_test.average_odds_difference())\n",
    "print(\"Test set: Theil_index = %f\" % classified_metric_debiasing_test.theil_index())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "\n",
    "    References:\n",
    "    [1] B. H. Zhang, B. Lemoine, and M. Mitchell, \"Mitigating UnwantedBiases with Adversarial Learning,\" \n",
    "    AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society, 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}